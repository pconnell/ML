[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Protected Classes as Predictors in Mortgage Approval",
    "section": "",
    "text": "1 Introduction\nHow strong is the connection between your age, gender, or race and ability to attain a mortgage?\nIn early 2024, US Senator Sherrod Brown, along with colleagues, called for a review of lending practices by Navy Federal Credit Union (NFCU) as a result of a late 2023 CNN report that oulined disparate impact to persons of color seeking to acquire conventional loans for home purchases. The disparity in results between groups appears damning to the organization’s reputation. CNN’s analysis predominantly explored the mathematical differences between racial groups and various lenders, but did not necessarily outline a direct cause-and-effect relationship (e.g. being a person of color seeking a loan at NFCU is guaranteed to decrease your approval chances). Their analysis instead covered the rates of approval for the different groups, and compared those approval rates between groups and against other financial organizations’ approval rates.\n\n\n\nvisual comparison of NFCU to other financial institutions regarding loans to persons of color\n\n\nEach of the organizations in the above graphic (from CNN) appear to have some degree of disparity in holding higher approvals for White/Caucasian applicants than for Black/African American, at first glance. The graphic certainly highlights the highest difference in approval rates at NFCU vs that of other financial institutions. Being at the top of the list of disparity absolutely draws ire from onlookers, yet the other institutions on the list may also be worthy of similar scrutiny. Disparate outcomes are disparate outcomes, and any case in which there are significant differences in outcomes for individuals on a basis of age, gender, race, or ethnicity (also called protected classes) is of concern to everyone.\nA follow-up article by CNN includes statements from NFCU and an external reviewer. Some of the statements claim that non-race factors such as income verification and incomplete credit applications weren’t included in CNN’s study, and that other proprietary and non-public information are included in the organization’s mortgage underwriting processes.\nAlso of interest is an August 2024 story from Lehigh University on Artificial Intelligence, in particular Large Language Models, exhibiting bias in mortgage underwriting decisions. The article covers a study done using ChatGPT 3.5 and 4.0, Llama3-8B and 3-70, and Claude3 Sonnet and Opus, in which the LLMs were prompted to decide whether or not to approve a loan application based on provided data. The study found that the LLMs had similar biases for recommending interest rates, and great variation in bias as to whether or not loans would be approved. When the LLMs were directly instructed to use no bias and ignore race in making the decisions, the disparity in approval outcomes disappeared. This suggests that technology in decision-making processes is a double-edged sword, capable of doing both great harm and great good, depending on how it is weilded.\nThe Home Mortgage Disclosure Act (HMDA) is a federal law that establishes mandatory reporting for financial institutions that provide loans to borrowers. It mandates reporting of various values, metrics, and qualities of applicants and co-applicants, such as their debt-to-income ratio, loan amount requested vs. property value, age/gender/race of applicants, whether or not the loan was approved, along with many other datapoints. Part of the intent of this law is to enable the examination of lenders for potential discriminatory practices in lending, and enable accountability and oversight to combat such issues. HMDA data was leveraged for both the CNN’s report and for the study referenced in Lehigh University’s article.\nIn an ideal world, a potential borrower’s protected class features should have no impact on their ability to attain a loan, but instead be based solely on the borrower’s overall ability to repay the loan, given multiple financial factors. This research seeks to leverage HMDA data to explore CNN’s identified disparities, and explore other possible disparities within other lending organizations, through predictive modeling. Leveraging multiple techniques such as identifying common associations and trends within the data, performing statistical analyses, developing multiple model constructs, and via inclusion and exclusion of protected classes as part of model training and evaluation data, this study seeks to explore and answer some of the following questions (solely using publically available data):\n\nWhich factors within avaialble HMDA data are the greatest influencers in mortgage approvals?\nAre protected classes of applicants and co-applicants strong predictors for determining loan approval?\nIf protected classes are strong predictors for loan approval, is the predictive strength greater for one institution over another?\nIf protected classes are strong predictors for loan approval, is the predictor’s strength higher within a particular geographic region?\nHow is predictive model performance impacted when including or excluding protected class data in training and testing data? e.g. Does performance increase when including? If so, by how much?\nHow well do predictive models perform when trained using protected class information that was collected by a lender observationally (e.g. via inspection of surname or visually seeing the applicant)?\nHow do loan-specific or home-specific qualities and features, absent borrower features, impact predictions of approval or denial?\nAre borrower features aside from race as or more associated with a result of loan denial? e.g. is being female or being under or over a certain age as associated with loan denial as being a certain race?\nCan available HMDA data be leveraged to effectively predict mortgage borrower interest rates?\nIf effective mortgage rate predictions are possible, how do they change when including or controlling for protected class data in the model?\nCan selected / identified features and outcome (e.g. debt-to-income-ratio, loan-to-value ratio, and decision for approval or denial of the loan) be used to predict the borrower’s protected class features of race, gender, or age group?\nAs inferred by lenders, can various latent variables be identified as impactful within modeling, and can the degree or extent of their impact on the decision process be determined as it pertains to mortgage underwriting decisions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "",
    "text": "2.1 Links to Data and Code\nThe outcomes of the work performed in this section are present in multiple locations. The data itself was too large to include as part of the github repository for this effort.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#links-to-data-and-code",
    "href": "data.html#links-to-data-and-code",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "",
    "text": "The GitHub Respository can be found here.\nScript to leverage APIs and download the target data is located here.\nInitial data download, prior to scoping and cleaning, is located here.\nThe script to scope, clean, and combine the dataset is located here.\nThe cleaned dataset is located here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#data-gathering",
    "href": "data.html#data-gathering",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.2 Data Gathering",
    "text": "2.2 Data Gathering\nTo pursue the established research objectives, the following resources were identified and leveraged as data sources:\n\nThe Global Legal Entity Identifier (GLEIF) API\nThe FFIEC Home Mortgage Disclosure Act (HMDA) API\n\nCNN provides explicit data selection, cleaning, and transformation information at the base of their first article in the section labeled “How We Reported This Story.” The scope of this research includes the scope identified by CNN, as the nature of their filtration and focus could be ascribed to what one would consider being part of the American dream - the ability to attain a first loan for a single-family property as a primary residence, not for the purpose of business or commerce. The CNN article predominantly leveraged 2022 and prior year data, whereas this research will inspect 2023 data.\nLenders in question, listed in the graphics for the first CNN article, are identified via the GLEIF API. HMDA Data solely contains the Legal Entity Identifier (LEI) in each record as opposed to the name of the entity. In order to perform aggregate analysis and labeling of data as being linked to one of these organizations, it is necessary to query another source to identify the appropriate LEIs for use in querying HMDA. Furthermore, HMDA is a large database, and being able to provide filtering conditions substantially reduces the size of the returned data. Absent key filters, large volumes are returned in a single file. At the same time, the number of filters that can be supplied to the HMDA API simultaneously is limited, so filtering by organization and other key filters will minimize the data pull before further cleaning.\nThe HMDA API provides simple access to a wide array of data on mortgages. Additionally, the API provides a well-documented data dictionary that spells out the returned features and their meanings in the context of the Loan Application Register. An API query returns a total of 99 columns with quantitative and qualitative data on the prospective borrower, the property they seek to purchase, aspects and features of the loan, and the final action taken on the loan application. Some of these columns may exceed the scope of this research, whereas many others are necessary and important to the established research questions. Finding creative and effective means of reducing dimensionality is key.\nNeither of the API endpoints leveraged in data collection for this research required an API key, thus simplifying the data gathering process.\nThe source code for data gathering efforts in this research is located here.\nExample use of the GLEIF API:\nimport requests\nendpoint = \"https://api.gleif.org/api/v1/lei-records\"\nparams = {\n    'page[size]':100, #specify number of records to return per page\n    'page[number]':1, #specify the page number to return within the same search scope\n    'filter[entity.names]':'NAVY FEDERAL CREDIT UNION' #provide an organization\n}\n\nrequests.get(endpoint,params)\nFor the GLEIF API, the JSON response will contain the LEI for organizations with similar names to what is provided in the filter[entity.names] parameter. Depending on how specific one is with the name provided, this could produce a long list of results. This was the case when examining larger banks like JP Morgan and Wells Fargo while building the code for this research. In some cases, the GLEIF API returned 400 records. When this occurs, it is necessary to make use of the page[size] and page[number] parameters for this API.\nIn the case of gathering 400 results, in order to pull all of them into data records via use of this API, one would need to perform 4 API calls, each time updating the page[number] parameter to the next valid value. The API response also always provides a lastPage value for valid responses under the path response.meta.pagination.lastPage, which one can use for iteration if needed to extract all LEIs for an entity.\nBelow depicts example use of the HMDA API in Python:\nhmda_endpoint = \"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv\"\nhmda_params = {\n    'years':'', #specify the year or list of years for which you seek to query\n    'loan_types':1, #conventional loans only\n    'leis':'' #will change/update based upon what orgs we're downloading\n}\n\nresp = requests.get(endpoint,params)\n\nwith open('/file/path/here.csv','wb') as f:\n    f.write(resp.content)\n    f.close()\nOf note for the HMDA endpoint in this case is the fact that it natively returns record data in CSV format. As such, the means to store the data is relatively simple, requiring no substantial parsing of JSON content to process the data into record or dataframe format.\nThe data, from the initial API queries and prior to further cleaning and tailoring, is located here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.3 Data Cleaning",
    "text": "2.3 Data Cleaning\nThe primary methods of cleaning in this research revolve around dimensionality reduction and row reduction. The goal for cleaning is to minimize the number of columns in the dataset to key quantitative and qualitative features holding potential use in modeling, eliminating records out of the scope of this research, and handling blank/missing/incorrect values in the remaining records.\n\n2.3.1 Dimensionality Reduction\n\n2.3.1.1 Collapsing Redundant Columns with Binary Encoding\nThis research collapses the following columns to deduplicate data and reduce columnar dimensionality:\nApproximately 29 columns in the data contain categorical information that can be encoded in binary and summed into a single column to retain all data while reducing dimensionality. These columns pertain to applicant and co-applicant race and ethnicity, the automated underwriting systems used by organizations to support decision making in approving or denying loan applications, and reasons for loan denial, if applicable.\nSince each of these columns can take on multiple values, the HMDA dataset owners have allowed for up to 5 columns for each of these categorical variables. By translating each possible value to a unique binary encoding (with number of bits equal to the number of classes, and solely a single 1 in each encoding), the sum across multiple columns will provide for a unique value containing all classifications in a single categorical variable.\nFurthermore - this transformation addressses some potential biases in CNN’s methods. Their report excluded records in the case of mixed-race applicants and co-applicants as being members of a racial group. The binary encoding allows for their inclusion as being part of such groups; while individuals of mixed race may not be fully part of any single group, they remain part of it.\nThe data contain multiple column datapoints for the ethnicity and race of applicants and co-applicants for a total of 20 additional columns (e.g. “co-applicant_race-1, co-applicant_race-2…”). Not all of these columns contain viable data. An individual can have multiple races and ethnicities. To allow for retaining of the data while also eliminating unnecessary blank columns, these columns are collapsed to a single column, each - reducing the data by 16 columns.\nAnother set of column collapses are performed on the ‘aus-’ (automated underwriting system) and ‘denial_reason’ columns. There are 5 columns in the source data containing the different system(s) used by lenders to support underwriting decision making processes. The denial_reason is rich with potentially important information that could lend itself to the identification of latent or unavailable variables (e.g. credit score, income verification, etc). These columns are collapsed in the same manner as for the aforementioned race and ethnicities of applicants.\nEach value was translated from the digit representing it to instead a binary representation with a number of bits equivalent to the number of valid classes available for the variable from the data dictionary. After each value in each column was mapped to a bitwise representation, the sum across each column was taken to produce a final column with bits containing all of the original information. This transformation enables simple bitwise AND operations to identify records that meet a specific condition (or combination of conditions) while also eliminating columnar redundancy.\n\n\n\n\nTable 2.1: Remapping of Racial Categories for applicants and co-applicants\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nAmerican Indian or Alaska Native\n1\n1\n\n\nAsian\n2\n2\n\n\nAsian Indian\n21\n4\n\n\nChinese\n22\n8\n\n\nFilipino\n23\n16\n\n\nJapanese\n24\n32\n\n\nKorean\n25\n64\n\n\nVietnamese\n26\n128\n\n\nOther Asian\n27\n256\n\n\nBlack or African American\n3\n512\n\n\nNative Hawaiian or Other Pacific Islander\n4\n1024\n\n\nNative Hawaiian\n41\n2048\n\n\nGuamanian or Chamorro\n42\n4096\n\n\nSamoan\n43\n8192\n\n\nOther Pacific Islander\n44\n16384\n\n\nWhite\n5\n32768\n\n\nInformation not provided by applicant in mail, internet, or telephone application\n6\n65536\n\n\nNot applicable\n7\n131072\n\n\nNo co-applicant\n8\n262144\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.2: Remapping of Ethnic Categories for applicants and co-applicants\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nHispanic or Latino\n1\n1\n\n\nMexican\n11\n2\n\n\nPuerto Rican\n12\n4\n\n\nCuban\n13\n8\n\n\nOther Hispanic or Latino\n14\n16\n\n\nNot Hispanic or Latino\n2\n32\n\n\nInformation not provided by applicant in mail, internet, or telephone application\n3\n64\n\n\nNot applicable\n4\n128\n\n\nNo co-applicant\n5\n256\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Remapping of Automated Underwriting Systems\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nDesktop Underwriter (DU)\n1\n1\n\n\nLoan Prospector (LP) or Loan Product Advisor\n2\n2\n\n\nTechnology Open to Approved Lenders (TOTAL) Scorecard\n3\n4\n\n\nGuaranteed Underwriting System (GUS)\n4\n8\n\n\nOther\n5\n16\n\n\nInternal Proprietary System\n7\n32\n\n\nNot applicable\n6\n64\n\n\nExempt\n1111\n128\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Remapping of Denial Reasons\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nDebt-to-income ratio\n1\n1\n\n\nEmployment history\n2\n2\n\n\nCredit history\n3\n4\n\n\nCollateral\n4\n8\n\n\nInsufficient cash (downpayment, closing costs)\n5\n16\n\n\nUnverifiable information\n6\n32\n\n\nCredit application incomplete\n7\n64\n\n\nMortgage insurance denied\n8\n128\n\n\nOther\n9\n256\n\n\nNot applicable\n10\n512\n\n\n\n\n\n\n\n\nExamining Table 2.4, for instance consider a cell value of 96 in the ‘denial_reason’ column. There are two unique binary encoding values - 32 and 64, that produce this sum. The value 96, then, would signify a loan that was denied both for having ‘Unverifiable Information’ and for ‘Credit application incomplete’. Similarly, referencing Table 2.1, a value of 32770 in this column would signify a person whose races are Asian (32768) and White (2) (2+32768=32770).\nThis binary numeric representation of categorical data allows the column to contain multiple distinct and potentially impactful datum while reducing dimensionality. E.g. to filter the dataset to records pertaining to the race Asian, the filter can be performed by selecting records where the binary AND of 32768 & [column_value] = 32768. Other races may also be present in the filtered records, but all Asians will be captured.\nLeveraging the columns natively for tasks such as clustering may not be effective or efficient, and could pose challenges in performing unsupervised learning. Further cleaning may be necessary for such tasks, but collapsing and retaining the data ensures its availablity for further transformation and cleaning. For instances, the re-splitting and pivoting of these columns into boolean values could be leveraged in association rule mining or in Bernoulli Naive Bayes analyses.\nNotice also that encodings for not available or not applicable types of data are higher values within each encoding list. This actually provides a very simple data validation technique, and allows for the identification or elimination of potentially erroneous records. Namely any value:\n\nwithin the exclusive interval (65536,131072), or greater than 131072 for applicant_race\nwithin the exclusive intervals (65536,131072) and (131072,262144), or greater than 262144 for co-applicant_race\nwithin the exculsive interval (64, 128), or greater than 128 for applicant_ethnicity\nwithin the exclusive intervals (64,128) and (128,256), or greater than 256 for co-applicant_ethnicity\nwithin the exclusive interval (64,128), or greater than 128 for aus (automated underwriting system)\ngreater than 512 for denial_reason\n\nwill signify an erroneous record. Namely, if any value is found to include “Not Applicable” in conjunction with another valid selection, one of the two selections were selected in error, and the data is not reliable. As such, these records can be filtered from the source data. As such, records meeting the above criteria are removed from the source data.\nBecause the totality of racial and ethnicity information can now be contained within a single column, the derived_ columns for ethnicity and race can be dropped from the dataset.\n\n\n2.3.1.2 Column Elimination\nThe following list of columns were eliminated from the source data\n\n\n\n\n\n\n\nColumn Dropped\nReason\n\n\n\n\nactivity_year\n2023 Data Only\n\n\nderived_msa-md\nUsing county_code\n\n\ncensus_tract\nUsing county_code\n\n\nderived_loan_product_type\nAvailable in other columns\n\n\nderived_dwelling_category\nAvailable in other columns\n\n\nconforming_loan_limit\nProject scope\n\n\nlien_status\nProject scope\n\n\nreverse_mortgage\nOne value after scope applied\n\n\nbusiness_or_commercial_purpose\nProject scope\n\n\nnegative_amortization\nOne value after scope applied\n\n\noccupancy_type\nProject scope\n\n\nconstruction_method\nProject scope\n\n\nmanufactured_home_secured_property_type\nProject scope\n\n\nmanufactured_home_land_property_interest\nProject scope\n\n\nsubmission_of_application\nProject scope\n\n\ninitially_payable_to_institution\nProject scope\n\n\nderived_ethnicity\nAvailable in other columns\n\n\nderived_race\nAvailable in other columns\n\n\nloan_type\nProject scope\n\n\nprepayment_penalty_term\nProject scope\n\n\napplicant_age_above_62\nAvailable in other columns\n\n\nco-applicant_age_above_62\nAvailable in other columns\n\n\ntotal_points_and_fees\n&lt;1% of records have values\n\n\nrate_spread\n&lt;1% of records have values\n\n\nmultifamily_affordable_units\n&lt;1% of records have values\n\n\nlei\nSubstituting company name\n\n\nstate_code\nUsing county_code\n\n\nhoepa_status\nSingle value in column after scope applied\n\n\n\n\n\n\n\n\n2.3.2 Row Elimination\nThe initial pull from the HMDA API allowed solely for filtering by one or two filters. This effort pulled data for 5 selected lenders in the year 2023, for conventional loans only. To reduce the data to the same scope stipulated in the CNN article, row records must further be eliminated based upon the following criteria:\n\nexact duplication of another row (from the source data, before any transformations are applied). The nature of the data is such that the probability of an exact duplicate of 99 column values between two records is negligible, and as such exact duplicates should be ignored.\nscoping to same frame as CNN:\n\nlien_status != 1, or only first lien secured properties\ntotal_units not in [1,2,3,4], or only 1-4 unit homes\nconforming_loan_limit != ‘C’, or only conforming loans\nbusiness_or_commercial purpose != 2, or non-commercial and non-business properties\noccupancy_type != 1, or primary residence\nloan_purpose != 1, or for the purchase of a home\n\nrecords in which there is no value for county_code\nrecords for which the loan would be neither approved nor denied: when the action_taken value is 4 (Application Withdrawn) or 5 (File closed for incompleteness).\nrecords that have invalid values as defined in the binary encoding section above for 6 variables (applicant_race, co-applicant_race, applicant_ethnicity, co-applicant_ethnicity, aus, denial_reason).\n\n\n\n2.3.3 Ordinal Encoding\nThe following columns are re-encoded as ordinal data:\n\ntotal_units\napplicant_age\ndebt_to_income_ratio\n\n\n\n2.3.4 Missing and Blank Values\nThe following aspects of the dataset have the potential for missing or blank values. The reporting requirements vary for the different columns in the HMDA LAR, and the blanks and missing values can occur for a multitude of reasons:\n\napplicant didn’t provide the information\napplicant withdrew their application\ndata was not entered or submitted by the institution\ninstitution did not have or receive the data for the applicant or their prospective property\nthe column may not be applicable or filled because the application was denied, or was not applicable for the particular loan circumstances (e.g. an introductory rate period wouldn’t be applicable for an adjustable rate mortgage)\n\nEach of the following is replaced by the median value of the variable when the data is grouped by state_code and total_units (e.g. number of rooms in the home):\n\nproperty_value\nloan_to_value_ratio. special note: when property_value and loan_amount are available for a record, this value is filled with the value of the loan amount (times 100) divided by the listed property value. Otherwise, any blanks are filled with the median loan to value ratio when the data is grouped by state and number of rooms.\n\nEach of the following is replaced by the median value of the variable when the data is grouped by state_code and company (e.g. institution processing the mortgage application):\n\nlender_credits\nintro_rate_period\ninterest_rate\norigination_charges\ndiscount_points\ntotal_loan_costs\n\nNote: Whenever the grouping by state_code and company produced N/A values, these were replaced with the value 1 in the event of a need for logarithmic transformations on these variables.\n\n\nEach of the following is replaced with the mode of the variable when the data is grouped by state_code and company (e.g. institution processing the mortgage application):\n\nintro_rate_period\ndebt_to_income_ratio\n\nNote: Whenever the grouping by state_code and total_units produced N/A values, these were replaced with the value 0 as they better align with categorical variables than as numeric, and may not undergo transformations.\n\n\nAnother common blank value included the income feature. Blanks and missing values are replaced with the value from the column ffiec_msa_md_median_family_income divided by 1000. This value is an approximation for median family income in the specific metropolitan statistical area (MSA) in which the home is located.\n\n\n2.3.5 Incorrect Column Values\nSome values of interest for inspection include loan_to_value_ratio, property_value, and loan_amount, which should, in theory, all be related and have reasonable values. A first-lien for-the-purpose-of-purchase, conventional, conforming loan is highly unlikely to have a value greater than 100%. This is because a savvy lender would not secure a property as collateral for a loan that greatly exceeds its value.\nThat being said, there may be cases were this ratio could stretch further based upon qualities of the borrower and status of the market, and especially if the borrower seeks to include the overage as part of plans for improvements of the home, or covering closing costs or other mortgage requirements - in combination with the borrower having an excellent credit score or having a very high income. The case of high loan-to-value-ratio (henceforth LTV Ratio), may be unlikely, but is not impossible.\nBelow is an examination for each lender for extreme values in LTV ratio:\n\n\n\n\n\n\n\n\nFigure 2.1: Violin plot of Loan To Value Ratio, by lender\n\n\n\n\n\nThere appear to be some extreme values to the high right for each lender, in some cases with the ratio nearing 1000%. To examine further, whether or not the loan was approved should be considered, and if the loan was denied, the reason for denial should be considered as well.\nIf any values in this extreme range consist of loans that were denied, and reasons for denial include items such as “insufficient collateral”, then these items may simply be extreme case outliers that are valid and real as opposed to errors in the data.\nConsidering a range of 100% or greater loan_to_value_ratio, the following information is available:\n\n\n\n\n\n\n\n\nFigure 2.2: Approvals/Denials for Applications with LTV &gt; 100%\n\n\n\n\n\nIn Figure 2.2, it is clear that the majority of loans in this category were denied by lenders. There is a subset of loans that were approved (~40%), and examining further to examine factors such as the loan_to_value_ratios in this group, as well as reasons for denial amongst the denied applications, is necessary before determining need for identifying these records as errors vs. outliers.\n\n\n\n\n\n\n\n\nFigure 2.3: Reasons Cited for Loan Denials with LTV Ratio &gt; 100%\n\n\n\n\n\nThis plot makes it apparent that many of these high LTV ratio loan applications are likely legitimate. Many of them were denied on the bases of debt to income ratio and collateral (e.g. the property is insufficient collateral to cover the risk of the loan).\nThat being said, it’s also necessary to examine the 40% of loans that were actually approved in this window.\n\n\n\n\n\n\n\n\nFigure 2.4: strip plot of high LTV (&gt;100%) 2023 loans by approval\n\n\n\n\n\nFrom Figure 2.4, its apparent that the approved loans in this excess range tended to be on the much lower end of the spectrum, seemingly hovering just above 100%, whereas the denied applications span from just over 100% all the way up to 1000%. All in all, it would seem these extreme values, in both the cases of approved and denied loans, are legitimate values.\nA remaining concern is for potential discrepancy in the loan to value ratio itself. It’s appears odd that many data points bunch up at the 1000% mark as opposed to going above or below it, like it’s an arbitrary cap on the actual value. Below the same plot is examined, but instead the Loan to Value Ratio is replaced the value of Loan Amount * 100 / Property Value.\n\n\n\n\n\n\n\n\nFigure 2.5: strip plot of high LTV (&gt;100%) 2023 loans by approval, with adjusted LTV value\n\n\n\n\n\nIt would seem that the 1000% mark may indeed be an arbitrary cap, or a potential error of some sort. Per the Consumer Financial Protection Bureau on reporting requirements, the value for the combined loan to value ratio is to be “the ratio of the total amount of debt secured by the property to the value of the property relied on in making the credit decision.”\nIn most other cases in which the values sit below 100%, this calculated value is within a small unit difference of the reported value. This may be because the decimal places on the decision are only to be included if those decimal points were relied upon to make the decision on whether or not to approve the loan.\nIn light of the exploration on loan to value ratio, the feature loan_to_value_ratio will be replaced with the value of \\(\\frac{100\\cdot\\text{loan amount}}{\\text{property value}}\\) . In cases where one of these values is blank, the loan_to_value_ratio will be filled with the median value for loan_to_value_ratio when the data is grouped by state_code and total_units (or number of rooms) in the home.\nAnother feature worth exploring is income. This feature, per the HMDA LAR Data Dictionary, is in thousands of dollars. There are millionaires and even billionaires in the United States. Generally, though, one might expect to see an exponential distribution of income in the source dataset. Furthermore, one should expect to see a narrow string of values getting thinner and thinner as it approaches the upper end of the dataset. Inspecting income with a boxplot, there are some clear challenges:\n\n\n\n\n\n\n\n\nFigure 2.6: boxplot of borrower income\n\n\n\n\n\nA cursory inspection of this data suggests that there may be a single, erroneous outlier sitting at a value of approximately $209M worth of income, compressing the visibility of the boxplot down to nearly nothing. It is possible that this datapoint is real and correct, and that there was a single, very wealthy applicant for a mortgage in the data.\nProducing the same plot, absent the outlier, however, displays similar results:\n\n\n\n\n\n\n\n\nFigure 2.7: Boxplot of Income (Outlier Removed)\n\n\n\n\n\nHowever, below is an examination of the data with and without this high-end datapoint using kernel density estimation for the natural logarithm of income:\n\n\n\n\n\n\n\n\nFigure 2.8: Logarithmic Kernel Density Estimate for Applicant Income\n\n\n\n\n\nWith the exception of some deviations from normality in the central portions of the curve, these plots appear to showcase a normally distributed variable. Examining the mean and standard deviations for the log of income, with and without the extreme outlier, produces the following:\n\n\n\n\nTable 2.5: mean and standard deviation, with and without extreme outlier for income\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\n\nWith Outlier\n4.725800\n0.623842\n\n\nWithout Outlier\n4.725769\n0.623654\n\n\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the log-transformed feature, income, appears to retain the same features of central tendency with or without this extreme value. Provided that a log transformation of the feature is used within any models, this outlier should have little to no impact on training and testing data, as the underlying parent distribution can be approximated by that of the normal distribution \\(N(4.726,0.624)\\) in either case. As such, the row will be retained, and the value will not be adjusted (minus the log-transformation).\n\n\n2.3.6 Cleaned Columns\nPrior to cleaning, there were numerous blank or not applicable values within the data. The below image depicts the state of a subset of the data, after initial download.\n\n\n\nData, after initial download, held multiple blanks\n\n\nThe column volume is not done justice by this image; the original dataset held nearly 100 columns with large swaths of blank and missing values. Through the methods described previously, this data was appropriately scoped, transformed, cleaned, and simplified for further use.\nThe final results of the cleaning process are too large to fully depict in images here. Despite a substantial cleaning effort, there remains a large volume of columns (~50 down from 100) of varying types. Here, some key cleanups performed during the cleaning effort are highlighted:\n\n\n\nAfter Filling Blanks With Medians, Modes\n\n\nOne can see that there are no blanks in this sector of the dataset, and that there is a wider array of values and reasonable variablility in the columns, examining record by record. There may be additional work and adjustments to be performed in the realm of cleaning, but having populated columns, filled with median and modal values, provides a point of departure for further examination and analysis.\n\n\n\nCollapsing 29 Columns to 6\n\n\nThe above depiction of the collapsed columns is a substantial dimensionality reduction while retaining all of the underlying data. Each of these columns were originally 5 columns in the source data (less denial_reason, which was 4 columns).\nIn any case where the value in one of these columns holds an exact power of 2, it means that only one of those 4-5 original columns held a value - values like 1, 16, 32, 64, 128, 256, 512, 32768, 65536, 131072, and 262144 (all present in this graphic).\nThis means there were nearly 24 columns worth of blanks for each record depicted in the above image. One can nearly count on their fingers the number of instances for these records in which more than one of those columns had legitimate values. This collapsing of records while retaining data is good compression and retention for storage and future modeling.\nThe cleaned dataset can be found and downloaded here. Alternatively, one can clone this research project’s GitHub Repository, run the data pulling script, and then run the data cleaning script, in order to reproduce the same dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#further-exploratory-data-analysis",
    "href": "data.html#further-exploratory-data-analysis",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.4 Further Exploratory Data Analysis",
    "text": "2.4 Further Exploratory Data Analysis\nTo examine the data under similar conditions as CNN for 2023 data, below is a plot of the top 5 lenders and their approval rates for select racial groups, controlling for no other variables:\n\n\n\n\n\n\n\n\nFigure 2.9: 2023 Loan Approval Rates, By Company and Select Race\n\n\n\n\n\nThe above appears to flow from 2022’s findings into 2023 - that, at least for NFCU, the trend appears to continue under the specified scope with approval rates of about 76% for White applicants, and 43.8% for Black/African American Applicants. Comparing Figure 2.9 to CNN’s figures, the outcomes are remarkably similar for 2023.\nSimilar exploration into sex and age is also of interest for this study:\n\n\n\n\n\n\n\n\nFigure 2.10: Approval Rates by Institution and Borrower Sex\n\n\n\n\n\nThis chart also reveals some interesting trends, not just for NFCU, but across lenders. Notably - when no sex is provided by the applicant, 100% of loans across lenders received approval of some sort. Somewhat similarly, for lenders like JP Morgan, Rocket Mortgage, and Wells Fargo, the institutions had high approval rates in cases for which the applicant selected both sexes on their application.\nLastly, as it is pertinent to the established research questions, examination of differences in outcomes for reported age is necessary.\n\n\n\n\n\n\n\n\nFigure 2.11: barplots of approval rates by age, 2023\n\n\n\n\n\nThis examination reveals similar results as for sex; when no age is reported or documented, it appears that the approval rate approaches 100% for applicants. However, it is not the case when race is not reported. In Figure 2.9, one can see that in the case of no race reported, approval rates for such mortgages ranged from 62.9% (NFCU) to 98.6% (JP Morgan).\nThese comparisons, however, are simply numerical and do not establish any kind of a cause and effect relationship in either case, for any of the variables. Just because the outcomes in 2023 when gender or age were not reported to these lenders were nearly 100% of the time an approved mortgage, does not mean that one not reporting their gender or age on a mortgage application guarantees that they will be approved by one of these 5 lenders.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#last-thoughts-on-source-data",
    "href": "data.html#last-thoughts-on-source-data",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.5 Last Thoughts on Source Data",
    "text": "2.5 Last Thoughts on Source Data\nThe provided visuals and exploration in this research thus far are observations of a snapshot in time - they, in and of themselves, do not establish cause-and-effect relationships or the presence of statistically significant differences in outcomes between protected classes and lenders.\nThe goals of this research include bi-directional modeling. While machine learning modeling on its own does not elucidate a cause-and-effect relationship between variables, it comes closer than simple observations or statistical analyses. Performing bi-directional modeling (can available features and subject’s age/gender/race predict their outcome? and can a subject’s outcome and features predict the subject’s age/gender/race) takes additional steps in the direction of causality.\nEstablishing significant differences can show a difference, but not the cause. Establishing models that effectively predict show that the statistical differences in groups can be leveraged effectively to predict outcomes, but also do not establish cause. Exploration of latent variables, interjection and intervention of available and latent variables, and further research, are all needed to establish causality.\nMore exploration is necessary within this data, and potentially additional cleaning and transformations. Few if any numeric variables were scaled or transformed to collapse them all within similar ranges. This will be needed for building certain models within this research effort. Further understanding of categorical variable associations and relationships need further exploration and study. This will be an ongoing effort throughout this research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "3  Principal Component Analysis",
    "section": "",
    "text": "3.1 Overview\nPrincipal component analysis (or PCA) is the act of rotating in combination with stretching or squishing the axes of a multi-dimensional dataset to better align with its direcitonality in multidimensional space. By realigning the axes, the variation within the data can be more directly tied to and explained by the axes (also called Principal Components). In some cases, the transformation can be so profound such that a substantial amount of the data’s variation can be explained using fewer dimensions.\nConsider the below graphic:\nFigure 3.1: A strongly correlated 2-dimensional dataset\nThe X and Y values of this data appear to be connected, correlated even. Note - PCA is NOT a correlation analysis, but leverages any existing correlation in the data between one or more variables to transform the basis of each datapoint and vector in the data.\nThe goal of PCA is to remove strong corrlelations with high R values from the data by realigning axes of the data along the directions in the data which contain the greatest variance in the data. The above plot appears to show that the x-coordinate is a good predictor for the y-coordinate, and a simple linear regression analysis reveals that this is the case.\nTable 3.1: Least Squares Regression, before PCA\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.985\n\n\nModel:\nOLS\nAdj. R-squared:\n0.985\n\n\nMethod:\nLeast Squares\nF-statistic:\n6400.\n\n\nDate:\nWed, 16 Oct 2024\nProb (F-statistic):\n4.49e-91\n\n\nTime:\n19:39:52\nLog-Likelihood:\n274.70\n\n\nNo. Observations:\n100\nAIC:\n-545.4\n\n\nDf Residuals:\n98\nBIC:\n-540.2\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n11.9091\n0.002\n6468.253\n0.000\n11.905\n11.913\n\n\nx1\n0.0168\n0.000\n80.000\n0.000\n0.016\n0.017\n\n\n\n\n\n\nOmnibus:\n41.522\nDurbin-Watson:\n0.564\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n139.401\n\n\nSkew:\n-1.374\nProb(JB):\n5.36e-31\n\n\nKurtosis:\n8.089\nCond. No.\n10.4\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nImagine instead if the axes, while retaining their perpendicularity (or orthogonality), streched in the general orientation of the regression line (we’ll call it \\(X'\\) or our new X axis) from Figure 3.1, with \\(Y'\\) bisecting \\(X'\\) at a \\(90^o\\) angle. Below is a depiction of the data after such a transformation.\nFigure 3.2: the 2D dataset, post PCA transformation\nFigure 3.2 shows us that after the transformation, the data has a correlation value closer to zero than having a strong positive value as it had within Figure 3.1. One might also note that Y’ is an directional inversion of Y in this case (e.g. values that were above the correlation plot line in Figure 3.1 are below the correlation line in Figure 3.2). The act of applying PCA transformation is a linear combination that can result in the rotation, expansion and/or contraction of a vector as it is transposed into the new basis space.\nPerforming regression analysis on this data once more reveals that the goal of PCA is reached, that the variables, when projected into this new basis, hold no correlation:\nTable 3.2: Least Squares Regression, after PCA\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared (uncentered):\n0.000\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n-0.010\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.000\n\n\nDate:\nWed, 16 Oct 2024\nProb (F-statistic):\n1.00\n\n\nTime:\n19:39:53\nLog-Likelihood:\n274.71\n\n\nNo. Observations:\n100\nAIC:\n-547.4\n\n\nDf Residuals:\n99\nBIC:\n-544.8\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nx1\n2.168e-19\n0.000\n1.04e-15\n1.000\n-0.000\n0.000\n\n\n\n\n\n\nOmnibus:\n41.519\nDurbin-Watson:\n0.564\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n139.378\n\n\nSkew:\n1.374\nProb(JB):\n5.43e-31\n\n\nKurtosis:\n8.089\nCond. No.\n1.00\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nConsidering the tightness of all of the datapoints in Figure 3.2 to the \\(X'\\) / PC1 axis, along with the total lack of correlation between the variables, one could potentially discard or disregard the \\(Y'\\) / PCA2 axis for the purpose of modeling.\nAbsent the PCA transformation, one could discard the y-datapoints and retain solely the x datapoints and the linear regression line equation \\(y=mx+b\\) between x and y such that one could re-calculate the y-value on-the-fly.\nBy performing PCA and transforming the variables in lieu of performing a recalculation of the y-value using the regression equation, the user can simply assume y to be zero in all cases and simply disregard the value. This allows the user to completely eliminate the feature while simultaneously retaining a high degree of explainability of variance within the data.\nThe eigenvalues of a PCA conveys the importance, and almost weight, of its corresponding eigenvector in explaining variation within the data. The eigenvalues are calucated from the covariance matrix of the original data, and then sorted in descending order, and then the corresponding eigenvectors, or basis vectors, are calculated and stored in a matrix. From here, eigenvalues and eigenvectors are pruned from the calculation based upon the needs of the user.\nThe dot product of the remaining eigenvectors and the original data then produce the PCA-transformed data.\nThe ratio of an individual eigenvalue over the sum total of all eigenvalues corresponds to an aforementioned weight or importance the corresponding eigenvector holds. This ratio corresponds to the amount of variance that is explained by the eigenvector and eigenvalue within the source data.\nIn best-case scenarios, one may have a high volume of dimensions, and many of those dimensions may have connections, correlations, or generally trend together. Due to the sheer volume thereof, it’s near impossible to visually inspect, determine, and prune correlated features from data. PCA, in the best case, allows a researcher to mathematically identify and trim all such correlations from the data, and capture all variation of the data within a fraction of the original dimensions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#pca-in-this-study",
    "href": "pca.html#pca-in-this-study",
    "title": "3  Principal Component Analysis",
    "section": "3.2 PCA in this Study",
    "text": "3.2 PCA in this Study\nThis study will explore PCA of select quantitative features of the cleaned dataset. While this will be explored, it is not necessarily ideal for the purpose and intent of this study.\nWhen leveraging principal components, one loses a degree of explainability for the data, and one arrives at a frame of reference that is not fully intuitive or easily digested. Each of the newly aligned axes, or principal components, is a linear combination of the original axes, and each datapoint is a combination of multiple features (e.g. the sum of 0.8 times feature A, 3.7 times feature B, 2.6 times feature C, as an arbitrary example). This makes the inputs and outputs less interpretable under direct observation.\nThis study is exploring the impact of categorical and numeric features and the strength of their predictive power in determining results or sources, i.e. \n\nCan one better predict mortgage outcomes (e.g. interest rate, approval or denial) from HMDA data when protected classes are included as predictors?\nCan one predict ones protected classes when using other available data about properties of the loan and the property it would purchase?\nHow strongly do these features lend themselves to such predictions?\n\nWith research questions like these in mind, performing PCA is less beneficial to explaining the outcomes.\nIf the HMDA data were to be used in conjunction with additional numerical information on the potential borrower, however, leveraging PCA could be of benefit. For instance, if additional features about the borrower such as total liquid savings, total invested dollars, credit score, age of credit history, and a substantial volume of other numeric variables, leveraging PCA could be beneficial in supporting assessment and analysis.\nIf a PCA of all of those numerics reduced the volume or dimension of the original data from, perhaps 20 to 5 features, it would simplify the process of training machine learning models. Whether building a multi-layer perceptron, performing a grid search with cross-validation of multiple model hyperparameters for models like logistic regression, support vector machines, ridge classifiers, or others - the reduction in dimensions reduces the computational time and complexity required to attain a more optimally-performing model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#data",
    "href": "pca.html#data",
    "title": "3  Principal Component Analysis",
    "section": "3.3 Data",
    "text": "3.3 Data\nTo perform PCA, there are a few requirements to ensure that the outcomes are relevant, valid, and (potentially) useful:\n\nThe data must not contain any labels (e.g. the dependent or response variable)\nThe data must include solely numeric data\n\nThe data must be standard scaled by the formula \\(\\frac{x-\\mu}{\\sigma}\\) where \\(x\\) is the variable in question, \\(\\mu\\) is the mean of \\(x\\), and \\(\\sigma\\) is the standard deviation of \\(x\\)\n\nFailure to perform this standard scaling will allow features with larger magnitudes to have a stronger impact on the outcome. To compare variances and the degree of explained variance, all features need to be on the same scale for comparison during PCA.\n\nThere are variables that may be represented as numbers in a source dataset, but the numbers leveraged in PCA must truly be numbers\nRemapping of categorical data to numbers cannot be performed\n\nWhen reducing dimensions in the transformed data, seek to retain a high degree of cumulative explained variance with the minimum number of dimensions required to achieve it\n\nWith these things in mind, only a small subset of the columns from the cleaned and consolidated HMDA dataset meet the numeric requirement for PCA. These columns include:\n\n\n\n\nTable 3.3: Numeric Columns Leveraged for PCA\n\n\n\n\n\n\n\n\nvariable\n\n\n\n\nproperty_value\n\n\nlender_credits\n\n\ndiscount_points\n\n\norigination_charges\n\n\ntotal_loan_costs\n\n\nproperty_value\n\n\nincome\n\n\ntotal_units\n\n\ntract_median_age_of_housing_units\n\n\ntract_minority_population_percent\n\n\nloan_to_value_ratio\n\n\ndebt_to_income_ratio\n\n\n\n\n\n\n\n\nFrom the initial efforts in data collection, all of these columns hold numeric and non-null data points to support calculation.\nThe data used to perform PCA is located here.\nData before Standard Scaling is as follows:\n\n\n\n\n\n\n\nincome\ninterest_rate\ntotal_loan_costs\nloan_to_value_ratio\nloan_term\nintro_rate_period\ntotal_units\ntract_minority_population_percent\ntract_population\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\n\n\n\n\n95.500000\n4.250000\n18615.000000\n85.000000\n360.000000\n120.000000\n1\n17.150000\n5599\n130.240000\n2248\n2492\n36\n\n\n0.000000\n4.250000\n5907.500000\n21.429000\n360.000000\n120.000000\n1\n13.370000\n3911\n292.450000\n1239\n142\n0\n\n\n607.000000\n5.250000\n3055.000000\n80.000000\n360.000000\n84.000000\n1\n35.670000\n10542\n292.450000\n753\n48\n0\n\n\n202.000000\n5.125000\n7322.120000\n92.175000\n360.000000\n60.000000\n1\n40.330000\n22780\n276.610000\n5430\n6341\n12\n\n\n298.000000\n5.625000\n5325.000000\n65.574000\n360.000000\n84.000000\n1\n19.470000\n3344\n193.660000\n1061\n1128\n69\n\n\n\n\n\n(203321, 13)\n\n\n\n\narray([[-0.09470815, -3.22780156,  3.05686922, ...,  1.74589773,\n         1.28680469, -0.02822475],\n       [-0.292129  , -3.22780156, -0.05490293, ...,  0.0306856 ,\n        -2.06048815, -1.92834658],\n       [ 0.96268207, -1.80965799, -0.75341405, ..., -0.79547207,\n        -2.19437986, -1.92834658],\n       ...,\n       [ 0.1089144 , -0.03697853,  0.2657449 , ...,  0.98773657,\n         0.78399857, -1.40053496],\n       [-0.19703623,  0.14028942,  0.32635803, ..., -0.53028566,\n        -0.50933841,  0.55236803],\n       [-0.04612815,  0.14028942, -0.03244405, ...,  0.25167428,\n         0.03335035,  0.44680571]])\n\n\n(203321, 13)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#code",
    "href": "pca.html#code",
    "title": "3  Principal Component Analysis",
    "section": "3.4 Code",
    "text": "3.4 Code\nThe code used to perform PCA is located here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#principal-component-analyses",
    "href": "pca.html#principal-component-analyses",
    "title": "3  Principal Component Analysis",
    "section": "3.5 Principal Component Analyses",
    "text": "3.5 Principal Component Analyses\n\n3.5.1 2D PCA\n\n\n\n\nTable 3.4: 2D PCA Calculation\n\n\n\n\n\n\n\n\nPC\nEigenvalues\nCumulative Variance\n\n\n\n\nPC1\n2.772669\n0.213281\n\n\nPC2\n1.477353\n0.326923\n\n\n\n\n\n\n\n\nThe 2D PCA achieves an explanation of approximately 32.6% of the variance in the source data.\n\n\n\n\n\n\n\n\nFigure 3.3: 2D PCA Visualization Plot\n\n\n\n\n\n\n\n3.5.2 3D PCA\n\n\n\n\nTable 3.5: 3D PCA Calculation\n\n\n\n\n\n\n\n\nPC\nEigenvalues\nCumulative Variance\n\n\n\n\nPC1\n2.772669\n0.213281\n\n\nPC2\n1.477353\n0.326923\n\n\nPC3\n1.225321\n0.421178\n\n\n\n\n\n\n\n\nThe 3D PCA achieves an explanation of approximately 42.1% of the variance in the source data.\n\n\n\n\n\n\n\n\nFigure 3.4: 3D PCA Visualization\n\n\n\n\n\n\n\n3.5.3 Multi-Dimensional PCA\n\n\n\n\nTable 3.6: Eigenvalues and Variance, all dimensions\n\n\n\n\n\n\n\n\nPC\nEigenvalues\nCumulative Variance\n\n\n\n\nPC1\n2.772669\n0.213281\n\n\nPC2\n1.477353\n0.326923\n\n\nPC3\n1.225321\n0.421178\n\n\nPC4\n1.123168\n0.507575\n\n\nPC5\n1.047675\n0.588165\n\n\nPC6\n0.976401\n0.663273\n\n\nPC7\n0.967920\n0.737728\n\n\nPC8\n0.931685\n0.809395\n\n\nPC9\n0.810819\n0.871766\n\n\nPC10\n0.700385\n0.925641\n\n\nPC11\n0.633995\n0.974410\n\n\nPC12\n0.218066\n0.991184\n\n\nPC13\n0.114606\n1.000000\n\n\n\n\n\n\n\n\nThe top three eigenvalues / eigenvectors are highlighted in each of Table 3.4, Table 3.5, and Table 3.6, of 2.77, 1.477, and 1.225.\nTo retain a minimum of 95% of the information in the dataset, the minimum required principal components are 11, allowing for dimensionality reduction of 2 dimensions from the original 13 while retaining the required amount information. The lowest value greater than 0.95 in the cumulative variance column is on the 11th row for Principal Component 11 at 0.974, thus one would have to include all components 1-11 to achieve at least 95% explained variance.\nThis finding suggests that the source data do not have strong correlations with one another, and thus do not provide much assistance in reducing dimensionality.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#multiple-correspondence-analysis",
    "href": "pca.html#multiple-correspondence-analysis",
    "title": "3  Principal Component Analysis",
    "section": "3.6 Multiple Correspondence Analysis",
    "text": "3.6 Multiple Correspondence Analysis\nMany of the research questions within this effort are inherently linked to categorical factors in lieu of numeric factors. Principal component analysis is only executable upon numeric data and not upon categorical data - even in the case of ordinal data. As a simple example, consider a ordinal variable “size” with categories small, medium, large, and extra-large. One could apply a simple encoding and assign small=1, medium=2, large=3, and extra-large=4. This encoding, while apparently holding a degree of validity in terms of increasing size, does not match up mathematically to reality. Consider getting a fountain drink at a fast-food restaurant and ask the question - is a large the same as 3 smalls? Is an extra large the same as 1 medium and two smalls? Rarely are either of these answers “yes”. One might have to add decimal places to the categories, and at that point, one may as well get the exact size measurements in terms of fluid ounces or liters, which may or may not be possible.\nThe additive and multiplicative challenges between these categories when assigning them a value produces challenges for ordinal variables. These challenges are further confounded when pivoting away from an ordinal variables. One runs the risk of making mathematical claims such as red is 4 times blue, or that sad is 3 less than happy. Such statements are nonsensical, have no foundation in mathematics, and while they may produce results in a model post-transformation, do not hold validity, explainability, or generalizability.\nEnter Multiple Correspondence Analysis (or MCA). MCA performs an analogous action on categorical variables as PCA performs upon numeric variables. To perform an MCA, one must construct a Complete Disjunctive Table, which is effectively a one-hot encoded matrix. One takes the source categorical columns and transforms them to a column per category, and for the new column, the value is set to 1 if the current row is a member of the category, and zero otherwise. This is repeated for all columns and categories until the dataset is fully expanded.\n\n\n\n\n\n\n\na\nb\n\n\n\n\ns\nf\n\n\nm\nw\n\n\ns\ns\n\n\ns\nf\n\n\nm\nf\n\n\nm\nf\n\n\nl\nw\n\n\nl\ns\n\n\nl\nf\n\n\nm\nw\n\n\n\n\n\nTaking the above example table, one can transform it to a one-hot encoded table:\n\n\n\n\n\n\n\na_l\na_m\na_s\nb_f\nb_s\nb_w\n\n\n\n\n0\n0\n1\n1\n0\n0\n\n\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n1\n0\n\n\n0\n0\n1\n1\n0\n0\n\n\n0\n1\n0\n1\n0\n0\n\n\n0\n1\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n0\n\n\n0\n1\n0\n0\n0\n1\n\n\n\n\n\nNotice that there are now have 6 columns from the original 2 columns. This is because column ‘a’ had 3 categories - s/m/l, as did column ‘b’ - s/w/f. A column is created for each combination of individual columns and their respective categories, hence 6 columns in this case.\nAfter performing this transformation, the following mathematical operations are applied:\n\nCalculate the sum of all values (0s and 1s) from the CDT as value \\(N\\)\nCalculate matrix \\(Z = \\frac{CDT}{N}\\)\nCalculate the column-wise sum as matrix \\(c\\). Transform to a diagonal matrix \\(D_c\\)\nCalculate the row-wise sum as matrix \\(r\\). Transform to a diagonal matrix \\(D_r\\)\nCalculate matrix \\(M = D_r^{-\\frac{1}{2}}(Z-rc^T)D_c^{-\\frac{1}{2}}\\)\n\nDue to some unforeseen challenges during this research, performance of MCA will be delayed until a later date. This type of analysis will be useful for future modeling purposes and further analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#results",
    "href": "pca.html#results",
    "title": "3  Principal Component Analysis",
    "section": "3.7 Results",
    "text": "3.7 Results\nPrincipal component analysis on the source data for this research is not an ideal endeavor, as this effort seeks to further establish the strength and connection of certain numeric and categorical variables to the outcome of whether or not an applicant will attain a mortgage.\nFurthermore, the need of the data to retain 11 of the 13 principal components to explain most of the data variation means that applying PCA to this data will not meet any dimensionality reduction goals for this research. There is little direct correlation between the variables in the source data, so it takes almost the same number of dimensions as we had in the source data to retain a high degree of explainability. As such, principal component analysis, solely performed on the identified numeric variables, may be insufficient for the purposes of clustering and modeling.\nThe multiple correspondence analysis, however, seems to lend itself well to the purposes and intent of this research. A limited degree of exploration into MCA was pursued, but not sufficiently enough to generate or communicate results at this time. In the next iteration of this research, MCA will be included in the analyses. MCA, used within clustering and potentially within modeling, could be more relevatory than basic numeric measures provided by PCA.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "4  Data Clustering",
    "section": "",
    "text": "4.1 Overview\nClustering is a combination of mathematical methods for identifying potential groupings or similarities between records within a dataset. This research explores use of K-Means, hierarchical, and density based clustering methods to examine the numeric data that was produced in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#overview",
    "href": "clustering.html#overview",
    "title": "4  Data Clustering",
    "section": "",
    "text": "4.1.1 K-Means\nK-Means clustering is a partitional clustering method that identifies and divides the data into distinct groups or clusters by using the expectation maximization algorithm. Conceptually, the algorithm is simple to understand and explain:\n\npick the number of clusters you wish to identify, k\nselect k-random points within the range of the data to serve as the initial cluster centers\nmeasure the distance from each point \\(x_i\\) to each of the \\(k_i\\) centers\nassign the current point \\(x_i\\) as being a member of center \\(k_i\\) if the distances between \\(x_i\\) and \\(k_i\\) is lower than the distance from \\(x_i\\) to any other center \\(k\\)\nfor every center \\(k\\), recalculate its location by taking the average of every point \\(x_i\\) that has been assigned as a member of \\(k\\).\nrepeat steps 4-6 above until either the centers stop moving beyond a certain threshold or until a desired number of iterations has been reached\n\n\n\n4.1.2 Density-Based Clustering\nDensity based clustering, like Density-Based Spatial Clustering of Applications with Noise or DBSCAN, can be used to find and identify clusters with more complex shapes and boundaries than that of partitional clustering methods like K-Means.\nDBSCAN operates under a different paradigm. Where K-Means operates off of distance between points and a center, DBSCAN operates off of distance between every point to determine how many points are in its neighborhood. As such, DBSCAN has two primary parameters, a maximum distance and a minimum number of points. To measure density, in general, one seeks to measure how many items are within a limited amount of space.\nDBSCAN measures a point’s density by how many other points are within the established maximum distance from that point. If that point meets or exceeds the established minimum number of points, then the point is considered a dense point or part of the core. Otherwise, the point is not considered dense. This action is measured and performed for every datapoint.\nAfter the measurements are performed, a dense point is chosen at random to be assigned as a member of a cluster. Then, all of its dense neighbors are assigned to be members of the same cluster. This process continues until all dense neighbors within a cluster are assigned as members of the cluster. Then, any non-dense points that directly neighbor dense points within the cluster are also assigned to the cluster. Non-dense, non-neighboring points are not assigned at this time.\nThe above process is repeated for other dense points that were not assigned as part of the first cluster until all dense points (and all non-dense points that neighbor dense clusters) are assigned as members of a cluster. Any remaining non-neighboring non-dense points are then assigned as members of an outlier cluster (typically denoted by -1 in most density clustering algorithms).\nBecause of the concept of this snaking and neighboring point adjacencies by density, DBSCAN can identify and connect points in a complex manner that K-Means cannot.\n\n\n4.1.3 Hierarchical Clustering\nHierarchical clustering is a methodology that allows one to divide or aggregate datapoints within a dataset based upon either a desired number of overarching clusters, or by a distance metric, and building a hierarchy to outline the cluster in which a datapoint belongs. Hierarchical clustering can be approached via aggregating many small clusters (agglomerative) or by dividing the data into progressively smaller clusters (divisive).\nHierarcical clustering can be of benefit for highly dimensional data. \n\n\n4.1.4 Clustering In This Study\nClustering is explored in this study to ascertain any connection between the source data variables and the outcomes of each mortgage application. If clusters could be produced that map data points to their outcomes (almost in a predictive manner), this could be of benefit, even if the cluster label is only sometimes correct. The cluster label could be used as a numeric factor in modeling to support improved predictive accuracy.\nFurthermore, the clustering will be used (when MCA analysis is complete) to evaluate the impact and influence of protected class variables when attempting to marry clustering results to loan outcomes. If protected classes are impactful and can effectively cluster a loan application to its outcome, this may warrant further investigation as to why.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#data",
    "href": "clustering.html#data",
    "title": "4  Data Clustering",
    "section": "4.2 Data",
    "text": "4.2 Data\nThe source data available for clustering in this research contains over 200k records. Hierarchical and density-based clustering methods both take an extensive amount of memory resources. In order to perform these clustering methods on the available data, a stratified sample (on outcome) will be taken from the data and these clustering methods will be performed on the samples. The label is temporarily added to the PCA-transformed data, a stratified sample of approximately 20% of the dataset is then taken, and clustering is performed upon this sample.\nThe data used to perform clustering is based solely on the 3D PCA performed on numeric variables. The 3D PCA transformed dataset is located here. The steps to produce this dataset are covered in Chapter 3. Below is the head of the dataframe of principal components used for clustering. This dataframe is the 20% sample that is leveraged for density-based and hierarchical clustering.\n\n\n\n\n\n\n\nPC0\nPC1\nPC2\n\n\n\n\n-0.892240\n-0.448016\n-0.182751\n\n\n-0.931789\n-0.758654\n-1.987403\n\n\n-0.790594\n-0.409078\n1.276530\n\n\n-2.012753\n0.254438\n1.666342\n\n\n0.755317\n2.409991\n1.879752\n\n\n\n\n\nBefore the completion of this research, clustering will also be performed on MCA-transformed data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#code",
    "href": "clustering.html#code",
    "title": "4  Data Clustering",
    "section": "4.3 Code",
    "text": "4.3 Code\nThe code to clean and prepare the data for clustering is located here and written in Python. The code will include MCA transformations in the future. Additionally, this code leveraged an example from Scikit-Learn to best identify the number of clusters to use in K-Means for the PCA-transformed data.\n\n4.3.1 K-Means Clustering\nThe silhouette method was leveraged to identify several cluster options for K-Means using the 3D PCA dataset. This uses silhouette scores to determine how well datapoints are mapped to clusters and how well separated the clusters are within the data.\nUsing this method the best performing cluster counts included k=2, k=4 and k=5\n!\nK=2 K-Means Clustering had a silhouette score of 0.256.\n!\nK=4 K-Means Clustering had a silhouette score of 0.362.\n!\nK=5 K-Means Clustering had a silhouette score of 0.381.\n\n\n4.3.2 Hierarchical Clustering\nHierarchical clustering was performed on the 3d-transformed PCA data produced in Chapter 3. The hierarchy produced 2 main clusters, which may be beneficial for a binary classification problem such as predicting loan approval or denial. If the clusters had some ties to the actual grouping of approvals or denials, then this could be of benefit.\nHowever, as mentioned in Chapter 3, the 3d transformation of numeric-only data resulted in an explained variance of only about 43%. As such, the two produced clusters are unlikely to have any direct ties to the actual outcomes in the source data.\n!\nThe dendrogram is challenging to interpret in this case, as it’s splitting the data on a roughly PCA-transformed dataset with limited explainability in the data’s variance. Furthermore, the base labels of the dendrogram are simply based upon the datapoints indices themselves, and as such the dendrogram doesn’t add substantial information or interpretability in this context. Performing a plot of the points and their classifications will likely be a more fruitful endeavor.\nFurthermore, the silhouette score for the hierarchical clustering was 0.266, which is fairly low. Generally, a score closer to zero signifies indifference or poor matching and grouping of the data into potentially meaningful clusters. That being said, the performance of hierarchical clustering in comparison to K-Means pales in comparison. Even the worst performing K-means cluster of K=2 was approximately equivalent to hierarcical clustering (0.256 vs. 0.266).\n\n\n4.3.3 Density Clustering (DBSCAN)\nDensity-based clustering can support a researcher in identifying nested, non-uniformly sized or shaped clusters in multiple dimensions. Density can also be leveraged to cluster and group data together while isolating outliers that are in low-density regions. By examining points in the source data and measuring point density and distance, points continuously meeting the density criteria are added to the same cluster.\nPoints failing to meet the density criteria can only be added to a cluster, but are not considered for the continuity criteria for extending the cluster. Some of these points are added to clusters, and others are considered outliers (e.g. when they are substantially distant from any cluster points).\nThis methodology can produce different size clusters in different dimensions, and follow paths and patterns within the data based upon point densities.\nPerforming density-based clustering using DBSCAN on the source data produced a silhouette score of approximately 0.41.\nThe density clustering also…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#results",
    "href": "clustering.html#results",
    "title": "4  Data Clustering",
    "section": "4.4 Results",
    "text": "4.4 Results\nWhat is the difference in the outputs for the data, though? And how do those compare to the actual underlying labels?\nTo visualize, we’ll take the K-Means clusters and apply the same subset of the data as used for Agglomerative and…?\n\n\n\n\n\n\n\n\nFigure 4.1: Summarized Cluster Results, all methods, (20% of source data)\n\n\n\n\n\nThe outcome of these clusters does not directly further the intent of this research. In part, this is likely due to the small degree of variance (42%) that is encapsulated within a 3D PCA. The silhouette score for the 3D PCA with its own labels is shockingly low at an approximate value of -0.0096, meaning that the nature of this data makes it difficult to perform unsupervised clustering and potentially arrive at meaningful labels that are connected or related to the actual labels.\nFurthermore, additional challenges are reflected by the silhouette scoring for both K-Means and agglomerative below 0.3, and DBSCAN with a silhoutte score of approximately 0.41. These relatively low scores signify that none of these clustering methods, performed on the available data, has an ability to establish clusters that are substantially far or distinct from one another.\nPerforming clustering on MCA-transformed data, which will encapsulate categorical values in an appropriate transformation to numeric values, will likely give a far more interesting cluster analysis. Since this research seeks to explore categorical variables and their potential impacts to outcome of loan applications, this transformation will be performed, analyzed, and potentially used in the models for this research.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#conclusion",
    "href": "clustering.html#conclusion",
    "title": "4  Data Clustering",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nWhat do these technical results signify to the casual reader? The numeric data alone in HMDA records may not be sufficient to examine trends, interesting patterns, or predictions (such as clusters) within US mortgage applications. This could be due to data that is available, but not included in the attempts to cluster (such as categorical variables or the reduced dimensions), or data that is unavailable within HMDA records - outside influencers or causes that place a mortgage application into the “approve” or “deny” pile.\nThe K-Means clustering results at multiple levels didn’t produce strong, distinct, or separated clusters. Hierarchical clustering performed better in terms of cluster distinctness, but fails to resemble the actual outcomes for those loan applications. Density clustering also failed to produce anything resembling the source outcomes.\nNone of the clustering methods produced labels that resembled actual outcomes for each application. Additionally, examining the actual labels, there’s a fairly evident interspersing of the outcomes (rejection and approval) amongst one another in the source data, as is clear in Figure 4.1. With data in this form and quality, it is unlikely that a clustering method could differentiate between interesting groups within such a homogenous space.\nProducing MCA transformations of categorical variables in combination with these numeric variables is the best chance this research will have to seek out more meaningful clusters in comparison to the loan outcomes of approval and denial.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "5  Association Rule Mining",
    "section": "",
    "text": "5.1 Overview\nAssociation Rule Mining, henceforth ARM in this section, is the process of identifying commonly occuring combinations within a dataset. The method allows one to explore and uncover potentially unknown associations and groupings in data that are not necessarily immediately evident upon a direct human inspection.\nA common application of these methods is that of assessing common purchases by customers in a store or e-commerce website (e.g. if a customer buys a portable music player, is it common or frequent for those customers to purchase a USB cable, or Bluetooth headphones, or potentially both?). Manually inspecting or searching through the data with ad-hoc or independently built algorithms may not be effective or efficient to identify such patterns. When considering an application such as customer purchases, or transactional data, the possibility of combinations of items in each purchase is conceptually unbounded, and using brute-force methods to seek out these combinations will exceed the available computational resources of a machine before identifying anything of use.\nSo, to examine transactions for potentially insightful or useful metrics, one needs to go about said search wisely and efficiently. This is where a handful of algorithms and methods come into play, including Frequent Pattern growth and Apriori.\nWhat is a frequent pattern?\nA frequent pattern is defined as a collection of items that occur at or above a specified threshold within a dataset of transactions. When measuring frequent patterns, each item in the collection is unique, and multiple instances of the same item within an individual transaction are ignored so as to bring focus on the occurence of unique items being grouped together within the dataset. Since the threshold can be specified by the person conducting association rule mining, frequent is a relative and subjective term in this context, as it is based on the relative frequency of occurence of the group of items, and whether or not that relative frequency exceeds an arbitrary threshold.\nCommon items that are connected, grouped, or bundled together are be referred to as frequent patterns within datasets.\nHow does apriori work?\nThe Apriori algorithm leverages bayesian probability and induction to… To perform apriori ARM, one must set thresholds for the metric for which one is measuring the data. This is key, because depending on the number of unique items or the number of transactions within the dataset in question, the\nTo search for these connections and associations, the method leverages Bayes rule for probability, metrics such as entropy or Gini indices within the Apriori algorithm. The algorithm iteratively and inductively examines data for frequent patterns, generally, in the following manner -\nSo one can see that as the apriori algorithm proceeds, it does have an eventual halting point, depending on the initially established thresholds. The predominant metric and threshold is the relative frequency of occurence of an item (or combination of items). This relative frequency is also known as the support of the combination of items within the dataset.\nWhat is an association rule?\nAn association rule goes beyond individual relative frequencies of items or combinations thereof, and begin telling more about how strongly connected certain item combinations are within the data. ARM establishes a connection between an antecedent (or prior) and a consequent (or posterior) set of items within the dataset. The combination of antecedents and consequents are what form assosciation rules. It is similar to asking the question, given a customer has already placed items \\(A\\) and \\(B\\) in their shopping cart, what is the probability or likelihood that they will next place \\(C\\) in their basket? Knowing relative frequencies of individual items and combinations thereof across many transactions is necessary to answer this question, but does not necessarily answer how certain or strong those associations are.\nHow do we measure the strength of the rules?\nSupport has already been discussed in this section as the relative frequency of occurance of a set of items wihtin source data. Strength of association rules are measured with metrics including support, confidence, and lift.\nConfidence and lift tell us the most about the strength of a rule. High confidence (ranging from 0 to 1, with 1 being the highest) tell us how often this collection of items occurs.\nHow does one interpret association rules?\nARM does not establish causal relationships between antecedents and consequents. It is a frequentist method to examine relative probabilities within transaction data. When interpreting association rules, one can comment on the strength of identified rules using metrics like confidence and lift. Lift values for association rules are tantamount to pearson R correlation values with some differences in the range of the potential result:\nWith this similarity to correlation, one can interpret mined rules with high lift and high confidence with statements such as “Customers who buy \\(A\\) and \\(B\\) almost always buy \\(C\\).” And similarly, for a very low lift and high confidence, “Customers who buy \\(A\\) rarely if ever also buy \\(B\\).” One should not interpret mined rules in such manners as “Customers buy \\(B\\) because they bought \\(A\\)” or that “Customers who have \\(A\\) and \\(B\\) need \\(C\\).” The later statements are causal in nature, and such relationships are not established via ARM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#overview",
    "href": "arm.html#overview",
    "title": "5  Association Rule Mining",
    "section": "",
    "text": "Calculate or determine the relative frequency of item groups or sets of length 1 (based on input threshold).\nPrune items that do not meet the input threshold from further consideration.\nFor patterns of length \\(n\\), examine combinations of frequent patterns with length \\(n-1\\) and length 1 (e.g. for length 2, combine frequent item sets of length 1 and 1), and calculate those combinations’ relative frequencies.\nRetain only sets of items that meet the initially established threshold.\nRepeat steps 3 and 4 until no more sets of items of length \\(n\\) meet the specified threshold.\n\n\n\n\n\n\n\n\nConfidence - How often the items A and B occur together given the number of times A occurs. Helps us in that if someone is just buying A and B together and not C, we can rule out C at that point in time. \\(P(B|A) = \\frac{P(A\\cap B)}{P(A)}\\)\nOne can define a threshold for mininum support and confidence as initial parameters when beginning to build association rules. Once set these values are set, they serve as a filter that adjusts the number of rules that are found, and helps determine how long or specific those rules can be. Generally, since the algorithm is inductive, lengthy rules are rare (e.g. will have low support). By setting lower initial thresholds for support, more rules can be mined.\nLift - gives us the indepdendent occurence probability of item A and B. We observe that there is alot of between this random occurence and association. \\(\\frac{P(A\\cap B)}{P(A)\\cdot P(B)}\\)\n\nThe calculation of lift is based upon the assumption of statistical independence - \\(A\\) and \\(B\\) are indpendent \\(\\iff\\) \\(P(A\\cap B) = P(A)\\cdot P(B)\\). So, with the fraction \\(\\frac{P(A\\cap B)}{P(A)\\cdot P(B)}\\), it transforms the calculation in such a way we can garner important insight.\nLift values equal to 1 signify item occurences that are independent of one another.\nLift values greater than 1 are akin to saying the sum is greater than its parts, and gives greater creedence to a calculated confidence value. The higher lift is, the more assurance that we have that the confidence is meaningful and impactful.\nLift values less than 1 signify that there is an inverse relationship between the items in question, and that having one actually reduces the chances of the other occuring. The closer to zero this value approaches, the stronger the inverse relationship is.\n\n\n\n\n\na lift value substantially higher than 1 is analogous to a high, positive Pearson R value close to +1\na small lift value, very close to 0, is analogous to a low, negative Pearson R value close to -1\na lift value of 1 is analogous to a Pearson R value of 0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#arm-within-this-study",
    "href": "arm.html#arm-within-this-study",
    "title": "5  Association Rule Mining",
    "section": "5.2 ARM within this Study",
    "text": "5.2 ARM within this Study\nARM for the purpose of this study can help examine some of the findings from the CNN article, and help examine their findings as well as explore other research questions with respect to the top 5 lenders. Examining associations in which the consequent is either a result of loan approval or loan denial is of interest here. Furthermore, performing similar actions where antecedents include the specific financial institution, an individual or collection of protected class information, and other important features should be examined to pursue answers to the research questions established in Chapter 1.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#data",
    "href": "arm.html#data",
    "title": "5  Association Rule Mining",
    "section": "5.3 Data",
    "text": "5.3 Data\nApplying ARM to the collected HMDA mortgage data is somewhat of a challenge. The data itself is not necessarily organized in a way that is immediately conducive to searching for associations; it contains a mixture of quantitative and qualitative data. To perform ARM, we need transactional data - a list of all things that effectively went into the “basket” of each mortgage application. Additionally, numeric information is a detriment to identifying patterns, as any variable or feature that sits along an interval or continuous scale has countless possibile values which it can take on, and as such, identifying frequent patterns and results in the data may not be possible.\nGenerally, to prepare this mortgage data for use in ARM, a few actions were necessary to establish features as available and usable:\n\nperform discretization and binning of numeric variables into distinct categories\n\nnumeric variables were divided on percentile boundaries of width 20, including 0-20, 21-40, 41-60, 61-80, and &gt;80.\nwhich numeric vars?\n\nadd features of each mortgage application into a basket\ntransform the resulting baskets into a one-hot element frame of data\npivot data into single format (2 columns, transaction number and item)\n\nThe code to perform these transformations and prepare the data was written in Python and can be reviewed in Appendix C.\nPrior to performing the transaction transformation, the data is the same state as it was after initial collection:\n\n\n  state_code county_code       derived_sex action_taken purchaser_type\n1         OH       39153 Sex Not Available            1              0\n2         NY       36061              Male            1              0\n3         NY       36061 Sex Not Available            1              0\n4         FL       12011              Male            1              0\n5         MD       24031             Joint            1              0\n6         NC       37089             Joint            1              0\n\n\nExamples of the data, post transformation:\n\n\n  index                                variable\n1     0                                     3.0\n2     0 applicant_ethnicity:Not Hispanic/Latino\n3     0                            income:21-40\n4     0                      interest_rate:0-20\n5     0                    applicant_race:White\n6     0               loan_to_value_ratio:41-60\n\n\nThe transformed data can be found here",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#code",
    "href": "arm.html#code",
    "title": "5  Association Rule Mining",
    "section": "5.4 Code",
    "text": "5.4 Code\nThe code to prepare the data into single transaction format, execute the apriori algorithm, and measure metrics such as confidence, lift, and support, was written in R is located in Appendix C. Furthermore, the code is embedded, but hidden, within the quarto source code of this webpage, written in R. Examination of the source .qmd file will provide a view of the specific code used to generate the rules and visuals.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#results",
    "href": "arm.html#results",
    "title": "5  Association Rule Mining",
    "section": "5.5 Results",
    "text": "5.5 Results\nThe below tables and figures provide insight to mined association rules from the dataset. The overall first three tables, Table 5.1, Table 5.2, and Table 5.3 cover the top association rules when the totality of the dataset is mined via apriori.\nHowever, some other tables and figures are necessary to examine the individual institutions, as what is frequent for one institution may be infrequent for another. Being able to dive deeper on the individual institutions and the relative frequency of approvals and denials for their organizations is of interest to the intent of this research.\n\n\n\nTable 5.1: Top 15 Associations by Support\n\n\n\n     lhs                                           rhs                                         support confidence  coverage     lift  count\n[1]  {approve}                                  =&gt; {1 rooms}                                 0.8465587  0.9897986 0.8552837 1.001607 172124\n[2]  {1 rooms}                                  =&gt; {approve}                                 0.8465587  0.8566580 0.9882108 1.001607 172124\n[3]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {1 rooms}                                 0.7152005  0.9893591 0.7228928 1.001162 145416\n[4]  {1 rooms}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.7152005  0.7237327 0.9882108 1.001162 145416\n[5]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6253676  0.8650905 0.7228928 1.011466 127151\n[6]  {approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6253676  0.7311815 0.8552837 1.011466 127151\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                                                             \n      approve}                                  =&gt; {1 rooms}                                 0.6195837  0.9907512 0.6253676 1.002571 125975\n[8]  {1 rooms,                                                                                                                             \n      applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6195837  0.8663077 0.7152005 1.012889 125975\n[9]  {1 rooms,                                                                                                                             \n      approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6195837  0.7318852 0.8465587 1.012439 125975\n[10] {applicant_race:White}                     =&gt; {1 rooms}                                 0.6052911  0.9917401 0.6103324 1.003571 123069\n[11] {1 rooms}                                  =&gt; {applicant_race:White}                    0.6052911  0.6125121 0.9882108 1.003571 123069\n[12] {aus:Desktop Underwriter}                  =&gt; {1 rooms}                                 0.5645626  0.9910383 0.5696678 1.002861 114788\n[13] {1 rooms}                                  =&gt; {aus:Desktop Underwriter}                 0.5645626  0.5712977 0.9882108 1.002861 114788\n[14] {applicant_race:White}                     =&gt; {approve}                                 0.5349397  0.8764727 0.6103324 1.024774 108765\n[15] {approve}                                  =&gt; {applicant_race:White}                    0.5349397  0.6254529 0.8552837 1.024774 108765\n\n\n\n\nTable 5.1 outlines the overall top 15 mined rules by support, or relative frequency. One can see some relatively frequently occurring occurences here, however, this doesn’t mean they are useful or meaningful associations.\nFor instance, examining rule #14 with an untrained eye would be immediately concerning, as it seems 53% of the time, White applicants are simply approved because they are White. However, examining the lift of this rule being quite close to 1, this is actually a weak association within this data. Recalling that a lift value equal to 1 means that A and B are independent. While this value is greater than one, as are all values in Table 5.1, all of them are very close to 1. As such, every rule in this table is a weak association and simply a result of frequency of presence in the data.\n\n\n\nTable 5.2: Top 15 Associations by Confidence\n\n\n\n     lhs                                         rhs            support confidence   coverage     lift count\n[1]  {interest_rate:&gt;80}                      =&gt; {approve}   0.18578904          1 0.18578904 1.169203 37775\n[2]  {aus:Loan Prospector/Product Advisor,                                                                  \n      aus:Other}                              =&gt; {JP Morgan} 0.15580213          1 0.15580213 4.971320 31678\n[3]  {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:61-80}              =&gt; {approve}   0.04241056          1 0.04241056 1.169203  8623\n[4]  {interest_rate:&gt;80,                                                                                    \n      tract_minority_population_percent:0-20} =&gt; {approve}   0.04197283          1 0.04197283 1.169203  8534\n[5]  {debt_to_income_ratio:61-80,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04571074          1 0.04571074 1.169203  9294\n[6]  {interest_rate:&gt;80,                                                                                    \n      tract_to_msa_income_percentage:21-40}   =&gt; {approve}   0.04036454          1 0.04036454 1.169203  8207\n[7]  {debt_to_income_ratio:21-40,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04220891          1 0.04220891 1.169203  8582\n[8]  {Female,                                                                                               \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04209579          1 0.04209579 1.169203  8559\n[9]  {2.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04509104          1 0.04509104 1.169203  9168\n[10] {1.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.05827702          1 0.05827702 1.169203 11849\n[11] {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:21-40}              =&gt; {approve}   0.05279802          1 0.05279802 1.169203 10735\n[12] {interest_rate:&gt;80,                                                                                    \n      Joint}                                  =&gt; {approve}   0.06351993          1 0.06351993 1.169203 12915\n[13] {interest_rate:&gt;80,                                                                                    \n      Male}                                   =&gt; {approve}   0.06214773          1 0.06214773 1.169203 12636\n[14] {aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.08140290          1 0.08140290 1.169203 16551\n[15] {interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                        =&gt; {approve}   0.10214832          1 0.10214832 1.169203 20769\n\n\n\n\nIn Table 5.2, we have a bit more of a mixed bag of results. The strongest rule is actually the assocation that JP Morgan uses Loan Prospector/Product Advisor and Other Automated underwriting systems for their loan applications, moreseo than any other lender, for 2023 data. This rule (rule #2) has a substantially high value for lift (close to 5) and a confidence of 1, whereas nearly all other rules in this top-15 list are much closer to 1 in terms of lift.\nAll of these rules have greater lift than the rules mined in Table 5.1, and thus have more utility. However, most are still very close to 1 and not incredibly strong, less rule #2. As such, many of these rules are frequent, but not necessarily meaningful beyond their relative frequency.\n\n\n\nTable 5.3: Top 15 Associations by Lift\n\n\n\n     lhs                                                rhs                                      support confidence   coverage     lift count\n[1]  {applicant_ethnicity:Mexican}                   =&gt; {applicant_ethnicity:Hispanic/Latino} 0.04056128  0.9169446 0.04423525 8.207934  8247\n[2]  {applicant_ethnicity:Hispanic/Latino}           =&gt; {applicant_ethnicity:Mexican}         0.04056128  0.3630800 0.11171442 8.207934  8247\n[3]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07532387  0.7362273 0.10231062 7.834365 15315\n[4]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07603703  0.7353151 0.10340740 7.824658 15460\n[5]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06595941  0.7333224 0.08994600 7.803453 13411\n[6]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06651026  0.7322395 0.09083129 7.791930 13523\n[7]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05100776  0.7264131 0.07021867 7.729930 10371\n[8]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05134712  0.7252518 0.07079903 7.717572 10440\n[9]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04512547  0.7246663 0.06227068 7.711341  9175\n[10] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04538122  0.7233459 0.06273792 7.697291  9227\n[11] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04939456  0.6718625 0.07351885 7.149444 10043\n[12] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07602227  0.6709350 0.11330795 7.139574 15457\n[13] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04969457  0.6703377 0.07413364 7.133218 10104\n[14] {applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07674034  0.6700880 0.11452278 7.130562 15603\n[15] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06656437  0.6685768 0.09956129 7.114480 13534\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Visualization of top 15 rules (by lift)\n\n\n\n\nIn Table 5.3 and Figure 5.1, one can begin to see some interesting patterns. The rules in this list are quite interesting. Of particular interest are rules #11 and #13, establishing assoications between approval and the absence of ethnic, racial, and gender information on a loan application. While other features are include (e.g. 1 bedroom home, specific underwriting systems), the association of these things together within the overarching dataset could be used to tell a compelling story.\nNamely, with 66-70% confidence, it’s possible that you may boost your chances to have a loan approval if you omit your demographic information, as such omissions and exclusions are associated with loan approvals. Similarly speaking, if the former is a true statement, it may also be true that one may reduce their chances for approval when including their personal demographic information on a loan application. And by further examining these rules, both of these claims may best hold true if the loan has been processed by Rocket Mortgage.\nFrom here, it’s of interest to examine association rules mined when the transactions are filtered to specific organizations. The reason for this is that, for a set threshold of support and confidence, certain interesting rules for a given organization may not be available for mining simply due to a lower volume of transactions processed via that organization. As such, by first filtering down transactions to a set organization and then exploring the rules mined for that organization at set common thresholds for support and confidence, more interesting information may arise.\n\n\n\n\n\n\n\n\n\nFigure 5.2: top 10 NFCU Denial Rules by Lift\n\n\n\n\nNFCU has rules including debt-to-income ratio being above 80th percentile and loan interest rate being between 41 and 60th percentile as common features for all of its top 10 rules by lift. A total of 4 of the top ten rules include race or ethnicity (Non-hispanic/latino and black/African American applicants).\n\n\n\n\n\n\n\n\nFigure 5.3: top 10 JP Morgan Denial Rules by Lift\n\n\n\n\nJP Morgan has the same common features of debt-to-income ratio being above 80th percentile and loan interest rate being between 41 and 60th percentile for its top 10 rules. JP Morgan has a single rule covering ethnicity for denial (non-hispanic/latino applicants with the other common criteria).\n\n\n\n\n\n\n\n\nFigure 5.4: top 10 Bank of America Denial Rules by Lift\n\n\n\n\nBank of America appears as an oddity here. The main common traits are a moderate interest rate (in the range of 21st to 40th percentile) and the lack of use of an underwriting system (e.g. they didn’t use any underwriting).\nSurprisingly, there are strong rules for BoA for denial of White and Non-hispanic/latino applicants.\n\n\n\n\n\n\n\n\nFigure 5.5: top 10 Wells Fargo Denial Rules by Lift\n\n\n\n\nWells Fargo corporation has no presence of association rules tied to ethnic, racial, gender, or age in its top-10 rules. Their denials seem to be predominantly mapped to lower proposed interest rates married with a high debt-to-income ratio.\n\n\n\n\n\n\n\n\nFigure 5.6: top 10 Rocket Mortgage Denial Rules by Lift\n\n\n\n\nWells Fargo has 3 rules in its top 10 by lift tied to ethnicity (Denial of non-Hispanic/Latino applicants with moderate interest rates).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#conclusions",
    "href": "arm.html#conclusions",
    "title": "5  Association Rule Mining",
    "section": "5.6 Conclusions",
    "text": "5.6 Conclusions\n\nThese association rules support this research going a degree beyond basic statistical analyses of source variables. In particular, cases of lift being over 1 are of interest, as it suggests that the antecedents contribute more probabilistically to the precedents. Once again, not necessarily tending to causation, but instead establishing a probabilistic and associative connection between the variables.\nExamining the results for the top 15 rules by lift, one sees an interesting occurence. Namely, the following set of items is frequent and strongly associated (examining the 11th rule):\n{ 1 rooms, applicant_ethnicity:Information Not Provided, applicant_race:Information not provided approve, aus:Desktop Underwriter} =&gt; {Sex Not Available}\nNamely, that mortgage approval is strongly associated with not having protected class information (sex, ethnicity, race) available or listed on a mortgage application.\nWith lift values exceeding 7 and confidence of 66%. Moreover, this 66% confidence corresponds to the concept that when all items in the antecedent are met, 66% of the time it is followed by the sex not being listed or available in the application.\nThis suggests, then, that it is quite likely to see applications where no demographic protected class information is provided or available for the applicant, and the application is not denied. This finding appears connected and linked to those of Figure 2.9, Figure 2.10, and Figure 2.11. What is further interesting is that the findings for each individual chart in initial exploration appear to merge together as rules within this association rule mining. While ARM does not establish or produce causal relationsips, the further depth of the relationships between these protected class variables is intriguing.\nHowever, some potentially concerning rules did arise for rocket mortgage (male or White or non-hispanic/latino), Wells Fargo (non-hispanic/latino ethnicity), Bank of America (White or non-hispanic/latino), and JP Morgan (non-Hispanic/Latino). While these rules appear to include other relevant financial or risk-based lending information (high debt to income ratio and insufficient interest rate on the loan, and other similar financial indicators that the applicant’s ability to repay may be at risk), these rules suggest that, at least on a frequentist basis, all lenders are more likely to deny loans to non-Hispanic/Latino applicants when they fall within these financial categories.\nThe findings in denial for NFCU rules #7 and #8 in Figure 5.2 high confidence and lift are consistent with the findings of CNN’s report from the end of 2023, when taking the organization by itself and when not comparing to other institutions. To compare all institutions for such a rule of denial of black applicants, here, ARM is run once more, focused across the entirety of the dataset with minimum support = 0.04, confidence = 0.01, and setting loan denial as the consequent.\n\n\n    lhs                                           rhs       support confidence  coverage      lift count\n[1] {applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04213514  0.3407446 0.1236561 2.3546500  8567\n[2] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04138263  0.3381153 0.1223921 2.3364812  8414\n[3] {applicant_race:White}                     =&gt; {deny} 0.07539273  0.1235273 0.6103324 0.8536119 15329\n[4] {1 rooms,                                                                                           \n     applicant_race:White}                     =&gt; {deny} 0.07414348  0.1224923 0.6052911 0.8464593 15075\n[5] {applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04111213  0.1185674 0.3467406 0.8193371  8359\n[6] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04052193  0.1176227 0.3445077 0.8128092  8239\n[7] {applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05616707  0.1111652 0.5052577 0.7681857 11420\n[8] {1 rooms,                                                                                           \n     applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05537030  0.1103444 0.5017952 0.7625140 11258\n\n\nIn the above table, all rules from the totality of the dataset that include race are listed. Examining these rules, one can clearly see that at the selected minimum confidence and support levels, there are no stand-outs in terms of specific organizations having high-lift high-confidence associations between a particular protected class and denial of loan applications. One can also see, however, that with limited confidence and moderate lift in rules #1 and #2, White applicants whose interest rates would be in the 41-60th percentile of 2023 interest rates tended to be denied. The remainder of the rules are not useful as they have lift values less than 1 and thus have negative assoications with one another.\n\n\n     lhs                                           rhs          support confidence   coverage     lift count\n[1]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11876236          1 0.11876236 1.169203 24147\n[2]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04705836          1 0.04705836 1.169203  9568\n[3]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Male}                                     =&gt; {approve} 0.04324667          1 0.04324667 1.169203  8793\n[4]  {applicant_race:White,                                                                                 \n      aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                        =&gt; {approve} 0.05488339          1 0.05488339 1.169203 11159\n[5]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                          =&gt; {approve} 0.06145424          1 0.06145424 1.169203 12495\n[6]  {applicant_race:White,                                                                                 \n      aus:Desktop Underwriter,                                                                              \n      interest_rate:&gt;80}                        =&gt; {approve} 0.08051760          1 0.08051760 1.169203 16371\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.09885305          1 0.09885305 1.169203 20099\n[8]  {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11780329          1 0.11780329 1.169203 23952\n[9]  {applicant_race:White,                                                                                 \n      interest_rate:0-20,                                                                                   \n      Rocket Mortgage}                          =&gt; {approve} 0.05066840          1 0.05066840 1.169203 10302\n[10] {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04677802          1 0.04677802 1.169203  9511\n\n\nSince metrics such as confidence and lift originate from somewhat Bayesian probability measurements, the performance of naive Bayes and Bernoulli Naive Bayes classification methods on the data could potentially be effective in terms of accuracy, recall, and precision for cases when age, gender, or race are not listed for an application. What would still remain in question is the degree to which predictive strength for those models is impacted by the presence of specific protected classes (instead of their absence) for establishing a link to the outcome of approval or denial of the application.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "app2.html",
    "href": "app2.html",
    "title": "Appendix B — Component Analysis Code (PCA and MCA)",
    "section": "",
    "text": "B.1 Module and Data Imports\nimport pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.cluster import KMeans\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom prince import MCA\n\nfr = pd.read_csv('../data/final_clean.csv')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Component Analysis Code (PCA and MCA)</span>"
    ]
  },
  {
    "objectID": "app2.html#module-and-data-imports",
    "href": "app2.html#module-and-data-imports",
    "title": "Appendix B — Component Analysis Code (PCA and MCA)",
    "section": "",
    "text": "B.1.1 Principal Component Analysis\n\nB.1.1.1 Select & Scale Numeric Columns\n\ncolumns = [\n    #'loan_amount',\n    #'property_value',\n    'income',\n    'interest_rate',\n    'total_loan_costs',\n    'loan_to_value_ratio',\n    #'origination_costs',\n    #'discount_points',\n    #'lender_credits',\n    'loan_term',\n    'intro_rate_period',\n    'total_units',\n    'tract_minority_population_percent',\n    'tract_population',\n    'tract_to_msa_income_percentage',\n    'tract_owner_occupied_units',\n    'tract_one_to_four_family_homes',\n    'tract_median_age_of_housing_units',\n    #'debt_to_income_ratio'\n]\n\nX = fr[columns]\n\nX = StandardScaler().fit_transform(X)\n\n\n\nB.1.1.2 Perform 2D PCA\n\npca2d = PCA(n_components=2)\nresult2d = pd.DataFrame(pca2d.fit_transform(X))\nresult2d['outcome']  = fr['outcome'].astype(bool)\n\ndisplay(\n    np.cumsum(pca2d.explained_variance_) #eigenvalues\n)\n\nsns.scatterplot(\n    data=result2d,\n    x=0,y=1,hue='outcome'\n)\nnp.cumsum(pca2d.explained_variance_ratio_)\n\narray([2.71077093, 4.35879383])\n\n\narray([0.20851998, 0.33529045])\n\n\n\n\n\n\n\n\n\n\n\nB.1.1.3 Perform 3D PCA\n\npca3d = PCA(n_components=3)\nresult3d = pd.DataFrame(pca3d.fit_transform(X))\nresult3d['outcome']  = fr['outcome'].astype(bool)\ndisplay(\n    np.cumsum(pca3d.explained_variance_) #eigenvalues\n)\nresult3d\nnp.cumsum(pca3d.explained_variance_ratio_)\n\narray([2.71077093, 4.35879384, 5.61886437])\n\n\narray([0.20851998, 0.33529045, 0.43221855])\n\n\n\nfig = plt.figure(figsize=(12,12))\nax = Axes3D(fig,rect=[0,0,.9,1],elev=5,azim=225)\n\nfig.add_axes(ax)\n\nx=result3d[0]\ny=result3d[1]\nz=result3d[2]\n\nax.scatter(x,y,z, cmap=\"RdYlGn\", edgecolor='k', s=40, c=fr['outcome'].astype(int))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nB.1.2 Multiple Correspondence Analysis\n\nmapper = {\n    'applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    'co-applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    'applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    'co-applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    'aus':{\n        'Desktop Underwriter':0b00000001,\n        'Loan Prospector/Product Advisor':0b00000010,\n        'TOTAL Scorecard':0b00000100,\n        'GUS':0b00001000,\n        'Other':0b00010000,\n        'Internal Proprietary':0b00100000,\n        'Not applicable':0b01000000,\n        'Exempt':0b10000000,\n    }, \n    # 'denial_reason':{\n    #     'DTI':0b0000000001,\n    #     'Employment History':0b0000000010,\n    #     'Credit History':0b0000000100,\n    #     'Collateral':0b0000001000,\n    #     'Insufficient Cash':0b0000010000,\n    #     'Unverifiable Information':0b0000100000,\n    #     'Credit Application Incomplete':0b0001000000,\n    #     'Mortgage Insurance Denied':0b0010000000,\n    #     'Other':0b0100000000,\n    #     'Not Applicable':0b1000000000\n    # }\n}\n\nnew_mapper = {}\nfor k,v in mapper.items():\n    new_mapper[k] = {}\n    #print(k)\n    for j,w in v.items():\n        #print(w,j)\n        new_mapper[k][w] = j\n\n#drop balloon payment, interest_only_payment, other_nonamortizing_features\n#income_from_median, \n\nfr.drop(\n    labels = [\n        'balloon_payment', \n        'interest_only_payment', \n        'other_nonamortizing_features',\n        'income_from_median',\n        'state_code',\n        'county_code'\n    ],\n    axis=1,inplace=True\n)\n\n\n#adjust numerics to categoricals\n\nnumerics = [\n    'income',\n    'loan_amount',\n    'interest_rate',\n    'total_loan_costs',\n    'origination_charges',\n    'discount_points',\n    'lender_credits',\n    'loan_term',\n    'intro_rate_period',\n    'property_value',\n    'total_units',\n    'tract_population',\n    'tract_minority_population_percent',\n    'ffiec_msa_md_median_family_income',\n    'tract_to_msa_income_percentage',\n    'tract_owner_occupied_units',\n    'tract_one_to_four_family_homes',\n    'tract_median_age_of_housing_units',\n    'loan_to_value_ratio'\n]\n\nbounds = [i/5 for i in range(1,5)]\n# print(bounds)\nfor col in numerics:\n    \n    if col == 'income':\n        fr.loc[fr[col]&lt;=0,col] = 0.01\n        fr[col] = np.log(fr[col])\n\n    s = fr[col].std()\n\n    m = fr[col].mean()\n\n    cut_level = [\n        m-2*s,\n        m-s,\n        m+s,\n        m+2*s\n    ]\n\n    # cut_level = list(np.percentile(fr[col],bounds))\n    \n    cut_level = [-np.inf] + cut_level + [np.inf]\n\n    print(col)\n\n    print(cut_level)\n\n    fr[col] = pd.cut(\n        fr[col],\n        bins=cut_level,\n        labels=[\"L\",\"LM\",\"M\",\"HM\",\"H\"]\n    )\n\n    fr[col] = fr[col].astype('category')\n\nincome\n[-inf, 2.7180027623114835, 3.691628271937371, 5.638879291189146, 6.612504800815033, inf]\nloan_amount\n[-inf, -68885.65400987939, 151202.55099023087, 591378.9609904514, 811467.1659905617, inf]\ninterest_rate\n[-inf, 4.02023114950366, 5.140083590148193, 7.3797884714372595, 8.499640912081793, inf]\ntotal_loan_costs\n[-inf, -2446.957679791224, 1770.6419649977897, 10205.841254575818, 14423.440899364832, inf]\norigination_charges\n[-inf, -4045.310872758461, -365.6502210778044, 6993.671082283508, 10673.331733964165, inf]\ndiscount_points\n[-inf, -2357.9172857761437, 597.1363682257906, 6507.243676229659, 9462.297330231593, inf]\nlender_credits\n[-inf, -842.2913890194186, -176.0530214966863, 1156.423713548778, 1822.6620810715103, inf]\nloan_term\n[-inf, 275.81238302705765, 313.6950405012076, 389.4603554495076, 427.3430129236575, inf]\nintro_rate_period\n[-inf, -45.48724172851337, -19.858497136107758, 31.39899204870347, 57.02773664110909, inf]\nproperty_value\n[-inf, -345237.1221738993, 85757.54565566691, 947746.8813147994, 1378741.5491443656, inf]\ntotal_units\n[-inf, 0.6835870316270407, 0.8514812544468351, 1.1872697000864239, 1.3551639229062182, inf]\ntract_population\n[-inf, 687.7734929853423, 2781.9766748768784, 6970.383038659951, 9064.586220551486, inf]\ntract_minority_population_percent\n[-inf, -9.972849989412907, 15.859285363785276, 67.52355607018164, 93.35569142337982, inf]\nffiec_msa_md_median_family_income\n[-inf, 56976.55286780146, 80719.66644071146, 128205.89358653144, 151949.00715944145, inf]\ntract_to_msa_income_percentage\n[-inf, 21.157620955165612, 67.19762396694364, 159.2776299904997, 205.31763300227772, inf]\ntract_owner_occupied_units\n[-inf, 10.782079284308338, 603.3740933230408, 1788.5581214005058, 2381.1501354392385, inf]\ntract_one_to_four_family_homes\n[-inf, 125.07069895912605, 838.2899529096912, 2264.7284608108216, 2977.947714761387, inf]\ntract_median_age_of_housing_units\n[-inf, -2.840726279035522, 16.945238577610176, 56.51716829090157, 76.30313314754727, inf]\nloan_to_value_ratio\n[-inf, 38.836008423457336, 58.761188729881695, 98.6115493427304, 118.53672964915475, inf]\n\n\n\n#extract binary to separate frame\nfr_bin = fr[[\n    'applicant_race',\n    'applicant_ethnicity',\n    'co-applicant_race',\n    'co-applicant_ethnicity',\n    #'denial_reason',\n    'aus'\n]].copy()\n\nfor k,v in new_mapper.items():\n    for l,w in v.items():\n        fr_bin[k+'_'+w] = (fr_bin[k]&l &gt; 0).astype(int)\n\nfr_bin.drop(labels=[    'applicant_race',\n    'applicant_ethnicity',\n    'co-applicant_race',\n    'co-applicant_ethnicity',\n    #'denial_reason',\n    'aus'],\n    inplace=True,\n    axis=1\n)\nfr.drop(\n    labels=[\n        'applicant_race',\n        'applicant_ethnicity',\n        'co-applicant_race',\n        'co-applicant_ethnicity',\n        'denial_reason',\n        'aus',\n        'outcome',\n        'action_taken'\n    ],\n    inplace=True,\n    axis=1\n)\n\n\n# ohe = OneHotEncoder()\n# out = ohe.fit_transform(fr)\n# # ohe.get_feature_names_out().tolist()\n# outdf = pd.DataFrame(out.toarray(),columns=ohe.get_feature_names_out().tolist())\n# outdf_nr = outdf.copy()\n\n# for col in fr_bin.columns:\n#     outdf[col] = fr_bin[col].copy()\n\n# for col in outdf.columns:\n#     outdf[col] = outdf[col].astype(int)\n\n\n# mca3d = MCA(n_components=3,one_hot=False)\n# xform3d = mca3d.fit_transform(outdf.drop(labels=[\n#     'applicant_race_No Co-applicant','applicant_ethnicity_No Co-applicant',\n#     'aus_GUS','aus_Exempt'\n# ],axis=1))\n# xform3d.columns = ['PC{}'.format(i+1) for i in range(len(xform3d.columns))]\n\n# mcaNd = MCA(n_components=20,one_hot=False)\n# xformNd = mcaNd.fit_transform(outdf.drop(labels=[\n#     'applicant_race_No Co-applicant','applicant_ethnicity_No Co-applicant',\n#     'aus_GUS','aus_Exempt'\n# ],axis=1))\n# xformNd.columns = ['PC{}'.format(i+1) for i in range(len(xformNd.columns))]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Component Analysis Code (PCA and MCA)</span>"
    ]
  },
  {
    "objectID": "app4.html",
    "href": "app4.html",
    "title": "Appendix C — Association Rule Mining Code",
    "section": "",
    "text": "C.1 Module and Data Imports\nimport pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.cluster import KMeans\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom mlxtend.frequent_patterns import apriori\n\nfrom mlxtend.frequent_patterns import association_rules",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Association Rule Mining Code</span>"
    ]
  },
  {
    "objectID": "app4.html#transform-to-basket-transactions",
    "href": "app4.html#transform-to-basket-transactions",
    "title": "Appendix C — Association Rule Mining Code",
    "section": "C.2 Transform to Basket Transactions",
    "text": "C.2 Transform to Basket Transactions\n\nmapper = {\n    'applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    # 'co-applicant_race':{\n    #     'American Indian/Alaska Native':0b0000000000000000001,\n    #     'Asian':0b0000000000000000010,\n    #     'Asian Indian':0b0000000000000000100,\n    #     'Chinese':0b0000000000000001000,\n    #     'Filipino':0b0000000000000010000,\n    #     'Japanese':0b0000000000000100000,\n    #     'Korean':0b0000000000001000000,\n    #     'Vietnamese':0b0000000000010000000,\n    #     'Other Asian':0b0000000000100000000,\n    #     'Black/African American':0b0000000001000000000,\n    #     'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n    #     'Native Hawaiian':0b0000000100000000000,\n    #     'Guamanian/Chamorro':0b0000001000000000000,\n    #     'Samoan':0b0000010000000000000,\n    #     'Other Pacific Islander':0b0000100000000000000,\n    #     'White':0b0001000000000000000,\n    #     'Information not provided':0b0010000000000000000,\n    #     'Not Applicable':0b0100000000000000000,\n    #     'No Co-applicant':0b1000000000000000000\n    # },\n    'applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    # 'co-applicant_ethnicity':{\n    #     'Hispanic/Latino':0b000000001,\n    #     'Mexican':0b000000010,\n    #     'Puerto Rican':0b000000100,\n    #     'Cuban':0b000001000,\n    #     'Other Hispanic/Latino':0b000010000,\n    #     'Not Hispanic/Latino':0b000100000,\n    #     'Information Not Provided':0b001000000,\n    #     'Not Applicable':0b010000000,\n    #     'No Co-applicant':0b100000000\n    # },\n    'aus':{\n        'Desktop Underwriter':0b00000001,\n        'Loan Prospector/Product Advisor':0b00000010,\n        'TOTAL Scorecard':0b00000100,\n        'GUS':0b00001000,\n        'Other':0b00010000,\n        'Internal Proprietary':0b00100000,\n        'Not applicable':0b01000000,\n        'Exempt':0b10000000,\n    }, \n    'denial_reason':{\n        'DTI':0b0000000001,\n        'Employment History':0b0000000010,\n        'Credit History':0b0000000100,\n        'Collateral':0b0000001000,\n        'Insufficient Cash':0b0000010000,\n        'Unverifiable Information':0b0000100000,\n        'Credit Application Incomplete':0b0001000000,\n        'Mortgage Insurance Denied':0b0010000000,\n        'Other':0b0100000000,\n        'Not Applicable':0b1000000000\n    }\n}\n\nnew_mapper = {}\nfor k,v in mapper.items():\n    new_mapper[k] = {}\n    #print(k)\n    for j,w in v.items():\n        #print(w,j)\n        new_mapper[k][w] = j\n\n\nfr2 = pd.read_csv('../data/final_clean_r2.csv')#fr.copy()\n\npct = [20,40,60,80]\n\nlevels = ['0-20','21-40','41-60','61-80','&gt;80']\n\npct_cols = [\n    'income',\n    'debt_to_income_ratio',\n    'loan_to_value_ratio',\n    'tract_minority_population_percent',\n    'tract_to_msa_income_percentage',\n    'tract_median_age_of_housing_units',\n    'interest_rate'\n]\n\nfor col in pct_cols:\n    p = list(map(lambda x: np.percentile(fr2[col],x),pct))\n    p = [-np.inf] + p + [np.inf]\n    fr2[col] = pd.cut(fr2[col],bins=p,labels=levels)\n\n\nbasket = []\nbc = [\n    'income','debt_to_income_ratio','loan_to_value_ratio',\n    'tract_minority_population_percent','tract_to_msa_income_percent',\n    'derived_sex'\n]\nb1c = ['interest_rate','company','applicant_race']\nb2c = ['interest_rate','company','outcome']\nb3c = ['interest_rate','applicant_race','outcome']\n\nb1,b2,b3 = [],[],[]\n\nfor i, row in fr2.iterrows():\n    curr = []\n    for k,v in new_mapper.items():\n        for j,w in v.items():\n            #print(row[k],type(row[k]))\n            if row[k] & j &gt; 0:\n                curr.append(\"{}:{}\".format(k,w))\n\n    if row['balloon_payment'] == 1:\n        curr.append('balloon')\n    \n    if row['interest_only_payment'] == 1:\n        curr.append('interest only')\n    \n    curr.append(\"{} rooms\".format(row['total_units']))\n\n    for col in pct_cols:\n        curr.append(\"{}:{}\".format(col,row[col]))\n    \n    # curr.append(row['company'])\n\n    curr.append(row['derived_sex'])\n\n    curr.append(row['applicant_age'])\n\n    # #interest only payments & balloon payments, \n\n    # co/applicant sex\n\n    # sex | race via observation?\n\n    basket.append(curr)\n\n\nitems = set()\nfor trans in basket:\n    for item in trans:\n        items.add(item)\n\nresult = []\nfor record in basket:\n    rowset = set(record)\n    labels = {}\n    uncommons = list(items-rowset)\n    commons = list(items.intersection(rowset))\n    for uc in uncommons:\n        labels[uc] = False\n    for com in commons:\n        labels[com] = True\n    result.append(labels)\n\nohe_df = pd.DataFrame(result)\n\nsingle_basket = ohe_df.replace(\n    [0],[np.nan]\n).reset_index()\n\nsingle_basket = single_basket.melt(\n    id_vars='index',\n    value_vars=single_basket.columns[1:]\n)\n\nsingle_basket.sort_values(by='index',inplace=True)\nsingle_basket.dropna(inplace=True)\n\n# display(\n#     ohe_df.head(),\n#     single_basket.head()\n# )\n\nsingle_basket.to_csv('../data/ARM_single_basket.csv',index=False)\n\n\nC.2.1 Construct Frequent Items\n\nfreq_items=apriori(ohe_df,min_support=0.05,use_colnames=True,verbose=1)\n\n\nProcessing 3306 combinations | Sampling itemset size 2\nProcessing 25791 combinations | Sampling itemset size 3\nProcessing 25556 combinations | Sampling itemset size 4\nProcessing 13570 combinations | Sampling itemset size 5\nProcessing 3312 combinations | Sampling itemset size 6\nProcessing 343 combinations | Sampling itemset size 7\n\nars = association_rules(freq_items,metric='support',min_threshold=0.5)\nars.sort_values(by='lift',ascending=False,inplace=True)\n\n\n\nC.2.2 Construct Association Rules (Python)\n\n\nC.2.3 Save the Top 15 Association Rules by Category\n\n\nC.2.4 Performing Rule Mining and Visualization in R\nRule mining is performed on data that was already transformed in Python from previous steps\n\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)\n\nFirst load the data into R for use in rule mining.\n\ndat &lt;- read.transactions(\n    'C:/Users/pconn/OneDrive/Desktop/Machine Learning/ML/data/single_basket.csv',\n    sep=',',\n    rm.duplicates=TRUE,\n    format='single',\n    cols=c(1,2)\n)\n\nNow that the data is loaded, R is leveraged to perform rule mining\n\nset.seed = 9001\n\na_rules &lt;- arules::apriori(\n    dat,\n    control=list(verbose=F),\n    parameter=list(support=0.04,confidence=0.01,minlen=2)\n)\n\nUsing the mined rules, the rules can now be printed and/or visualized.\n\nsorted_arules &lt;- sort(a_rules,by='support',decreasing = T)\narules::inspect(sorted_arules[1:15])\n\n     lhs                                           rhs                                         support confidence  coverage     lift  count\n[1]  {approve}                                  =&gt; {1 rooms}                                 0.8465587  0.9897986 0.8552837 1.001607 172124\n[2]  {1 rooms}                                  =&gt; {approve}                                 0.8465587  0.8566580 0.9882108 1.001607 172124\n[3]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {1 rooms}                                 0.7152005  0.9893591 0.7228928 1.001162 145416\n[4]  {1 rooms}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.7152005  0.7237327 0.9882108 1.001162 145416\n[5]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6253676  0.8650905 0.7228928 1.011466 127151\n[6]  {approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6253676  0.7311815 0.8552837 1.011466 127151\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                                                             \n      approve}                                  =&gt; {1 rooms}                                 0.6195837  0.9907512 0.6253676 1.002571 125975\n[8]  {1 rooms,                                                                                                                             \n      applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6195837  0.8663077 0.7152005 1.012889 125975\n[9]  {1 rooms,                                                                                                                             \n      approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6195837  0.7318852 0.8465587 1.012439 125975\n[10] {applicant_race:White}                     =&gt; {1 rooms}                                 0.6052911  0.9917401 0.6103324 1.003571 123069\n[11] {1 rooms}                                  =&gt; {applicant_race:White}                    0.6052911  0.6125121 0.9882108 1.003571 123069\n[12] {aus:Desktop Underwriter}                  =&gt; {1 rooms}                                 0.5645626  0.9910383 0.5696678 1.002861 114788\n[13] {1 rooms}                                  =&gt; {aus:Desktop Underwriter}                 0.5645626  0.5712977 0.9882108 1.002861 114788\n[14] {applicant_race:White}                     =&gt; {approve}                                 0.5349397  0.8764727 0.6103324 1.024774 108765\n[15] {approve}                                  =&gt; {applicant_race:White}                    0.5349397  0.6254529 0.8552837 1.024774 108765\n\n\n\nsorted_arules &lt;- sort(a_rules,by='confidence',decreasing = T)\narules::inspect(sorted_arules[1:15])\n\n     lhs                                         rhs            support confidence   coverage     lift count\n[1]  {interest_rate:&gt;80}                      =&gt; {approve}   0.18578904          1 0.18578904 1.169203 37775\n[2]  {aus:Loan Prospector/Product Advisor,                                                                  \n      aus:Other}                              =&gt; {JP Morgan} 0.15580213          1 0.15580213 4.971320 31678\n[3]  {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:61-80}              =&gt; {approve}   0.04241056          1 0.04241056 1.169203  8623\n[4]  {interest_rate:&gt;80,                                                                                    \n      tract_minority_population_percent:0-20} =&gt; {approve}   0.04197283          1 0.04197283 1.169203  8534\n[5]  {debt_to_income_ratio:61-80,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04571074          1 0.04571074 1.169203  9294\n[6]  {interest_rate:&gt;80,                                                                                    \n      tract_to_msa_income_percentage:21-40}   =&gt; {approve}   0.04036454          1 0.04036454 1.169203  8207\n[7]  {debt_to_income_ratio:21-40,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04220891          1 0.04220891 1.169203  8582\n[8]  {Female,                                                                                               \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04209579          1 0.04209579 1.169203  8559\n[9]  {2.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04509104          1 0.04509104 1.169203  9168\n[10] {1.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.05827702          1 0.05827702 1.169203 11849\n[11] {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:21-40}              =&gt; {approve}   0.05279802          1 0.05279802 1.169203 10735\n[12] {interest_rate:&gt;80,                                                                                    \n      Joint}                                  =&gt; {approve}   0.06351993          1 0.06351993 1.169203 12915\n[13] {interest_rate:&gt;80,                                                                                    \n      Male}                                   =&gt; {approve}   0.06214773          1 0.06214773 1.169203 12636\n[14] {aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.08140290          1 0.08140290 1.169203 16551\n[15] {interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                        =&gt; {approve}   0.10214832          1 0.10214832 1.169203 20769\n\n\n\nsorted_arules &lt;- sort(a_rules,by='lift',decreasing = T)\narules::inspect(sorted_arules[1:15])\n\n     lhs                                                rhs                                      support confidence   coverage     lift count\n[1]  {applicant_ethnicity:Mexican}                   =&gt; {applicant_ethnicity:Hispanic/Latino} 0.04056128  0.9169446 0.04423525 8.207934  8247\n[2]  {applicant_ethnicity:Hispanic/Latino}           =&gt; {applicant_ethnicity:Mexican}         0.04056128  0.3630800 0.11171442 8.207934  8247\n[3]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07532387  0.7362273 0.10231062 7.834365 15315\n[4]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07603703  0.7353151 0.10340740 7.824658 15460\n[5]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06595941  0.7333224 0.08994600 7.803453 13411\n[6]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06651026  0.7322395 0.09083129 7.791930 13523\n[7]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05100776  0.7264131 0.07021867 7.729930 10371\n[8]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05134712  0.7252518 0.07079903 7.717572 10440\n[9]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04512547  0.7246663 0.06227068 7.711341  9175\n[10] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04538122  0.7233459 0.06273792 7.697291  9227\n[11] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04939456  0.6718625 0.07351885 7.149444 10043\n[12] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07602227  0.6709350 0.11330795 7.139574 15457\n[13] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04969457  0.6703377 0.07413364 7.133218 10104\n[14] {applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07674034  0.6700880 0.11452278 7.130562 15603\n[15] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06656437  0.6685768 0.09956129 7.114480 13534\n\n\n\nsub &lt;- head(sort(a_rules,by='lift',decreasing = T),10)\nplot(sub,method=\"graph\",engine=\"html\")\n\n\n\n\n\n\n\nC.2.5 Examining Organization Specific Rules\n\nNFCU &lt;- subset(dat, subset = items %in% \"Navy Federal Credit Union\")\nJPM &lt;- subset(dat, subset = items %in% 'JP Morgan')\nBOA &lt;- subset(dat, subset = items %in% 'Bank of America')\nWF &lt;- subset(dat, subset = items %in% 'Wells Fargo')\nRM &lt;- subset(dat, subset = items %in% 'Rocket Mortgage')\n\nget_rules &lt;- function(trns,appear,sup,conf,len){\n    arules::apriori(\n        trns,parameter=list(support=sup,confidence=conf,minlen=len) ,\n        control=list(verbose=F),\n        appearance = appear\n    )\n}\n\nNFCU_app &lt;- get_rules(trns=NFCU,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nNFCU_den &lt;- get_rules(trns=NFCU,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nJPM_app &lt;- get_rules(trns=JPM,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nJPM_den &lt;- get_rules(trns=JPM,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nBOA_app &lt;- get_rules(trns=BOA,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nBOA_den &lt;- get_rules(trns=BOA,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nWF_app &lt;- get_rules(trns=WF,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nWF_den &lt;- get_rules(trns=WF,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nRM_app &lt;- get_rules(trns=RM,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nRM_den &lt;- get_rules(trns=RM,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\n\nplot(sort(NFCU_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(JPM_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(BOA_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(WF_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(RM_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\na_rules_den &lt;- arules::apriori(\n    dat,\n    control=list(verbose=F),\n    parameter=list(support=0.04,confidence=0.01,minlen=2),\n    appearance = list(default='lhs',rhs='deny')\n)\n\na_rules_app &lt;- arules::apriori(\n    dat,\n    control=list(verbose=F),\n    parameter=list(support=0.04,confidence=0.01,minlen=2),\n    appearance = list(default='lhs',rhs='approve')\n)\ninspect(sort(\n    subset(\n        a_rules_den,lhs %pin% 'race:'\n    ), by='lift'\n))\n\n    lhs                                           rhs       support confidence  coverage      lift count\n[1] {applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04213514  0.3407446 0.1236561 2.3546500  8567\n[2] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04138263  0.3381153 0.1223921 2.3364812  8414\n[3] {applicant_race:White}                     =&gt; {deny} 0.07539273  0.1235273 0.6103324 0.8536119 15329\n[4] {1 rooms,                                                                                           \n     applicant_race:White}                     =&gt; {deny} 0.07414348  0.1224923 0.6052911 0.8464593 15075\n[5] {applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04111213  0.1185674 0.3467406 0.8193371  8359\n[6] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04052193  0.1176227 0.3445077 0.8128092  8239\n[7] {applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05616707  0.1111652 0.5052577 0.7681857 11420\n[8] {1 rooms,                                                                                           \n     applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05537030  0.1103444 0.5017952 0.7625140 11258\n\ninspect(sort(\n    subset(\n        a_rules_app,lhs %pin% 'race:'\n    ), by='lift'\n)[1:10])\n\n     lhs                                           rhs          support confidence   coverage     lift count\n[1]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11876236          1 0.11876236 1.169203 24147\n[2]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04705836          1 0.04705836 1.169203  9568\n[3]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Male}                                     =&gt; {approve} 0.04324667          1 0.04324667 1.169203  8793\n[4]  {applicant_race:White,                                                                                 \n      aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                        =&gt; {approve} 0.05488339          1 0.05488339 1.169203 11159\n[5]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                          =&gt; {approve} 0.06145424          1 0.06145424 1.169203 12495\n[6]  {applicant_race:White,                                                                                 \n      aus:Desktop Underwriter,                                                                              \n      interest_rate:&gt;80}                        =&gt; {approve} 0.08051760          1 0.08051760 1.169203 16371\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.09885305          1 0.09885305 1.169203 20099\n[8]  {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11780329          1 0.11780329 1.169203 23952\n[9]  {applicant_race:White,                                                                                 \n      interest_rate:0-20,                                                                                   \n      Rocket Mortgage}                          =&gt; {approve} 0.05066840          1 0.05066840 1.169203 10302\n[10] {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04677802          1 0.04677802 1.169203  9511",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Association Rule Mining Code</span>"
    ]
  }
]