[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Protected Classes as Predictors in Mortgage Approval",
    "section": "",
    "text": "1 Introduction\nHow strong is the connection between your age, gender, or race and ability to attain a mortgage?\nIn early 2024, US Senator Sherrod Brown, along with colleagues, called for a review of lending practices by Navy Federal Credit Union (NFCU) as a result of a late 2023 CNN report that oulined disparate impact to persons of color seeking to acquire conventional loans for home purchases. The disparity in results between groups appears damning to the organization’s reputation. CNN’s analysis predominantly explored the mathematical differences between racial groups and various lenders, but did not necessarily outline a direct cause-and-effect relationship (e.g. being a person of color seeking a loan at NFCU is guaranteed to decrease your approval chances). Their analysis instead covered the rates of approval for the different groups, and compared those approval rates between groups and against other financial organizations’ approval rates.\n\n\n\nvisual comparison of NFCU to other financial institutions regarding loans to persons of color\n\n\nEach of the organizations in the above graphic (from CNN) appear to have some degree of disparity in holding higher approvals for White/Caucasian applicants than for Black/African American, at first glance. The graphic certainly highlights the highest difference in approval rates at NFCU vs that of other financial institutions. Being at the top of the list of disparity absolutely draws ire from onlookers, yet the other institutions on the list may also be worthy of similar scrutiny. Disparate outcomes are disparate outcomes, and any case in which there are significant differences in outcomes for individuals on a basis of age, gender, race, or ethnicity (also called protected classes) is of concern to everyone.\nA follow-up article by CNN includes statements from NFCU and an external reviewer. Some of the statements claim that non-race factors such as income verification and incomplete credit applications weren’t included in CNN’s study, and that other proprietary and non-public information are included in the organization’s mortgage underwriting processes.\nAlso of interest is an August 2024 story from Lehigh University on Artificial Intelligence, in particular Large Language Models, exhibiting bias in mortgage underwriting decisions. The article covers a study done using ChatGPT 3.5 and 4.0, Llama3-8B and 3-70, and Claude3 Sonnet and Opus, in which the LLMs were prompted to decide whether or not to approve a loan application based on provided data. The study found that the LLMs had similar biases for recommending interest rates, and great variation in bias as to whether or not loans would be approved. When the LLMs were directly instructed to use no bias and ignore race in making the decisions, the disparity in approval outcomes disappeared. This suggests that technology in decision-making processes is a double-edged sword, capable of doing both great harm and great good, depending on how it is weilded.\nThe Home Mortgage Disclosure Act (HMDA) is a federal law that establishes mandatory reporting for financial institutions that provide loans to borrowers. It mandates reporting of various values, metrics, and qualities of applicants and co-applicants, such as their debt-to-income ratio, loan amount requested vs. property value, age/gender/race of applicants, whether or not the loan was approved, along with many other datapoints. Part of the intent of this law is to enable the examination of lenders for potential discriminatory practices in lending, and enable accountability and oversight to combat such issues. HMDA data was leveraged for both the CNN’s report and for the study referenced in Lehigh University’s article.\n\nThe examination of available HMDA data solely from measures of central tendency (e.g. means, medians, and spreads) can be useful as an exploratory analysis, but may not provide a full picture of what is and is not happening under the hood in terms of the factors that influence a lender’s decision to ultimately approve or deny a mortgage application. HMDA data provides publicly available information, but not any trade-secret, proprietary, or protected information that they leverage to arrive at their decision. It may not be possible to know or to attain knowledge of the underlying variables and considerations employed to reach the decision. It may also pose challenges in building predictive mechanisms to support reaching such decisions. That being said, this study seeks to explore the available data to see if effective predictions can be made, and what conditions or subsets of the available features and values of the data are needed to make them.\nIn an ideal world, a potential borrower’s protected class features should have no impact on their ability to attain a loan, but instead be based solely on the borrower’s overall ability to repay the loan, given multiple financial factors. This research seeks to leverage HMDA data to explore CNN’s identified disparities, and explore other possible disparities within other lending organizations, through predictive modeling. Leveraging multiple techniques such as identifying common associations and trends within the data, performing statistical analyses, developing multiple model constructs, and via inclusion and exclusion of protected classes as part of model training and evaluation data, this study seeks to explore and answer some of the following questions (solely using publically available data):\n\nWhich factors within avaialble HMDA data are the greatest influencers in mortgage approvals?\nAre protected classes of applicants and co-applicants strong predictors for determining loan approval?\n\nIf protected classes are strong predictors for loan approval, is the predictive strength greater for one institution over another?\n\nIf protected classes are strong predictors for loan approval, is the predictor’s strength higher within a particular geographic region?\n\nHow is predictive model performance impacted when including or excluding protected class data in training and testing data? e.g. Does performance increase when including? If so, by how much?\n\nHow well do predictive models perform when trained using protected class information that was collected by a lender observationally (e.g. via inspection of surname or visually seeing the applicant)?\nHow do loan-specific or home-specific qualities and features, absent borrower features, impact predictions of approval or denial?\nAre borrower features aside from race as or more associated with a result of loan denial? e.g. is being female or being under or over a certain age as associated with loan denial as being a certain race?\nCan available HMDA data be leveraged to effectively predict mortgage borrower interest rates?\nIf effective mortgage rate predictions are possible, how do they change when including or controlling for protected class data in the model?\nCan selected / identified features and outcome (e.g. debt-to-income-ratio, loan-to-value ratio, and decision for approval or denial of the loan) be used to predict the borrower’s protected class features of race, gender, or age group?\nAs inferred by lenders, can various latent variables be identified as impactful within modeling, and can the degree or extent of their impact on the decision process be determined as it pertains to mortgage underwriting decisions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "",
    "text": "2.1 Links to Data and Code\nThe outcomes of the work performed in this section are present in multiple locations. The data itself was too large to include as part of the github repository for this effort.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#links-to-data-and-code",
    "href": "data.html#links-to-data-and-code",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "",
    "text": "The GitHub Respository can be found here.\nScript to leverage APIs and download the target data is located here.\nInitial data download, prior to scoping and cleaning, is located here.\nThe script to scope, clean, and combine the dataset is located here.\nThe cleaned dataset is located here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#data-gathering",
    "href": "data.html#data-gathering",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.2 Data Gathering",
    "text": "2.2 Data Gathering\nTo pursue the established research objectives, the following resources were identified and leveraged as data sources:\n\nThe Global Legal Entity Identifier (GLEIF) API\nThe FFIEC Home Mortgage Disclosure Act (HMDA) API\n\nCNN provides explicit data selection, cleaning, and transformation information at the base of their first article in the section labeled “How We Reported This Story.” The scope of this research includes the scope identified by CNN, as the nature of their filtration and focus could be ascribed to what one would consider being part of the American dream - the ability to attain a first loan for a single-family property as a primary residence, not for the purpose of business or commerce. The CNN article predominantly leveraged 2022 and prior year data, whereas this research will inspect 2023 data.\nLenders in question, listed in the graphics for the first CNN article, are identified via the GLEIF API. HMDA Data solely contains the Legal Entity Identifier (LEI) in each record as opposed to the name of the entity. In order to perform aggregate analysis and labeling of data as being linked to one of these organizations, it is necessary to query another source to identify the appropriate LEIs for use in querying HMDA. Furthermore, HMDA is a large database, and being able to provide filtering conditions substantially reduces the size of the returned data. Absent key filters, large volumes are returned in a single file. At the same time, the number of filters that can be supplied to the HMDA API simultaneously is limited, so filtering by organization and other key filters will minimize the data pull before further cleaning.\nThe HMDA API provides simple access to a wide array of data on mortgages. Additionally, the API provides a well-documented data dictionary that spells out the returned features and their meanings in the context of the Loan Application Register. An API query returns a total of 99 columns with quantitative and qualitative data on the prospective borrower, the property they seek to purchase, aspects and features of the loan, and the final action taken on the loan application. Some of these columns may exceed the scope of this research, whereas many others are necessary and important to the established research questions. Finding creative and effective means of reducing dimensionality is key.\nNeither of the API endpoints leveraged in data collection for this research required an API key, thus simplifying the data gathering process.\nThe source code for data gathering efforts in this research is located here.\nExample use of the GLEIF API:\nimport requests\nendpoint = \"https://api.gleif.org/api/v1/lei-records\"\nparams = {\n    'page[size]':100, #specify number of records to return per page\n    'page[number]':1, #specify the page number to return within the same search scope\n    'filter[entity.names]':'NAVY FEDERAL CREDIT UNION' #provide an organization\n}\n\nrequests.get(endpoint,params)\nFor the GLEIF API, the JSON response will contain the LEI for organizations with similar names to what is provided in the filter[entity.names] parameter. Depending on how specific one is with the name provided, this could produce a long list of results. This was the case when examining larger banks like JP Morgan and Wells Fargo while building the code for this research. In some cases, the GLEIF API returned 400 records. When this occurs, it is necessary to make use of the page[size] and page[number] parameters for this API.\nIn the case of gathering 400 results, in order to pull all of them into data records via use of this API, one would need to perform 4 API calls, each time updating the page[number] parameter to the next valid value. The API response also always provides a lastPage value for valid responses under the path response.meta.pagination.lastPage, which one can use for iteration if needed to extract all LEIs for an entity.\nBelow depicts example use of the HMDA API in Python:\nhmda_endpoint = \"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv\"\nhmda_params = {\n    'years':'', #specify the year or list of years for which you seek to query\n    'loan_types':1, #conventional loans only\n    'leis':'' #will change/update based upon what orgs we're downloading\n}\n\nresp = requests.get(endpoint,params)\n\nwith open('/file/path/here.csv','wb') as f:\n    f.write(resp.content)\n    f.close()\nOf note for the HMDA endpoint in this case is the fact that it natively returns record data in CSV format. As such, the means to store the data is relatively simple, requiring no substantial parsing of JSON content to process the data into record or dataframe format.\nThe data, from the initial API queries and prior to further cleaning and tailoring, is located here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.3 Data Cleaning",
    "text": "2.3 Data Cleaning\nThe primary methods of cleaning in this research revolve around dimensionality reduction and row reduction. The goal for cleaning is to minimize the number of columns in the dataset to key quantitative and qualitative features holding potential use in modeling, eliminating records out of the scope of this research, and handling blank/missing/incorrect values in the remaining records.\n\n2.3.1 Dimensionality Reduction\n\n2.3.1.1 Collapsing Redundant Columns with Binary Encoding\nThis research collapses the following columns to deduplicate data and reduce columnar dimensionality:\nApproximately 29 columns in the data contain categorical information that can be encoded in binary and summed into a single column to retain all data while reducing dimensionality. These columns pertain to applicant and co-applicant race and ethnicity, the automated underwriting systems used by organizations to support decision making in approving or denying loan applications, and reasons for loan denial, if applicable.\nSince each of these columns can take on multiple values, the HMDA dataset owners have allowed for up to 5 columns for each of these categorical variables. By translating each possible value to a unique binary encoding (with number of bits equal to the number of classes, and solely a single 1 in each encoding), the sum across multiple columns will provide for a unique value containing all classifications in a single categorical variable.\nFurthermore - this transformation addressses some potential biases in CNN’s methods. Their report excluded records in the case of mixed-race applicants and co-applicants as being members of a racial group. The binary encoding allows for their inclusion as being part of such groups; while individuals of mixed race may not be fully part of any single group, they remain part of it.\nThe data contain multiple column datapoints for the ethnicity and race of applicants and co-applicants for a total of 20 additional columns (e.g. “co-applicant_race-1, co-applicant_race-2…”). Not all of these columns contain viable data. An individual can have multiple races and ethnicities. To allow for retaining of the data while also eliminating unnecessary blank columns, these columns are collapsed to a single column, each - reducing the data by 16 columns.\nAnother set of column collapses are performed on the ‘aus-’ (automated underwriting system) and ‘denial_reason’ columns. There are 5 columns in the source data containing the different system(s) used by lenders to support underwriting decision making processes. The denial_reason is rich with potentially important information that could lend itself to the identification of latent or unavailable variables (e.g. credit score, income verification, etc). These columns are collapsed in the same manner as for the aforementioned race and ethnicities of applicants.\nEach value was translated from the digit representing it to instead a binary representation with a number of bits equivalent to the number of valid classes available for the variable from the data dictionary. After each value in each column was mapped to a bitwise representation, the sum across each column was taken to produce a final column with bits containing all of the original information. This transformation enables simple bitwise AND operations to identify records that meet a specific condition (or combination of conditions) while also eliminating columnar redundancy.\n\n\n\n\nTable 2.1: Remapping of Racial Categories for applicants and co-applicants\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nAmerican Indian or Alaska Native\n1\n1\n\n\nAsian\n2\n2\n\n\nAsian Indian\n21\n4\n\n\nChinese\n22\n8\n\n\nFilipino\n23\n16\n\n\nJapanese\n24\n32\n\n\nKorean\n25\n64\n\n\nVietnamese\n26\n128\n\n\nOther Asian\n27\n256\n\n\nBlack or African American\n3\n512\n\n\nNative Hawaiian or Other Pacific Islander\n4\n1024\n\n\nNative Hawaiian\n41\n2048\n\n\nGuamanian or Chamorro\n42\n4096\n\n\nSamoan\n43\n8192\n\n\nOther Pacific Islander\n44\n16384\n\n\nWhite\n5\n32768\n\n\nInformation not provided by applicant in mail, internet, or telephone application\n6\n65536\n\n\nNot applicable\n7\n131072\n\n\nNo co-applicant\n8\n262144\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.2: Remapping of Ethnic Categories for applicants and co-applicants\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nHispanic or Latino\n1\n1\n\n\nMexican\n11\n2\n\n\nPuerto Rican\n12\n4\n\n\nCuban\n13\n8\n\n\nOther Hispanic or Latino\n14\n16\n\n\nNot Hispanic or Latino\n2\n32\n\n\nInformation not provided by applicant in mail, internet, or telephone application\n3\n64\n\n\nNot applicable\n4\n128\n\n\nNo co-applicant\n5\n256\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Remapping of Automated Underwriting Systems\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nDesktop Underwriter (DU)\n1\n1\n\n\nLoan Prospector (LP) or Loan Product Advisor\n2\n2\n\n\nTechnology Open to Approved Lenders (TOTAL) Scorecard\n3\n4\n\n\nGuaranteed Underwriting System (GUS)\n4\n8\n\n\nOther\n5\n16\n\n\nInternal Proprietary System\n7\n32\n\n\nNot applicable\n6\n64\n\n\nExempt\n1111\n128\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Remapping of Denial Reasons\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nDebt-to-income ratio\n1\n1\n\n\nEmployment history\n2\n2\n\n\nCredit history\n3\n4\n\n\nCollateral\n4\n8\n\n\nInsufficient cash (downpayment, closing costs)\n5\n16\n\n\nUnverifiable information\n6\n32\n\n\nCredit application incomplete\n7\n64\n\n\nMortgage insurance denied\n8\n128\n\n\nOther\n9\n256\n\n\nNot applicable\n10\n512\n\n\n\n\n\n\n\n\nExamining Table 2.4, for instance consider a cell value of 96 in the ‘denial_reason’ column. There are two unique binary encoding values - 32 and 64, that produce this sum. The value 96, then, would signify a loan that was denied both for having ‘Unverifiable Information’ and for ‘Credit application incomplete’. Similarly, referencing Table 2.1, a value of 32770 in this column would signify a person whose races are Asian (32768) and White (2) (2+32768=32770).\nThis binary numeric representation of categorical data allows the column to contain multiple distinct and potentially impactful datum while reducing dimensionality. E.g. to filter the dataset to records pertaining to the race Asian, the filter can be performed by selecting records where the binary AND of 32768 & [column_value] = 32768. Other races may also be present in the filtered records, but all Asians will be captured.\nLeveraging the columns natively for tasks such as clustering may not be effective or efficient, and could pose challenges in performing unsupervised learning. Further cleaning may be necessary for such tasks, but collapsing and retaining the data ensures its availablity for further transformation and cleaning. For instances, the re-splitting and pivoting of these columns into boolean values could be leveraged in association rule mining or in Bernoulli Naive Bayes analyses.\nNotice also that encodings for not available or not applicable types of data are higher values within each encoding list. This actually provides a very simple data validation technique, and allows for the identification or elimination of potentially erroneous records. Namely any value:\n\nwithin the exclusive interval (65536,131072), or greater than 131072 for applicant_race\nwithin the exclusive intervals (65536,131072) and (131072,262144), or greater than 262144 for co-applicant_race\nwithin the exculsive interval (64, 128), or greater than 128 for applicant_ethnicity\nwithin the exclusive intervals (64,128) and (128,256), or greater than 256 for co-applicant_ethnicity\nwithin the exclusive interval (64,128), or greater than 128 for aus (automated underwriting system)\ngreater than 512 for denial_reason\n\nwill signify an erroneous record. Namely, if any value is found to include “Not Applicable” in conjunction with another valid selection, one of the two selections were selected in error, and the data is not reliable. As such, these records can be filtered from the source data. As such, records meeting the above criteria are removed from the source data.\nBecause the totality of racial and ethnicity information can now be contained within a single column, the derived_ columns for ethnicity and race can be dropped from the dataset.\n\n\n2.3.1.2 Column Elimination\nThe following list of columns were eliminated from the source data\n\n\n\n\n\n\n\nColumn Dropped\nReason\n\n\n\n\nactivity_year\n2023 Data Only\n\n\nderived_msa-md\nUsing county_code\n\n\ncensus_tract\nUsing county_code\n\n\nderived_loan_product_type\nAvailable in other columns\n\n\nderived_dwelling_category\nAvailable in other columns\n\n\nconforming_loan_limit\nProject scope\n\n\nlien_status\nProject scope\n\n\nreverse_mortgage\nOne value after scope applied\n\n\nbusiness_or_commercial_purpose\nProject scope\n\n\nnegative_amortization\nOne value after scope applied\n\n\noccupancy_type\nProject scope\n\n\nconstruction_method\nProject scope\n\n\nmanufactured_home_secured_property_type\nProject scope\n\n\nmanufactured_home_land_property_interest\nProject scope\n\n\nsubmission_of_application\nProject scope\n\n\ninitially_payable_to_institution\nProject scope\n\n\nderived_ethnicity\nAvailable in other columns\n\n\nderived_race\nAvailable in other columns\n\n\nloan_type\nProject scope\n\n\nprepayment_penalty_term\nProject scope\n\n\napplicant_age_above_62\nAvailable in other columns\n\n\nco-applicant_age_above_62\nAvailable in other columns\n\n\ntotal_points_and_fees\n&lt;1% of records have values\n\n\nrate_spread\n&lt;1% of records have values\n\n\nmultifamily_affordable_units\n&lt;1% of records have values\n\n\nlei\nSubstituting company name\n\n\nstate_code\nUsing county_code\n\n\nhoepa_status\nSingle value in column after scope applied\n\n\n\n\n\n\n\n\n2.3.2 Row Elimination\nThe initial pull from the HMDA API allowed solely for filtering by one or two filters. This effort pulled data for 5 selected lenders in the year 2023, for conventional loans only. To reduce the data to the same scope stipulated in the CNN article, row records must further be eliminated based upon the following criteria:\n\nexact duplication of another row (from the source data, before any transformations are applied). The nature of the data is such that the probability of an exact duplicate of 99 column values between two records is negligible, and as such exact duplicates should be ignored.\nscoping to same frame as CNN:\n\nlien_status != 1, or only first lien secured properties\ntotal_units not in [1,2,3,4], or only 1-4 unit homes\nconforming_loan_limit != ‘C’, or only conforming loans\nbusiness_or_commercial purpose != 2, or non-commercial and non-business properties\noccupancy_type != 1, or primary residence\nloan_purpose != 1, or for the purchase of a home\n\nrecords in which there is no value for county_code\nrecords for which the loan would be neither approved nor denied: when the action_taken value is 4 (Application Withdrawn) or 5 (File closed for incompleteness).\nrecords that have invalid values as defined in the binary encoding section above for 6 variables (applicant_race, co-applicant_race, applicant_ethnicity, co-applicant_ethnicity, aus, denial_reason).\n\n\n\n2.3.3 Ordinal Encoding\nThe following columns are re-encoded as ordinal data:\n\ntotal_units\napplicant_age\ndebt_to_income_ratio\n\n\n\n2.3.4 Missing and Blank Values\nThe following aspects of the dataset have the potential for missing or blank values. The reporting requirements vary for the different columns in the HMDA LAR, and the blanks and missing values can occur for a multitude of reasons:\n\napplicant didn’t provide the information\napplicant withdrew their application\ndata was not entered or submitted by the institution\ninstitution did not have or receive the data for the applicant or their prospective property\nthe column may not be applicable or filled because the application was denied, or was not applicable for the particular loan circumstances (e.g. an introductory rate period wouldn’t be applicable for an adjustable rate mortgage)\n\nEach of the following is replaced by the median value of the variable when the data is grouped by state_code and total_units (e.g. number of rooms in the home):\n\nproperty_value\nloan_to_value_ratio. special note: when property_value and loan_amount are available for a record, this value is filled with the value of the loan amount (times 100) divided by the listed property value. Otherwise, any blanks are filled with the median loan to value ratio when the data is grouped by state and number of rooms.\n\nEach of the following is replaced by the median value of the variable when the data is grouped by state_code and company (e.g. institution processing the mortgage application):\n\nlender_credits\nintro_rate_period\ninterest_rate\norigination_charges\ndiscount_points\ntotal_loan_costs\n\nNote: Whenever the grouping by state_code and company produced N/A values, these were replaced with the value 1 in the event of a need for logarithmic transformations on these variables.\n\n\nEach of the following is replaced with the mode of the variable when the data is grouped by state_code and company (e.g. institution processing the mortgage application):\n\nintro_rate_period\ndebt_to_income_ratio\n\nNote: Whenever the grouping by state_code and total_units produced N/A values, these were replaced with the value 0 as they better align with categorical variables than as numeric, and may not undergo transformations.\n\n\nAnother common blank value included the income feature. Blanks and missing values are replaced with the value from the column ffiec_msa_md_median_family_income divided by 1000. This value is an approximation for median family income in the specific metropolitan statistical area (MSA) in which the home is located.\n\n\n2.3.5 Incorrect Column Values\nSome values of interest for inspection include loan_to_value_ratio, property_value, and loan_amount, which should, in theory, all be related and have reasonable values. A first-lien for-the-purpose-of-purchase, conventional, conforming loan is highly unlikely to have a value greater than 100%. This is because a savvy lender would not secure a property as collateral for a loan that greatly exceeds its value.\nThat being said, there may be cases were this ratio could stretch further based upon qualities of the borrower and status of the market, and especially if the borrower seeks to include the overage as part of plans for improvements of the home, or covering closing costs or other mortgage requirements - in combination with the borrower having an excellent credit score or having a very high income. The case of high loan-to-value-ratio (henceforth LTV Ratio), may be unlikely, but is not impossible.\nBelow is an examination for each lender for extreme values in LTV ratio:\n\n\n\n\n\n\n\n\nFigure 2.1: Violin plot of Loan To Value Ratio, by lender\n\n\n\n\n\nThere appear to be some extreme values to the high right for each lender, in some cases with the ratio nearing 1000%. To examine further, whether or not the loan was approved should be considered, and if the loan was denied, the reason for denial should be considered as well.\nIf any values in this extreme range consist of loans that were denied, and reasons for denial include items such as “insufficient collateral”, then these items may simply be extreme case outliers that are valid and real as opposed to errors in the data.\nConsidering a range of 100% or greater loan_to_value_ratio, the following information is available:\n\n\n\n\n\n\n\n\nFigure 2.2: Approvals/Denials for Applications with LTV &gt; 100%\n\n\n\n\n\nIn Figure 2.2, it is clear that the majority of loans in this category were denied by lenders. There is a subset of loans that were approved (~40%), and examining further to examine factors such as the loan_to_value_ratios in this group, as well as reasons for denial amongst the denied applications, is necessary before determining need for identifying these records as errors vs. outliers.\n\n\n\n\n\n\n\n\nFigure 2.3: Reasons Cited for Loan Denials with LTV Ratio &gt; 100%\n\n\n\n\n\nThis plot makes it apparent that many of these high LTV ratio loan applications are likely legitimate. Many of them were denied on the bases of debt to income ratio and collateral (e.g. the property is insufficient collateral to cover the risk of the loan).\nThat being said, it’s also necessary to examine the 40% of loans that were actually approved in this window.\n\n\n\n\n\n\n\n\nFigure 2.4: strip plot of high LTV (&gt;100%) 2023 loans by approval\n\n\n\n\n\nFrom Figure 2.4, its apparent that the approved loans in this excess range tended to be on the much lower end of the spectrum, seemingly hovering just above 100%, whereas the denied applications span from just over 100% all the way up to 1000%. All in all, it would seem these extreme values, in both the cases of approved and denied loans, are legitimate values.\nA remaining concern is for potential discrepancy in the loan to value ratio itself. It’s appears odd that many data points bunch up at the 1000% mark as opposed to going above or below it, like it’s an arbitrary cap on the actual value. Below the same plot is examined, but instead the Loan to Value Ratio is replaced the value of Loan Amount * 100 / Property Value.\n\n\n\n\n\n\n\n\nFigure 2.5: strip plot of high LTV (&gt;100%) 2023 loans by approval, with adjusted LTV value\n\n\n\n\n\nIt would seem that the 1000% mark may indeed be an arbitrary cap, or a potential error of some sort. Per the Consumer Financial Protection Bureau on reporting requirements, the value for the combined loan to value ratio is to be “the ratio of the total amount of debt secured by the property to the value of the property relied on in making the credit decision.”\nIn most other cases in which the values sit below 100%, this calculated value is within a small unit difference of the reported value. This may be because the decimal places on the decision are only to be included if those decimal points were relied upon to make the decision on whether or not to approve the loan.\nIn light of the exploration on loan to value ratio, the feature loan_to_value_ratio will be replaced with the value of \\(\\frac{100\\cdot\\text{loan amount}}{\\text{property value}}\\) . In cases where one of these values is blank, the loan_to_value_ratio will be filled with the median value for loan_to_value_ratio when the data is grouped by state_code and total_units (or number of rooms) in the home.\nAnother feature worth exploring is income. This feature, per the HMDA LAR Data Dictionary, is in thousands of dollars. There are millionaires and even billionaires in the United States. Generally, though, one might expect to see an exponential distribution of income in the source dataset. Furthermore, one should expect to see a narrow string of values getting thinner and thinner as it approaches the upper end of the dataset. Inspecting income with a boxplot, there are some clear challenges:\n\n\n\n\n\n\n\n\nFigure 2.6: boxplot of borrower income\n\n\n\n\n\nA cursory inspection of this data suggests that there may be a single, erroneous outlier sitting at a value of approximately $209M worth of income, compressing the visibility of the boxplot down to nearly nothing. It is possible that this datapoint is real and correct, and that there was a single, very wealthy applicant for a mortgage in the data.\nProducing the same plot, absent the outlier, however, displays similar results:\n\n\n\n\n\n\n\n\nFigure 2.7: Boxplot of Income (Outlier Removed)\n\n\n\n\n\nHowever, below is an examination of the data with and without this high-end datapoint using kernel density estimation for the natural logarithm of income:\n\n\n\n\n\n\n\n\nFigure 2.8: Logarithmic Kernel Density Estimate for Applicant Income\n\n\n\n\n\nWith the exception of some deviations from normality in the central portions of the curve, these plots appear to showcase a normally distributed variable. Examining the mean and standard deviations for the log of income, with and without the extreme outlier, produces the following:\n\n\n\n\nTable 2.5: mean and standard deviation, with and without extreme outlier for income\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\n\nWith Outlier\n4.725800\n0.623842\n\n\nWithout Outlier\n4.725769\n0.623654\n\n\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the log-transformed feature, income, appears to retain the same features of central tendency with or without this extreme value. Provided that a log transformation of the feature is used within any models, this outlier should have little to no impact on training and testing data, as the underlying parent distribution can be approximated by that of the normal distribution \\(N(4.726,0.624)\\) in either case. As such, the row will be retained, and the value will not be adjusted (minus the log-transformation).\n\n\n2.3.6 Cleaned Columns\nPrior to cleaning, there were numerous blank or not applicable values within the data. The below image depicts the state of a subset of the data, after initial download.\n\n\n\nData, after initial download, held multiple blanks\n\n\nThe column volume is not done justice by this image; the original dataset held nearly 100 columns with large swaths of blank and missing values. Through the methods described previously, this data was appropriately scoped, transformed, cleaned, and simplified for further use.\nThe final results of the cleaning process are too large to fully depict in images here. Despite a substantial cleaning effort, there remains a large volume of columns (~50 down from 100) of varying types. Here, some key cleanups performed during the cleaning effort are highlighted:\n\n\n\nAfter Filling Blanks With Medians, Modes\n\n\nOne can see that there are no blanks in this sector of the dataset, and that there is a wider array of values and reasonable variablility in the columns, examining record by record. There may be additional work and adjustments to be performed in the realm of cleaning, but having populated columns, filled with median and modal values, provides a point of departure for further examination and analysis.\n\n\n\nCollapsing 29 Columns to 6\n\n\nThe above depiction of the collapsed columns is a substantial dimensionality reduction while retaining all of the underlying data. Each of these columns were originally 5 columns in the source data (less denial_reason, which was 4 columns).\nIn any case where the value in one of these columns holds an exact power of 2, it means that only one of those 4-5 original columns held a value - values like 1, 16, 32, 64, 128, 256, 512, 32768, 65536, 131072, and 262144 (all present in this graphic).\nThis means there were nearly 24 columns worth of blanks for each record depicted in the above image. One can nearly count on their fingers the number of instances for these records in which more than one of those columns had legitimate values. This collapsing of records while retaining data is good compression and retention for storage and future modeling.\nThe cleaned dataset can be found and downloaded here. Alternatively, one can clone this research project’s GitHub Repository, run the data pulling script, and then run the data cleaning script, in order to reproduce the same dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#further-exploratory-data-analysis",
    "href": "data.html#further-exploratory-data-analysis",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.4 Further Exploratory Data Analysis",
    "text": "2.4 Further Exploratory Data Analysis\nTo examine the data under similar conditions as CNN for 2023 data, below is a plot of the top 5 lenders and their approval rates for select racial groups, controlling for no other variables:\n\n\n\n\n\n\n\n\nFigure 2.9: 2023 Loan Approval Rates, By Company and Select Race\n\n\n\n\n\nThe above appears to flow from 2022’s findings into 2023 - that, at least for NFCU, the trend appears to continue under the specified scope with approval rates of about 76% for White applicants, and 43.8% for Black/African American Applicants. Comparing Figure 2.9 to CNN’s figures, the outcomes are remarkably similar for 2023.\nSimilar exploration into sex and age is also of interest for this study:\n\n\n\n\n\n\n\n\nFigure 2.10: Approval Rates by Institution and Borrower Sex\n\n\n\n\n\nThis chart also reveals some interesting trends, not just for NFCU, but across lenders. Notably - when no sex is provided by the applicant, 100% of loans across lenders received approval of some sort. Somewhat similarly, for lenders like JP Morgan, Rocket Mortgage, and Wells Fargo, the institutions had high approval rates in cases for which the applicant selected both sexes on their application.\nLastly, as it is pertinent to the established research questions, examination of differences in outcomes for reported age is necessary.\n\n\n\n\n\n\n\n\nFigure 2.11: barplots of approval rates by age, 2023\n\n\n\n\n\nThis examination reveals similar results as for sex; when no age is reported or documented, it appears that the approval rate approaches 100% for applicants. However, it is not the case when race is not reported. In Figure 2.9, one can see that in the case of no race reported, approval rates for such mortgages ranged from 62.9% (NFCU) to 98.6% (JP Morgan).\nThese comparisons, however, are simply numerical and do not establish any kind of a cause and effect relationship in either case, for any of the variables. Just because the outcomes in 2023 when gender or age were not reported to these lenders were nearly 100% of the time an approved mortgage, does not mean that one not reporting their gender or age on a mortgage application guarantees that they will be approved by one of these 5 lenders.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#last-thoughts-on-source-data",
    "href": "data.html#last-thoughts-on-source-data",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.5 Last Thoughts on Source Data",
    "text": "2.5 Last Thoughts on Source Data\nThe provided visuals and exploration in this research thus far are observations of a snapshot in time - they, in and of themselves, do not establish cause-and-effect relationships or the presence of statistically significant differences in outcomes between protected classes and lenders.\nThe goals of this research include bi-directional modeling. While machine learning modeling on its own does not elucidate a cause-and-effect relationship between variables, it comes closer than simple observations or statistical analyses. Performing bi-directional modeling (can available features and subject’s age/gender/race predict their outcome? and can a subject’s outcome and features predict the subject’s age/gender/race) takes additional steps in the direction of causality.\nEstablishing significant differences can show a difference, but not the cause. Establishing models that effectively predict show that the statistical differences in groups can be leveraged effectively to predict outcomes, but also do not establish cause. Exploration of latent variables, interjection and intervention of available and latent variables, and further research, are all needed to establish causality.\nMore exploration is necessary within this data, and potentially additional cleaning and transformations. Few if any numeric variables were scaled or transformed to collapse them all within similar ranges. This will be needed for building certain models within this research effort. Further understanding of categorical variable associations and relationships need further exploration and study. This will be an ongoing effort throughout this research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "3  Principal Component Analysis",
    "section": "",
    "text": "3.1 Overview\nPrincipal component analysis (or PCA) is the act of rotating in combination with stretching or squishing the axes of a multi-dimensional dataset to better align with its direcitonality in multidimensional space. By realigning the axes, the variation within the data can be more directly tied to and explained by the axes (also called Principal Components). In some cases, the transformation can be so profound such that a substantial amount of the data’s variation can be explained using fewer dimensions.\nConsider the below graphic:\nFigure 3.1: A strongly correlated 2-dimensional dataset\nThe X and Y values of this data appear to be connected, correlated even. Note - PCA is NOT a correlation analysis, but leverages any existing correlation in the data between one or more variables to transform the basis of each datapoint and vector in the data.\nThe goal of PCA is to remove strong corrlelations with high R values from the data by realigning axes of the data along the directions in the data which contain the greatest variance in the data. The above plot appears to show that the x-coordinate is a good predictor for the y-coordinate, and a simple linear regression analysis reveals that this is the case.\nTable 3.1: Least Squares Regression, before PCA\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.985\n\n\nModel:\nOLS\nAdj. R-squared:\n0.985\n\n\nMethod:\nLeast Squares\nF-statistic:\n6400.\n\n\nDate:\nWed, 13 Nov 2024\nProb (F-statistic):\n4.49e-91\n\n\nTime:\n22:17:28\nLog-Likelihood:\n274.70\n\n\nNo. Observations:\n100\nAIC:\n-545.4\n\n\nDf Residuals:\n98\nBIC:\n-540.2\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n11.9091\n0.002\n6468.253\n0.000\n11.905\n11.913\n\n\nx1\n0.0168\n0.000\n80.000\n0.000\n0.016\n0.017\n\n\n\n\n\n\nOmnibus:\n41.522\nDurbin-Watson:\n0.564\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n139.401\n\n\nSkew:\n-1.374\nProb(JB):\n5.36e-31\n\n\nKurtosis:\n8.089\nCond. No.\n10.4\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nImagine instead if the axes, while retaining their perpendicularity (or orthogonality), streched in the general orientation of the regression line (we’ll call it \\(X'\\) or our new X axis) from Figure 3.1, with \\(Y'\\) bisecting \\(X'\\) at a \\(90^o\\) angle. Below is a depiction of the data after such a transformation.\nFigure 3.2: the 2D dataset, post PCA transformation\nFigure 3.2 shows us that after the transformation, the data has a correlation value closer to zero than having a strong positive value as it had within Figure 3.1. One might also note that Y’ is an directional inversion of Y in this case (e.g. values that were above the correlation plot line in Figure 3.1 are below the correlation line in Figure 3.2). The act of applying PCA transformation is a linear combination that can result in the rotation, expansion and/or contraction of a vector as it is transposed into the new basis space.\nPerforming regression analysis on this data once more reveals that the goal of PCA is reached, that the variables, when projected into this new basis, hold no correlation:\nTable 3.2: Least Squares Regression, after PCA\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared (uncentered):\n0.000\n\n\nModel:\nOLS\nAdj. R-squared (uncentered):\n-0.010\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.000\n\n\nDate:\nWed, 13 Nov 2024\nProb (F-statistic):\n1.00\n\n\nTime:\n22:17:28\nLog-Likelihood:\n274.71\n\n\nNo. Observations:\n100\nAIC:\n-547.4\n\n\nDf Residuals:\n99\nBIC:\n-544.8\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nx1\n2.168e-19\n0.000\n1.04e-15\n1.000\n-0.000\n0.000\n\n\n\n\n\n\nOmnibus:\n41.519\nDurbin-Watson:\n0.564\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n139.378\n\n\nSkew:\n1.374\nProb(JB):\n5.43e-31\n\n\nKurtosis:\n8.089\nCond. No.\n1.00\n\n\n\nNotes:[1] R² is computed without centering (uncentered) since the model does not contain a constant.[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nConsidering the tightness of all of the datapoints in Figure 3.2 to the \\(X'\\) / PC1 axis, along with the total lack of correlation between the variables, one could potentially discard or disregard the \\(Y'\\) / PCA2 axis for the purpose of modeling.\nAbsent the PCA transformation, one could discard the y-datapoints and retain solely the x datapoints and the linear regression line equation \\(y=mx+b\\) between x and y such that one could re-calculate the y-value on-the-fly.\nBy performing PCA and transforming the variables in lieu of performing a recalculation of the y-value using the regression equation, the user can simply assume y to be zero in all cases and simply disregard the value. This allows the user to completely eliminate the feature while simultaneously retaining a high degree of explainability of variance within the data.\nThe eigenvalues of a PCA conveys the importance, and almost weight, of its corresponding eigenvector in explaining variation within the data. The eigenvalues are calucated from the covariance matrix of the original data, and then sorted in descending order, and then the corresponding eigenvectors, or basis vectors, are calculated and stored in a matrix. From here, eigenvalues and eigenvectors are pruned from the calculation based upon the needs of the user.\nThe dot product of the remaining eigenvectors and the original data then produce the PCA-transformed data.\nThe ratio of an individual eigenvalue over the sum total of all eigenvalues corresponds to an aforementioned weight or importance the corresponding eigenvector holds. This ratio corresponds to the amount of variance that is explained by the eigenvector and eigenvalue within the source data.\nIn best-case scenarios, one may have a high volume of dimensions, and many of those dimensions may have connections, correlations, or generally trend together. Due to the sheer volume thereof, it’s near impossible to visually inspect, determine, and prune correlated features from data. PCA, in the best case, allows a researcher to mathematically identify and trim all such correlations from the data, and capture all variation of the data within a fraction of the original dimensions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#pca-in-this-study",
    "href": "pca.html#pca-in-this-study",
    "title": "3  Principal Component Analysis",
    "section": "3.2 PCA in this Study",
    "text": "3.2 PCA in this Study\nThis study will explore PCA of select quantitative features of the cleaned dataset. While this will be explored, it is not necessarily ideal for the purpose and intent of this study.\nWhen leveraging principal components, one loses a degree of explainability for the data, and one arrives at a frame of reference that is not fully intuitive or easily digested. Each of the newly aligned axes, or principal components, is a linear combination of the original axes, and each datapoint is a combination of multiple features (e.g. the sum of 0.8 times feature A, 3.7 times feature B, 2.6 times feature C, as an arbitrary example). This makes the inputs and outputs less interpretable under direct observation.\nThis study is exploring the impact of categorical and numeric features and the strength of their predictive power in determining results or sources, i.e. \n\nCan one better predict mortgage outcomes (e.g. interest rate, approval or denial) from HMDA data when protected classes are included as predictors?\nCan one predict ones protected classes when using other available data about properties of the loan and the property it would purchase?\nHow strongly do these features lend themselves to such predictions?\n\nWith research questions like these in mind, performing PCA is less beneficial to explaining the outcomes.\nIf the HMDA data were to be used in conjunction with additional numerical information on the potential borrower, however, leveraging PCA could be of benefit. For instance, if additional features about the borrower such as total liquid savings, total invested dollars, credit score, age of credit history, and a substantial volume of other numeric variables, leveraging PCA could be beneficial in supporting assessment and analysis.\nIf a PCA of all of those numerics reduced the volume or dimension of the original data from, perhaps 20 to 5 features, it would simplify the process of training machine learning models. Whether building a multi-layer perceptron, performing a grid search with cross-validation of multiple model hyperparameters for models like logistic regression, support vector machines, ridge classifiers, or others - the reduction in dimensions reduces the computational time and complexity required to attain a more optimally-performing model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#data",
    "href": "pca.html#data",
    "title": "3  Principal Component Analysis",
    "section": "3.3 Data",
    "text": "3.3 Data\nTo perform PCA, there are a few requirements to ensure that the outcomes are relevant, valid, and (potentially) useful:\n\nThe data must not contain any labels (e.g. the dependent or response variable)\nThe data must include solely numeric data\n\nThe data must be standard scaled by the formula \\(\\frac{x-\\mu}{\\sigma}\\) where \\(x\\) is the variable in question, \\(\\mu\\) is the mean of \\(x\\), and \\(\\sigma\\) is the standard deviation of \\(x\\)\n\nFailure to perform this standard scaling will allow features with larger magnitudes to have a stronger impact on the outcome. To compare variances and the degree of explained variance, all features need to be on the same scale for comparison during PCA.\n\nThere are variables that may be represented as numbers in a source dataset, but the numbers leveraged in PCA must truly be numbers\nRemapping of categorical data to numbers cannot be performed\n\nWhen reducing dimensions in the transformed data, seek to retain a high degree of cumulative explained variance with the minimum number of dimensions required to achieve it\n\nWith these things in mind, only a small subset of the columns from the cleaned and consolidated HMDA dataset meet the numeric requirement for PCA. These columns include:\n\n\n\n\nTable 3.3: Numeric Columns Leveraged for PCA\n\n\n\n\n\n\n\n\nvariable\n\n\n\n\nproperty_value\n\n\nlender_credits\n\n\ndiscount_points\n\n\norigination_charges\n\n\ntotal_loan_costs\n\n\nproperty_value\n\n\nincome\n\n\ntotal_units\n\n\ntract_median_age_of_housing_units\n\n\ntract_minority_population_percent\n\n\nloan_to_value_ratio\n\n\ndebt_to_income_ratio\n\n\n\n\n\n\n\n\nFrom the initial efforts in data collection, all of these columns hold numeric and non-null data points to support calculation.\nThe data used to perform PCA is located here.\nData before Standard Scaling is as follows:\n\n\n\n\nTable 3.4: data, pre-standardization\n\n\n\n\n\n\n\n\nincome\ninterest_rate\ntotal_loan_costs\nloan_to_value_ratio\nloan_term\nintro_rate_period\ntotal_units\ntract_minority_population_percent\ntract_population\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\n\n\n\n\n95.500000\n4.250000\n18615.000000\n85.000000\n360.000000\n120.000000\n1\n17.150000\n5599\n130.240000\n2248\n2492\n36\n\n\n0.000000\n4.250000\n5907.500000\n21.429000\n360.000000\n120.000000\n1\n13.370000\n3911\n292.450000\n1239\n142\n0\n\n\n607.000000\n5.250000\n3055.000000\n80.000000\n360.000000\n84.000000\n1\n35.670000\n10542\n292.450000\n753\n48\n0\n\n\n202.000000\n5.125000\n7322.120000\n92.175000\n360.000000\n60.000000\n1\n40.330000\n22780\n276.610000\n5430\n6341\n12\n\n\n298.000000\n5.625000\n5325.000000\n65.574000\n360.000000\n84.000000\n1\n19.470000\n3344\n193.660000\n1061\n1128\n69\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.5: data, ,post-standardization\n\n\n\n\n\n\n\n\n\n\nincome\ninterest_rate\ntotal_loan_costs\nloan_to_value_ratio\nloan_term\nintro_rate_period\ntotal_units\ntract_minority_population_percent\ntract_population\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\n\n\n\n\n0\n-0.094708\n-3.227802\n3.056869\n0.264355\n0.236292\n6.982175\n-0.099862\n-0.921922\n0.337094\n0.390175\n1.745898\n1.286805\n-0.028225\n\n\n1\n-0.292129\n-3.227802\n-0.054903\n-2.930017\n0.236292\n6.982175\n-0.099862\n-1.068476\n-0.463000\n4.060474\n0.030686\n-2.060488\n-1.928347\n\n\n2\n0.962682\n-1.809658\n-0.753414\n0.013111\n0.236292\n4.823390\n-0.099862\n-0.203884\n2.680023\n4.060474\n-0.795472\n-2.194380\n-1.928347\n\n\n3\n0.125452\n-1.986926\n0.291505\n0.624891\n0.236292\n3.384200\n-0.099862\n-0.023212\n8.480705\n3.702065\n7.155021\n6.769243\n-1.294973\n\n\n4\n0.323907\n-1.277854\n-0.197544\n-0.711780\n0.236292\n4.823390\n-0.099862\n-0.831973\n-0.731752\n1.825169\n-0.271899\n-0.656050\n1.713554\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n203316\n0.168864\n0.140289\n0.429219\n-0.489378\n0.236292\n-0.213775\n-0.099862\n0.392025\n-0.670134\n-1.093014\n-1.274848\n-0.883950\n1.344085\n\n\n203317\n1.409205\n0.140289\n0.429219\n-0.238134\n0.236292\n-0.213775\n-0.099862\n1.187992\n-0.169601\n-2.556744\n-0.044111\n-0.505065\n-0.608818\n\n\n203318\n0.108914\n-0.036979\n0.265745\n-0.991867\n0.236292\n-0.213775\n-0.099862\n0.177622\n1.465188\n0.695637\n0.987737\n0.783999\n-1.400535\n\n\n203319\n-0.197036\n0.140289\n0.326358\n0.867342\n0.236292\n-0.213775\n-0.099862\n-0.046862\n-0.647382\n-0.420318\n-0.530286\n-0.509338\n0.552368\n\n\n203320\n-0.046128\n0.140289\n-0.032444\n0.013111\n0.236292\n-0.213775\n-0.099862\n-1.367012\n-0.509925\n0.404430\n0.251674\n0.033350\n0.446806\n\n\n\n\n203321 rows × 13 columns",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#code",
    "href": "pca.html#code",
    "title": "3  Principal Component Analysis",
    "section": "3.4 Code",
    "text": "3.4 Code\nThe code used to perform PCA is located here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#principal-component-analyses",
    "href": "pca.html#principal-component-analyses",
    "title": "3  Principal Component Analysis",
    "section": "3.5 Principal Component Analyses",
    "text": "3.5 Principal Component Analyses\n\n3.5.1 2D PCA\n\n\n\n\nTable 3.6: 2D PCA Calculation\n\n\n\n\n\n\n\n\nPC\nEigenvalues\nCumulative Variance\n\n\n\n\nPC1\n2.772669\n0.213281\n\n\nPC2\n1.477353\n0.326923\n\n\n\n\n\n\n\n\nThe 2D PCA achieves an explanation of approximately 32.6% of the variance in the source data.\n\n\n\n\n\n\n\n\nFigure 3.3: 2D PCA Visualization Plot\n\n\n\n\n\n\n\n3.5.2 3D PCA\n\n\n\n\nTable 3.7: 3D PCA Calculation\n\n\n\n\n\n\n\n\nPC\nEigenvalues\nCumulative Variance\n\n\n\n\nPC1\n2.772669\n0.213281\n\n\nPC2\n1.477353\n0.326923\n\n\nPC3\n1.225321\n0.421178\n\n\n\n\n\n\n\n\nThe 3D PCA achieves an explanation of approximately 42.1% of the variance in the source data.\n\n\n\n\n\n\n\n\nFigure 3.4: 3D PCA Visualization\n\n\n\n\n\n\n\n3.5.3 Multi-Dimensional PCA\n\n\n\n\nTable 3.8: Eigenvalues and Variance, all dimensions\n\n\n\n\n\n\n\n\nPC\nEigenvalues\nCumulative Variance\n\n\n\n\nPC1\n2.772669\n0.213281\n\n\nPC2\n1.477353\n0.326923\n\n\nPC3\n1.225321\n0.421178\n\n\nPC4\n1.123168\n0.507575\n\n\nPC5\n1.047675\n0.588165\n\n\nPC6\n0.976401\n0.663273\n\n\nPC7\n0.967920\n0.737728\n\n\nPC8\n0.931685\n0.809395\n\n\nPC9\n0.810819\n0.871766\n\n\nPC10\n0.700385\n0.925641\n\n\nPC11\n0.633995\n0.974410\n\n\nPC12\n0.218066\n0.991184\n\n\nPC13\n0.114606\n1.000000\n\n\n\n\n\n\n\n\nThe top three eigenvalues / eigenvectors are highlighted in each of Table 3.6, Table 3.7, and Table 3.8, of 2.77, 1.477, and 1.225.\nTo retain a minimum of 95% of the information in the dataset, the minimum required principal components are 11, allowing for dimensionality reduction of 2 dimensions from the original 13 while retaining the required amount information. The lowest value greater than 0.95 in the cumulative variance column is on the 11th row for Principal Component 11 at 0.974, thus one would have to include all components 1-11 to achieve at least 95% explained variance.\nThis finding suggests that the source data do not have strong correlations with one another, and thus do not provide much assistance in reducing dimensionality.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#multiple-correspondence-analysis",
    "href": "pca.html#multiple-correspondence-analysis",
    "title": "3  Principal Component Analysis",
    "section": "3.6 Multiple Correspondence Analysis",
    "text": "3.6 Multiple Correspondence Analysis\nMany of the research questions within this effort are inherently linked to categorical factors in lieu of numeric factors. Principal component analysis is only executable upon numeric data and not upon categorical data - even in the case of ordinal data. As a simple example, consider a ordinal variable “size” with categories small, medium, large, and extra-large. One could apply a simple encoding and assign small=1, medium=2, large=3, and extra-large=4. This encoding, while apparently holding a degree of validity in terms of increasing size, does not match up mathematically to reality. Consider getting a fountain drink at a fast-food restaurant and ask the question - is a large the same as 3 smalls? Is an extra large the same as 1 medium and two smalls? Rarely are either of these answers “yes”. One might have to add decimal places to the categories, and at that point, one may as well get the exact size measurements in terms of fluid ounces or liters, which may or may not be possible.\nThe additive and multiplicative challenges between these categories when assigning them a value produces challenges for ordinal variables. These challenges are further confounded when pivoting away from an ordinal variables. One runs the risk of making mathematical claims such as red is 4 times blue, or that sad is 3 less than happy. Such statements are nonsensical, have no foundation in mathematics, and while they may produce results in a model post-transformation, do not hold validity, explainability, or generalizability.\nEnter Multiple Correspondence Analysis (or MCA). MCA performs an analogous action on categorical variables as PCA performs upon numeric variables. To perform an MCA, one must construct a Complete Disjunctive Table, which is effectively a one-hot encoded matrix. One takes the source categorical columns and transforms them to a column per category, and for the new column, the value is set to 1 if the current row is a member of the category, and zero otherwise. This is repeated for all columns and categories until the dataset is fully expanded.\n\n\n\n\n\n\n\na\nb\n\n\n\n\ns\nf\n\n\nm\nw\n\n\ns\ns\n\n\ns\nf\n\n\nm\nf\n\n\nm\nf\n\n\nl\nw\n\n\nl\ns\n\n\nl\nf\n\n\nm\nw\n\n\n\n\n\nTaking the above example table, one can transform it to a one-hot encoded table:\n\n\n\n\n\n\n\na_l\na_m\na_s\nb_f\nb_s\nb_w\n\n\n\n\n0\n0\n1\n1\n0\n0\n\n\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n1\n0\n\n\n0\n0\n1\n1\n0\n0\n\n\n0\n1\n0\n1\n0\n0\n\n\n0\n1\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n0\n\n\n0\n1\n0\n0\n0\n1\n\n\n\n\n\nNotice that there are now have 6 columns from the original 2 columns. This is because column ‘a’ had 3 categories - s/m/l, as did column ‘b’ - s/w/f. A column is created for each combination of individual columns and their respective categories, hence 6 columns in this case.\nAfter performing this transformation, the following mathematical operations are applied:\n\nCalculate the sum of all values (0s and 1s) from the CDT as value \\(N\\)\nCalculate matrix \\(Z = \\frac{CDT}{N}\\)\nCalculate the column-wise sum as matrix \\(c\\). Transform to a diagonal matrix \\(D_c\\)\nCalculate the row-wise sum as matrix \\(r\\). Transform to a diagonal matrix \\(D_r\\)\nCalculate matrix \\(M = D_r^{-\\frac{1}{2}}(Z-rc^T)D_c^{-\\frac{1}{2}}\\)\n\nDue to some unforeseen challenges during this research, performance of MCA will be delayed until a later date. This type of analysis will be useful for future modeling purposes and further analysis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#results",
    "href": "pca.html#results",
    "title": "3  Principal Component Analysis",
    "section": "3.7 Results",
    "text": "3.7 Results\nPrincipal component analysis on the source data for this research is not an ideal endeavor, as this effort seeks to further establish the strength and connection of certain numeric and categorical variables to the outcome of whether or not an applicant will attain a mortgage.\nFurthermore, the need of the data to retain 11 of the 13 principal components to explain most of the data variation means that applying PCA to this data will not meet any dimensionality reduction goals for this research. There is little direct correlation between the variables in the source data, so it takes almost the same number of dimensions as we had in the source data to retain a high degree of explainability. As such, principal component analysis, solely performed on the identified numeric variables, may be insufficient for the purposes of clustering and modeling.\nThe multiple correspondence analysis, however, seems to lend itself well to the purposes and intent of this research. A limited degree of exploration into MCA was pursued, but not sufficiently enough to generate or communicate results at this time. In the next iteration of this research, MCA will be included in the analyses. MCA, used within clustering and potentially within modeling, could be more relevatory than basic numeric measures provided by PCA.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "mca.html",
    "href": "mca.html",
    "title": "4  Multiple Correspondence Analysis",
    "section": "",
    "text": "4.1 Overview\nMany of the research questions within this effort are inherently linked to categorical factors in lieu of numeric factors. Principal component analysis is only executable upon numeric data and not upon categorical data - even in the case of ordinal data. As a simple example, consider a ordinal variable “size” with categories small, medium, large, and extra-large. One could apply a simple encoding and assign small=1, medium=2, large=3, and extra-large=4. This encoding, while apparently holding a degree of validity in terms of increasing size, does not match up mathematically to reality. Consider getting a fountain drink at a fast-food restaurant and ask the question - is a large the same as 3 smalls? Is an extra large the same as 1 medium and two smalls? Rarely are either of these answers “yes”. One might have to add decimal places to the categories, and at that point, one may as well get the exact size measurements in terms of fluid ounces or liters, which may or may not be possible.\nThe additive and multiplicative challenges between these categories when assigning them a value produces challenges for ordinal variables. These challenges are further confounded when pivoting away from an ordinal variables. One runs the risk of making mathematical claims such as red is 4 times blue, or that sad is 3 less than happy. Such statements are nonsensical, have no foundation in mathematics, and while they may produce results in a model post-transformation, do not hold validity, explainability, or generalizability.\nEnter Multiple Correspondence Analysis (or MCA). MCA performs an analogous action on categorical variables as PCA performs upon numeric variables. To perform an MCA, one must construct a Complete Disjunctive Table, which is effectively a one-hot encoded matrix. One takes the source categorical columns and transforms them to a column per category, and for the new column, the value is set to 1 if the current row is a member of the category, and zero otherwise. This is repeated for all columns and categories until the dataset is fully expanded.\na\nb\n\n\n\n\ns\nf\n\n\nm\nw\n\n\ns\ns\n\n\ns\nf\n\n\nm\nf\n\n\nm\nf\n\n\nl\nw\n\n\nl\ns\n\n\nl\nf\n\n\nm\nw\nTaking the above example table, one can transform it to a one-hot encoded table:\na_l\na_m\na_s\nb_f\nb_s\nb_w\n\n\n\n\n0\n0\n1\n1\n0\n0\n\n\n0\n1\n0\n0\n0\n1\n\n\n0\n0\n1\n0\n1\n0\n\n\n0\n0\n1\n1\n0\n0\n\n\n0\n1\n0\n1\n0\n0\n\n\n0\n1\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n0\n1\n\n\n1\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n0\n\n\n0\n1\n0\n0\n0\n1\nNotice that there are now have 6 columns from the original 2 columns. This is because column ‘a’ had 3 categories - s/m/l, as did column ‘b’ - s/w/f. A column is created for each combination of individual columns and their respective categories, hence 6 columns in this case.\nAfter performing this transformation, the following mathematical operations are applied:\narray([[0.  , 0.  , 0.05, 0.05, 0.  , 0.  ],\n       [0.  , 0.05, 0.  , 0.  , 0.  , 0.05],\n       [0.  , 0.  , 0.05, 0.  , 0.05, 0.  ],\n       [0.  , 0.  , 0.05, 0.05, 0.  , 0.  ],\n       [0.  , 0.05, 0.  , 0.05, 0.  , 0.  ],\n       [0.  , 0.05, 0.  , 0.05, 0.  , 0.  ],\n       [0.05, 0.  , 0.  , 0.  , 0.  , 0.05],\n       [0.05, 0.  , 0.  , 0.  , 0.05, 0.  ],\n       [0.05, 0.  , 0.  , 0.05, 0.  , 0.  ],\n       [0.  , 0.05, 0.  , 0.  , 0.  , 0.05]])\nc:\n[[0.15 0.2  0.15 0.25 0.1  0.15]]\nDc: [[0.15 0.   0.   0.   0.   0.  ]\n [0.   0.2  0.   0.   0.   0.  ]\n [0.   0.   0.15 0.   0.   0.  ]\n [0.   0.   0.   0.25 0.   0.  ]\n [0.   0.   0.   0.   0.1  0.  ]\n [0.   0.   0.   0.   0.   0.15]]\nr:\n[[0.1]\n [0.1]\n [0.1]\n [0.1]\n [0.1]\n [0.1]\n [0.1]\n [0.1]\n [0.1]\n [0.1]]\nDr: [[0.1 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.1 0.  0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.1 0.  0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.1 0.  0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.1 0.  0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.1 0.  0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.1 0.  0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.1 0.  0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.1 0. ]\n [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1]]\nmatrix([[-0.12247449, -0.14142136,  0.2857738 ,  0.15811388, -0.1       ,\n         -0.12247449],\n        [-0.12247449,  0.21213203, -0.12247449, -0.15811388, -0.1       ,\n          0.2857738 ],\n        [-0.12247449, -0.14142136,  0.2857738 , -0.15811388,  0.4       ,\n         -0.12247449],\n        [-0.12247449, -0.14142136,  0.2857738 ,  0.15811388, -0.1       ,\n         -0.12247449],\n        [-0.12247449,  0.21213203, -0.12247449,  0.15811388, -0.1       ,\n         -0.12247449],\n        [-0.12247449,  0.21213203, -0.12247449,  0.15811388, -0.1       ,\n         -0.12247449],\n        [ 0.2857738 , -0.14142136, -0.12247449, -0.15811388, -0.1       ,\n          0.2857738 ],\n        [ 0.2857738 , -0.14142136, -0.12247449, -0.15811388,  0.4       ,\n         -0.12247449],\n        [ 0.2857738 , -0.14142136, -0.12247449,  0.15811388, -0.1       ,\n         -0.12247449],\n        [-0.12247449,  0.21213203, -0.12247449, -0.15811388, -0.1       ,\n          0.2857738 ]])\nmatrix([[ 0.2668487 , -0.36973681, -0.02965527, -0.33505172,  0.40790586,\n          0.05256851],\n        [-0.44855833,  0.05243584,  0.3155619 , -0.06604041, -0.61898123,\n          0.17517594],\n        [ 0.5022207 ,  0.09366613,  0.5636881 ,  0.07394102, -0.17747711,\n         -0.43474836],\n        [ 0.2668487 , -0.36973681, -0.02965527, -0.33505172, -0.44893205,\n         -0.20433625],\n        [-0.16826172, -0.2934583 , -0.12732264,  0.42101532, -0.00278799,\n         -0.40682761],\n        [-0.16826172, -0.2934583 , -0.12732264,  0.42101532, -0.00278799,\n         -0.40682761],\n        [-0.1994144 ,  0.45207925, -0.19614346, -0.49896387,  0.06725846,\n         -0.61814986],\n        [ 0.31625419,  0.56958804, -0.04568462,  0.39708461,  0.03180824,\n          0.04373763],\n        [ 0.0808822 ,  0.1061851 , -0.639028  , -0.01190814, -0.31757   ,\n         -0.01210386],\n        [-0.44855833,  0.05243584,  0.3155619 , -0.06604041,  0.33321947,\n         -0.14354217]])\n\n\nmatrix([[ 0.09313213, -0.50322667,  0.48794398,  0.10145015,  0.47216689,\n         -0.51649423],\n        [ 0.58423147, -0.21624736, -0.33453053, -0.4895997 ,  0.42078311,\n          0.28850252],\n        [-0.58423147,  0.21624736,  0.33453053, -0.4895997 ,  0.42078311,\n          0.28850252],\n        [-0.09313213,  0.50322667, -0.48794398,  0.10145015,  0.47216689,\n         -0.51649423],\n        [ 0.5451003 ,  0.62942762,  0.5451003 , -0.06910931, -0.04370857,\n         -0.05353184],\n        [ 0.05353184,  0.06181325,  0.05353184,  0.70372147,  0.44507253,\n          0.5451003 ]])\narray([7.51207074e-01, 6.21131266e-01, 3.78868734e-01, 2.48792926e-01,\n       1.95441618e-32, 3.22830823e-33])\nTo maximize the use of variables within the dataset and to support the answering of various research questions, performing MCA on transformations of the data is necessary.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis</span>"
    ]
  },
  {
    "objectID": "mca.html#overview",
    "href": "mca.html#overview",
    "title": "4  Multiple Correspondence Analysis",
    "section": "",
    "text": "Calculate the sum of all values (0s and 1s) from the CDT as value \\(N\\)\nCalculate matrix \\(Z = \\frac{CDT}{N}\\)\n\n\n\nCalculate the column-wise sum as matrix \\(c\\). Transform to a diagonal matrix \\(D_c\\)\n\n\n\nCalculate the row-wise sum as matrix \\(r\\). Transform to a diagonal matrix \\(D_r\\)\n\n\n\nCalculate matrix \\(M = D_r^{-\\frac{1}{2}}(Z-rc^T)D_c^{-\\frac{1}{2}}\\)\n\n\n\nPerform Matrix decomposition on \\(M\\):\nseek two unitary matrices (e.g. of total length 1), P and Q, and the generalized diagonal matrix of singular values \\(\\Delta\\) such that \\(M=P\\Delta Q^T\\)\n\n\n\n\\(\\Delta^2\\) provides the eigenvalues of the target matrix.\n\n\n\nUse the eigenvalues to apply transformations of the input data into a new the new eigenbasis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis</span>"
    ]
  },
  {
    "objectID": "mca.html#data-code",
    "href": "mca.html#data-code",
    "title": "4  Multiple Correspondence Analysis",
    "section": "4.2 Data & Code",
    "text": "4.2 Data & Code\nThe data used the source data for this effort, converted into a one-hot encoded / sparse matrix format of the data. The code and applied transformations can be seen in Appendix G.\nThe completed transformed data has two forms. One of these forms is a transformation that includes all protected class variables (age, gender, and race), and the second form does not contain these variables. These two different forms allow exploration of the research questions for this effort.\n(add notes on how numerics were converted to categories).\n\nSource Data for Transformation\nMCA with protected class features - transformed data\nMCA with protected class features - Eigenvalue Summary\nMCA with protected class features - Column Contributions\nMCA without protected class features\nMCA without protected class features - Eigenvalue Summary\nMCA without protected class features - Column Contributions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis</span>"
    ]
  },
  {
    "objectID": "mca.html#results",
    "href": "mca.html#results",
    "title": "4  Multiple Correspondence Analysis",
    "section": "4.3 Results",
    "text": "4.3 Results\n\n\n\n\n\n\nMCA Summary of Eigenvalues (with protected class information) - Top 5 {#cell-mca-nd-eig-top}\n\n\n\neigenvalue\n% of variance\n% of variance (cumulative)\n\n\n\n\n0\n0.215\n4.69%\n4.69%\n\n\n1\n0.167\n3.63%\n8.32%\n\n\n2\n0.166\n3.62%\n11.93%\n\n\n3\n0.115\n2.49%\n14.43%\n\n\n4\n0.102\n2.21%\n16.64%\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCA Summary of Eigenvalues (with protected class information) - Bottom 5 {#cell-mca-nd-eig-bot}\n\n\n\neigenvalue\n% of variance\n% of variance (cumulative)\n\n\n\n\n176\n0.001\n0.02%\n99.96%\n\n\n177\n0.001\n0.01%\n99.97%\n\n\n178\n0.000\n0.01%\n99.98%\n\n\n179\n0.000\n0.01%\n99.99%\n\n\n180\n0.000\n0.01%\n100.00%\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCA Summary of Eigenvalues (without protected class information) - Top 5 {#cell-mca-nd-npc-eig-top}\n\n\n\neigenvalue\n% of variance\n% of variance (cumulative)\n\n\n\n\n0\n0.124\n3.21%\n3.21%\n\n\n1\n0.116\n3.01%\n6.23%\n\n\n2\n0.100\n2.61%\n8.83%\n\n\n3\n0.086\n2.23%\n11.06%\n\n\n4\n0.085\n2.22%\n13.28%\n\n\n\n\n\n\n\n\n\n\n\n\n\nMCA Summary of Eigenvalues (without protected class information) - Bottom 5 {#cell-mca-nd-npc-eig-bot}\n\n\n\neigenvalue\n% of variance\n% of variance (cumulative)\n\n\n\n\n95\n0.009\n0.24%\n99.52%\n\n\n96\n0.007\n0.17%\n99.69%\n\n\n97\n0.007\n0.17%\n99.86%\n\n\n98\n0.004\n0.11%\n99.97%\n\n\n99\n0.001\n0.03%\n100.00%\n\n\n\n\n\n\n\nMCA doesn’t necessarily provide direct dimensionality reduction, but does enable one to reduce dimensions from a sparse matrix. Instead it enables use of more variables while also (potentially) increasing the data’s dimensionality. The sparse matrices to produce the transformations had 243 columns (with protected) and 179 columns (without protected).\nThe outcomes of the transformations allow substantial dimensionality reduction from these sparse matrices. With 181 components, the data containing protected class information achieved (99.99%) explained variance in the source data (reduction of 62 features). Similarly, with 100 components, the data excluding protected class information achieved 99.99% explained variance (reduction of 79 features).\nMCA also provides us with insight as to which columns provide the greatest contributions to each primary component in the output data. Exploring some of these provides interesting insights. To explore these, we’ll look at the first 3 components, sorted in descending order, and examine which columns from the source data provide the strongest contributions to the transformation:\n\n4.3.1 With Protected Classes\n\n\n\n\n\n\nMC1 (with protected class information) - Top 10 Column Contributors {#cell-mca-col-conts1}\n\n\n\nColumn\nMC1\n\n\n\n\n121\nco-applicant_sex_observed_2\n0.061418\n\n\n96\nco-applicant_ethnicity_observed_2\n0.061381\n\n\n103\nco-applicant_race_observed_2\n0.061369\n\n\n1\nderived_sex_Joint\n0.053307\n\n\n90\nco-applicant_credit_score_type_9\n0.047954\n\n\n229\nco-applicant_ethnicity_Not Hispanic/Latino\n0.046580\n\n\n98\nco-applicant_ethnicity_observed_4\n0.045165\n\n\n140\nco-applicant_age_8.0\n0.045165\n\n\n105\nco-applicant_race_observed_4\n0.045165\n\n\n123\nco-applicant_sex_observed_4\n0.045165\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2 (with protected class information) - Top 10 Column Contributors {#cell-mca-col-conts2}\n\n\n\nColumn\nMC2\n\n\n\n\n196\napplicant_race_Not Applicable\n0.141186\n\n\n131\napplicant_age_7.0\n0.141186\n\n\n109\napplicant_sex_4\n0.141186\n\n\n101\napplicant_race_observed_3\n0.141186\n\n\n94\napplicant_ethnicity_observed_3\n0.141186\n\n\n119\napplicant_sex_observed_3\n0.141186\n\n\n223\napplicant_ethnicity_Not Applicable\n0.141186\n\n\n231\nco-applicant_ethnicity_Not Applicable\n0.001395\n\n\n214\nco-applicant_race_Not Applicable\n0.001395\n\n\n139\nco-applicant_age_7.0\n0.001395\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC3 (with protected class information) - Top 10 Column Contributors {#cell-mca-col-conts3}\n\n\n\nColumn\nMC3\n\n\n\n\n231\nco-applicant_ethnicity_Not Applicable\n0.141338\n\n\n214\nco-applicant_race_Not Applicable\n0.141338\n\n\n139\nco-applicant_age_7.0\n0.141338\n\n\n122\nco-applicant_sex_observed_3\n0.141338\n\n\n114\nco-applicant_sex_4\n0.141338\n\n\n104\nco-applicant_race_observed_3\n0.141338\n\n\n97\nco-applicant_ethnicity_observed_3\n0.141338\n\n\n119\napplicant_sex_observed_3\n0.001412\n\n\n223\napplicant_ethnicity_Not Applicable\n0.001412\n\n\n94\napplicant_ethnicity_observed_3\n0.001412\n\n\n\n\n\n\n\nExamining the above three tables, it is evident that features revolving around protected class information contribute substantially to each Multiple Correspondence Component (MC). For each of the MCs, much of the contributions come from feature values revolving around all protected classes of age, sex, race, and ethnicity. Using data of this nature could easily produce predictive outcomes of models trained with with biases (either for or against) testing data that fits within these categories.\n\n\n4.3.2 Without Protected Classes\n\n\n\n\n\n\nMC1 (without protected class information) - Top 10 Column Contributors {#cell-mca-npc-col-conts1}\n\n\n\nColumn\nMC1\n\n\n\n\n29\norigination_charges_H\n0.088933\n\n\n25\ntotal_loan_costs_H\n0.084998\n\n\n32\ndiscount_points_H\n0.076381\n\n\n14\nloan_amount_ML\n0.054607\n\n\n27\ntotal_loan_costs_MH\n0.049097\n\n\n31\norigination_charges_MH\n0.047695\n\n\n50\nproperty_value_ML\n0.043231\n\n\n34\ndiscount_points_MH\n0.033078\n\n\n0\npurchaser_type_0\n0.032864\n\n\n9\nopen-end_line_of_credit_1\n0.030098\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC2 (without protected class information) - Top 10 Column Contributors {#cell-mca-npc-col-conts2}\n\n\n\nColumn\nMC2\n\n\n\n\n47\nproperty_value_H\n0.078352\n\n\n11\nloan_amount_H\n0.075937\n\n\n81\napplicant_credit_score_type_9\n0.062141\n\n\n56\nincome_MH\n0.051055\n\n\n0\npurchaser_type_0\n0.051019\n\n\n44\nintro_rate_period_H\n0.042191\n\n\n121\ncompany_Bank of America\n0.040706\n\n\n9\nopen-end_line_of_credit_1\n0.034975\n\n\n124\ncompany_Rocket Mortgage\n0.034505\n\n\n49\nproperty_value_MH\n0.031335\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC3 (without protected class information) - Top 10 Column Contributors {#cell-mca-npc-col-conts3}\n\n\n\nColumn\nMC3\n\n\n\n\n107\ntract_owner_occupied_units_H\n0.089676\n\n\n112\ntract_one_to_four_family_homes_H\n0.080570\n\n\n111\ntract_owner_occupied_units_ML\n0.077914\n\n\n88\ntract_population_H\n0.073417\n\n\n115\ntract_one_to_four_family_homes_MH\n0.056009\n\n\n110\ntract_owner_occupied_units_MH\n0.051876\n\n\n116\ntract_one_to_four_family_homes_ML\n0.049923\n\n\n106\ntract_to_msa_income_percentage_ML\n0.044282\n\n\n92\ntract_population_ML\n0.037601\n\n\n91\ntract_population_MH\n0.037590\n\n\n\n\n\n\n\nExamining the MCs for data that exludes protected class information, we immediately see that each holds data that is likely highly relevant to making a decision of whether or not to approve a loan. The _H and _MH in MC1 column contributions signify that the loan is between 1 and 2 standard deviations, or over 2 standard deviations from the mean. With high loan costs, high origination charges, high discount points and so forth all are likely candidates to impact the decision making process of whether or not to grant a loan.\nIt’s also interesting to see that in MC2, two banks stand out - Bank of America and Rocket Mortgage, apparently having a higher influence on the loan outcome in the 2nd MC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "5  Data Clustering",
    "section": "",
    "text": "5.1 Overview\nClustering is a combination of mathematical methods for identifying potential groupings or similarities between records within a dataset. This research explores use of K-Means, hierarchical, and density based clustering methods to examine the numeric data that was produced in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#overview",
    "href": "clustering.html#overview",
    "title": "5  Data Clustering",
    "section": "",
    "text": "5.1.1 K-Means\nK-Means clustering is a partitional clustering method that identifies and divides the data into distinct groups or clusters by using the expectation maximization algorithm. Conceptually, the algorithm is simple to understand and explain:\n\npick the number of clusters you wish to identify, k\nselect k-random points within the range of the data to serve as the initial cluster centers\nmeasure the distance from each point \\(x_i\\) to each of the \\(k_i\\) centers\nassign the current point \\(x_i\\) as being a member of center \\(k_i\\) if the distances between \\(x_i\\) and \\(k_i\\) is lower than the distance from \\(x_i\\) to any other center \\(k\\)\nfor every center \\(k\\), recalculate its location by taking the average of every point \\(x_i\\) that has been assigned as a member of \\(k\\).\nrepeat steps 4-6 above until either the centers stop moving beyond a certain threshold or until a desired number of iterations has been reached\n\n\n\n5.1.2 Density-Based Clustering\nDensity based clustering, like Density-Based Spatial Clustering of Applications with Noise or DBSCAN, can be used to find and identify clusters with more complex shapes and boundaries than that of partitional clustering methods like K-Means.\nDBSCAN operates under a different paradigm. Where K-Means operates off of distance between points and a center, DBSCAN operates off of distance between every point to determine how many points are in its neighborhood. As such, DBSCAN has two primary parameters, a maximum distance and a minimum number of points. To measure density, in general, one seeks to measure how many items are within a limited amount of space.\nDBSCAN measures a point’s density by how many other points are within the established maximum distance from that point. If that point meets or exceeds the established minimum number of points, then the point is considered a dense point or part of the core. Otherwise, the point is not considered dense. This action is measured and performed for every datapoint.\nAfter the measurements are performed, a dense point is chosen at random to be assigned as a member of a cluster. Then, all of its dense neighbors are assigned to be members of the same cluster. This process continues until all dense neighbors within a cluster are assigned as members of the cluster. Then, any non-dense points that directly neighbor dense points within the cluster are also assigned to the cluster. Non-dense, non-neighboring points are not assigned at this time.\nThe above process is repeated for other dense points that were not assigned as part of the first cluster until all dense points (and all non-dense points that neighbor dense clusters) are assigned as members of a cluster. Any remaining non-neighboring non-dense points are then assigned as members of an outlier cluster (typically denoted by -1 in most density clustering algorithms).\nBecause of the concept of this snaking and neighboring point adjacencies by density, DBSCAN can identify and connect points in a complex manner that K-Means cannot.\n\n\n5.1.3 Hierarchical Clustering\nHierarchical clustering is a methodology that allows one to divide or aggregate datapoints within a dataset based upon either a desired number of overarching clusters, or by a distance metric, and building a hierarchy to outline the cluster in which a datapoint belongs. Hierarchical clustering can be approached via aggregating many small clusters (agglomerative) or by dividing the data into progressively smaller clusters (divisive).\nHierarcical clustering can be of benefit for highly dimensional data. \n\n\n5.1.4 Clustering In This Study\nClustering is explored in this study to ascertain any connection between the source data variables and the outcomes of each mortgage application. If clusters could be produced that map data points to their outcomes (almost in a predictive manner), this could be of benefit, even if the cluster label is only sometimes correct. The cluster label could be used as a numeric factor in modeling to support improved predictive accuracy.\nFurthermore, the clustering will be used (when MCA analysis is complete) to evaluate the impact and influence of protected class variables when attempting to marry clustering results to loan outcomes. If protected classes are impactful and can effectively cluster a loan application to its outcome, this may warrant further investigation as to why.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#data",
    "href": "clustering.html#data",
    "title": "5  Data Clustering",
    "section": "5.2 Data",
    "text": "5.2 Data\nThe source data available for clustering in this research contains over 200k records. Hierarchical and density-based clustering methods both take an extensive amount of memory resources. In order to perform these clustering methods on the available data, a stratified sample (on outcome) will be taken from the data and these clustering methods will be performed on the samples. The label is temporarily added to the PCA-transformed data, a stratified sample of approximately 20% of the dataset is then taken, and clustering is performed upon this sample.\nThe data used to perform clustering is based solely on the 3D PCA performed on numeric variables. The 3D PCA transformed dataset is located here. The steps to produce this dataset are covered in Chapter 3. Below is the head of the dataframe of principal components used for clustering. This dataframe is the 20% sample that is leveraged for density-based and hierarchical clustering.\n\n\n\n\nTable 5.1: 3D PCA Data\n\n\n\n\n\n\n\n\nPC0\nPC1\nPC2\n\n\n\n\n-0.892240\n-0.448016\n-0.182751\n\n\n-0.931789\n-0.758654\n-1.987403\n\n\n-0.790594\n-0.409078\n1.276530\n\n\n-2.012753\n0.254438\n1.666342\n\n\n0.755317\n2.409991\n1.879752\n\n\n\n\n\n\n\n\nData, before any transformations, can be viewed in Table 3.4, and standard scaled data in Table 3.5.\nBefore the completion of this research, clustering will also be performed on MCA-transformed data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#code",
    "href": "clustering.html#code",
    "title": "5  Data Clustering",
    "section": "5.3 Code",
    "text": "5.3 Code\nThe code to clean and prepare the data for clustering is located here and written in Python. The code will include MCA transformations in the future. Additionally, this code leveraged an example from Scikit-Learn to best identify the number of clusters to use in K-Means for the PCA-transformed data.\n\n5.3.1 K-Means Clustering\nThe silhouette method was leveraged to identify several cluster options for K-Means using the 3D PCA dataset. This uses silhouette scores to determine how well datapoints are mapped to clusters and how well separated the clusters are within the data.\nUsing this method the best performing cluster counts included k=2, k=4 and k=5\n\n\n\nK=2 K-Means\n\n\nK=2 K-Means Clustering had a silhouette score of 0.256.\n\n\n\nK=4 K-Means\n\n\nK=4 K-Means Clustering had a silhouette score of 0.362.\n\n\n\nK=5 K-Means\n\n\nK=5 K-Means Clustering had a silhouette score of 0.381.\n\n\n5.3.2 Hierarchical Clustering\nHierarchical clustering was performed on the 3d-transformed PCA data produced in Chapter 3. The hierarchy produced 2 main clusters, which may be beneficial for a binary classification problem such as predicting loan approval or denial. If the clusters had some ties to the actual grouping of approvals or denials, then this could be of benefit.\nHowever, as mentioned in Chapter 3, the 3d transformation of numeric-only data resulted in an explained variance of only about 43%. As such, the two produced clusters are unlikely to have any direct ties to the actual outcomes in the source data.\n\n\n\nDendrogram for Hierarchical Clustering\n\n\nThe dendrogram is challenging to interpret in this case, as it’s splitting the data on a roughly PCA-transformed dataset with limited explainability in the data’s variance. Furthermore, the base labels of the dendrogram are simply based upon the datapoints indices themselves, and as such the dendrogram doesn’t add substantial information or interpretability in this context. Performing a plot of the points and their classifications will likely be a more fruitful endeavor.\nFurthermore, the silhouette score for the hierarchical clustering was 0.266, which is fairly low. Generally, a score closer to zero signifies indifference or poor matching and grouping of the data into potentially meaningful clusters. That being said, the performance of hierarchical clustering in comparison to K-Means pales in comparison. Even the worst performing K-means cluster of K=2 was approximately equivalent to hierarcical clustering (0.256 vs. 0.266).\n\n\n5.3.3 Density Clustering (DBSCAN)\nDensity-based clustering can support a researcher in identifying nested, non-uniformly sized or shaped clusters in multiple dimensions. Density can also be leveraged to cluster and group data together while isolating outliers that are in low-density regions. By examining points in the source data and measuring point density and distance, points continuously meeting the density criteria are added to the same cluster.\nPoints failing to meet the density criteria can only be added to a cluster, but are not considered for the continuity criteria for extending the cluster. Some of these points are added to clusters, and others are considered outliers (e.g. when they are substantially distant from any cluster points).\nThis methodology can produce different size clusters in different dimensions, and follow paths and patterns within the data based upon point densities.\nPerforming density-based clustering using DBSCAN on the source data produced a silhouette score of approximately 0.33.\n\n\n\n\n\n\n\n\nFigure 5.1: DBSCAN Cluster Results",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#results",
    "href": "clustering.html#results",
    "title": "5  Data Clustering",
    "section": "5.4 Results",
    "text": "5.4 Results\nWhat is the difference in the outputs for the data, though? And how do those compare to the actual underlying labels?\nTo visualize, one can take the K-Means clusters and apply the same subset of the data as used for Agglomerative and DBSCAN, and also visualize the source data’s labels. This is done in the below figure.\n\n\n\n\n\n\n\n\nFigure 5.2: Summarized Cluster Results, all methods, (20% of source data)\n\n\n\n\n\nThe outcome of these clusters does not directly further the intent of this research. In part, this is likely due to the small degree of variance (42%) that is encapsulated within a 3D PCA. The silhouette score for the 3D PCA with its own labels is shockingly low at an approximate value of -0.0096, meaning that the nature of this data makes it difficult to perform unsupervised clustering and potentially arrive at meaningful labels that are connected or related to the actual labels.\nFurthermore, additional challenges are reflected by the silhouette scoring for both K-Means and agglomerative below 0.3, and DBSCAN with a silhoutte score of approximately 0.41. These relatively low scores signify that none of these clustering methods, performed on the available data, has an ability to establish clusters that are substantially far or distinct from one another.\nPerforming clustering on MCA-transformed data, which will encapsulate categorical values in an appropriate transformation to numeric values, will likely give a far more interesting cluster analysis. Since this research seeks to explore categorical variables and their potential impacts to outcome of loan applications, this transformation will be performed, analyzed, and potentially used in the models for this research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#conclusion",
    "href": "clustering.html#conclusion",
    "title": "5  Data Clustering",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nWhat do these technical results signify to the casual reader? The numeric data alone in HMDA records may not be sufficient to examine trends, interesting patterns, or predictions (such as clusters) within US mortgage applications. This could be due to data that is available, but not included in the attempts to cluster (such as categorical variables or the reduced dimensions), or data that is unavailable within HMDA records - outside influencers or causes that place a mortgage application into the “approve” or “deny” pile.\nThe K-Means clustering results at multiple levels didn’t produce strong, distinct, or separated clusters. Hierarchical clustering performed better in terms of cluster distinctness, but fails to resemble the actual outcomes for those loan applications. Density clustering also failed to produce anything resembling the source outcomes.\nNone of the clustering methods produced labels that resembled actual outcomes for each application. Additionally, examining the actual labels, there’s a fairly evident interspersing of the outcomes (rejection and approval) amongst one another in the source data, as is clear in Figure 5.2. With data in this form and quality, it is unlikely that a clustering method could differentiate between interesting groups within such a homogenous space.\nProducing MCA transformations of categorical variables in combination with these numeric variables is the best chance this research will have to seek out more meaningful clusters in comparison to the loan outcomes of approval and denial.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "6  Association Rule Mining",
    "section": "",
    "text": "6.1 Overview\nAssociation Rule Mining, henceforth ARM in this section, is the process of identifying commonly occuring combinations within a dataset. The method allows one to explore and uncover potentially unknown associations and groupings in data that are not necessarily immediately evident upon a direct human inspection.\nA common application of these methods is that of assessing common purchases by customers in a store or e-commerce website (e.g. if a customer buys a portable music player, is it common or frequent for those customers to purchase a USB cable, or Bluetooth headphones, or potentially both?). Manually inspecting or searching through the data with ad-hoc or independently built algorithms may not be effective or efficient to identify such patterns. When considering an application such as customer purchases, or transactional data, the possibility of combinations of items in each purchase is conceptually unbounded, and using brute-force methods to seek out these combinations will exceed the available computational resources of a machine before identifying anything of use.\nSo, to examine transactions for potentially insightful or useful metrics, one needs to go about said search wisely and efficiently. This is where a handful of algorithms and methods come into play, including Frequent Pattern growth and Apriori.\nWhat is a frequent pattern?\nA frequent pattern is defined as a collection of items that occur at or above a specified threshold within a dataset of transactions. When measuring frequent patterns, each item in the collection is unique, and multiple instances of the same item within an individual transaction are ignored so as to bring focus on the occurence of unique items being grouped together within the dataset. Since the threshold can be specified by the person conducting association rule mining, frequent is a relative and subjective term in this context, as it is based on the relative frequency of occurence of the group of items, and whether or not that relative frequency exceeds an arbitrary threshold.\nCommon items that are connected, grouped, or bundled together are be referred to as frequent patterns within datasets.\nHow does apriori work?\nThe Apriori algorithm leverages bayesian probability and induction to… To perform apriori ARM, one must set thresholds for the metric for which one is measuring the data. This is key, because depending on the number of unique items or the number of transactions within the dataset in question, the\nTo search for these connections and associations, the method leverages Bayes rule for probability, metrics such as entropy or Gini indices within the Apriori algorithm. The algorithm iteratively and inductively examines data for frequent patterns, generally, in the following manner -\nSo one can see that as the apriori algorithm proceeds, it does have an eventual halting point, depending on the initially established thresholds. The predominant metric and threshold is the relative frequency of occurence of an item (or combination of items). This relative frequency is also known as the support of the combination of items within the dataset.\nWhat is an association rule?\nAn association rule goes beyond individual relative frequencies of items or combinations thereof, and begin telling more about how strongly connected certain item combinations are within the data. ARM establishes a connection between an antecedent (or prior) and a consequent (or posterior) set of items within the dataset. The combination of antecedents and consequents are what form assosciation rules. It is similar to asking the question, given a customer has already placed items \\(A\\) and \\(B\\) in their shopping cart, what is the probability or likelihood that they will next place \\(C\\) in their basket? Knowing relative frequencies of individual items and combinations thereof across many transactions is necessary to answer this question, but does not necessarily answer how certain or strong those associations are.\nHow do we measure the strength of the rules?\nSupport has already been discussed in this section as the relative frequency of occurance of a set of items wihtin source data. Strength of association rules are measured with metrics including support, confidence, and lift.\nConfidence and lift tell us the most about the strength of a rule. High confidence (ranging from 0 to 1, with 1 being the highest) tell us how often this collection of items occurs.\nHow does one interpret association rules?\nARM does not establish causal relationships between antecedents and consequents. It is a frequentist method to examine relative probabilities within transaction data. When interpreting association rules, one can comment on the strength of identified rules using metrics like confidence and lift. Lift values for association rules are tantamount to pearson R correlation values with some differences in the range of the potential result:\nWith this similarity to correlation, one can interpret mined rules with high lift and high confidence with statements such as “Customers who buy \\(A\\) and \\(B\\) almost always buy \\(C\\).” And similarly, for a very low lift and high confidence, “Customers who buy \\(A\\) rarely if ever also buy \\(B\\).” One should not interpret mined rules in such manners as “Customers buy \\(B\\) because they bought \\(A\\)” or that “Customers who have \\(A\\) and \\(B\\) need \\(C\\).” The later statements are causal in nature, and such relationships are not established via ARM.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#overview",
    "href": "arm.html#overview",
    "title": "6  Association Rule Mining",
    "section": "",
    "text": "Calculate or determine the relative frequency of item groups or sets of length 1 (based on input threshold).\nPrune items that do not meet the input threshold from further consideration.\nFor patterns of length \\(n\\), examine combinations of frequent patterns with length \\(n-1\\) and length 1 (e.g. for length 2, combine frequent item sets of length 1 and 1), and calculate those combinations’ relative frequencies.\nRetain only sets of items that meet the initially established threshold.\nRepeat steps 3 and 4 until no more sets of items of length \\(n\\) meet the specified threshold.\n\n\n\n\n\n\n\n\nConfidence - How often the items A and B occur together given the number of times A occurs. Helps us in that if someone is just buying A and B together and not C, we can rule out C at that point in time. \\(P(B|A) = \\frac{P(A\\cap B)}{P(A)}\\)\nOne can define a threshold for mininum support and confidence as initial parameters when beginning to build association rules. Once set these values are set, they serve as a filter that adjusts the number of rules that are found, and helps determine how long or specific those rules can be. Generally, since the algorithm is inductive, lengthy rules are rare (e.g. will have low support). By setting lower initial thresholds for support, more rules can be mined.\nLift - gives us the indepdendent occurence probability of item A and B. We observe that there is alot of between this random occurence and association. \\(\\frac{P(A\\cap B)}{P(A)\\cdot P(B)}\\)\n\nThe calculation of lift is based upon the assumption of statistical independence - \\(A\\) and \\(B\\) are indpendent \\(\\iff\\) \\(P(A\\cap B) = P(A)\\cdot P(B)\\). So, with the fraction \\(\\frac{P(A\\cap B)}{P(A)\\cdot P(B)}\\), it transforms the calculation in such a way we can garner important insight.\nLift values equal to 1 signify item occurences that are independent of one another.\nLift values greater than 1 are akin to saying the sum is greater than its parts, and gives greater creedence to a calculated confidence value. The higher lift is, the more assurance that we have that the confidence is meaningful and impactful.\nLift values less than 1 signify that there is an inverse relationship between the items in question, and that having one actually reduces the chances of the other occuring. The closer to zero this value approaches, the stronger the inverse relationship is.\n\n\n\n\n\na lift value substantially higher than 1 is analogous to a high, positive Pearson R value close to +1\na small lift value, very close to 0, is analogous to a low, negative Pearson R value close to -1\na lift value of 1 is analogous to a Pearson R value of 0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#arm-within-this-study",
    "href": "arm.html#arm-within-this-study",
    "title": "6  Association Rule Mining",
    "section": "6.2 ARM within this Study",
    "text": "6.2 ARM within this Study\nARM for the purpose of this study can help examine some of the findings from the CNN article, and help examine their findings as well as explore other research questions with respect to the top 5 lenders. Examining associations in which the consequent is either a result of loan approval or loan denial is of interest here. Furthermore, performing similar actions where antecedents include the specific financial institution, an individual or collection of protected class information, and other important features should be examined to pursue answers to the research questions established in Chapter 1.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#data",
    "href": "arm.html#data",
    "title": "6  Association Rule Mining",
    "section": "6.3 Data",
    "text": "6.3 Data\nApplying ARM to the collected HMDA mortgage data is somewhat of a challenge. The data itself is not necessarily organized in a way that is immediately conducive to searching for associations; it contains a mixture of quantitative and qualitative data. To perform ARM, we need transactional data - a list of all things that effectively went into the “basket” of each mortgage application. Additionally, numeric information is a detriment to identifying patterns, as any variable or feature that sits along an interval or continuous scale has countless possibile values which it can take on, and as such, identifying frequent patterns and results in the data may not be possible.\nGenerally, to prepare this mortgage data for use in ARM, a few actions were necessary to establish features as available and usable:\n\nperform discretization and binning of numeric variables into distinct categories\n\nnumeric variables were divided on percentile boundaries of width 20, including 0-20, 21-40, 41-60, 61-80, and &gt;80.\nwhich numeric vars?\n\nadd features of each mortgage application into a basket\ntransform the resulting baskets into a one-hot element frame of data\npivot data into single format (2 columns, transaction number and item)\n\nThe code to perform these transformations and prepare the data was written in Python and can be reviewed in Appendix C.\nPrior to performing the transaction transformation, the data is the same state as it was after initial collection:\n\n\n  state_code county_code       derived_sex action_taken purchaser_type\n1         OH       39153 Sex Not Available            1              0\n2         NY       36061              Male            1              0\n3         NY       36061 Sex Not Available            1              0\n4         FL       12011              Male            1              0\n5         MD       24031             Joint            1              0\n6         NC       37089             Joint            1              0\n\n\nExamples of the data, post transformation:\n\n\n  index                                variable\n1     0                                     3.0\n2     0 applicant_ethnicity:Not Hispanic/Latino\n3     0                            income:21-40\n4     0                      interest_rate:0-20\n5     0                    applicant_race:White\n6     0               loan_to_value_ratio:41-60\n\n\nThe transformed data can be found here",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#code",
    "href": "arm.html#code",
    "title": "6  Association Rule Mining",
    "section": "6.4 Code",
    "text": "6.4 Code\nThe code to prepare the data into single transaction format, execute the apriori algorithm, and measure metrics such as confidence, lift, and support, was written in R is located in Appendix C. Furthermore, the code is embedded, but hidden, within the quarto source code of this webpage, written in R. Examination of the source .qmd file will provide a view of the specific code used to generate the rules and visuals.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#results",
    "href": "arm.html#results",
    "title": "6  Association Rule Mining",
    "section": "6.5 Results",
    "text": "6.5 Results\nThe below tables and figures provide insight to mined association rules from the dataset. The overall first three tables, Table 6.1, Table 6.2, and Table 6.3 cover the top association rules when the totality of the dataset is mined via apriori.\nHowever, some other tables and figures are necessary to examine the individual institutions, as what is frequent for one institution may be infrequent for another. Being able to dive deeper on the individual institutions and the relative frequency of approvals and denials for their organizations is of interest to the intent of this research.\n\n\n\nTable 6.1: Top 15 Associations by Support\n\n\n\n     lhs                                           rhs                                         support confidence  coverage     lift  count\n[1]  {approve}                                  =&gt; {1 rooms}                                 0.8465587  0.9897986 0.8552837 1.001607 172124\n[2]  {1 rooms}                                  =&gt; {approve}                                 0.8465587  0.8566580 0.9882108 1.001607 172124\n[3]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {1 rooms}                                 0.7152005  0.9893591 0.7228928 1.001162 145416\n[4]  {1 rooms}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.7152005  0.7237327 0.9882108 1.001162 145416\n[5]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6253676  0.8650905 0.7228928 1.011466 127151\n[6]  {approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6253676  0.7311815 0.8552837 1.011466 127151\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                                                             \n      approve}                                  =&gt; {1 rooms}                                 0.6195837  0.9907512 0.6253676 1.002571 125975\n[8]  {1 rooms,                                                                                                                             \n      applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6195837  0.8663077 0.7152005 1.012889 125975\n[9]  {1 rooms,                                                                                                                             \n      approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6195837  0.7318852 0.8465587 1.012439 125975\n[10] {applicant_race:White}                     =&gt; {1 rooms}                                 0.6052911  0.9917401 0.6103324 1.003571 123069\n[11] {1 rooms}                                  =&gt; {applicant_race:White}                    0.6052911  0.6125121 0.9882108 1.003571 123069\n[12] {aus:Desktop Underwriter}                  =&gt; {1 rooms}                                 0.5645626  0.9910383 0.5696678 1.002861 114788\n[13] {1 rooms}                                  =&gt; {aus:Desktop Underwriter}                 0.5645626  0.5712977 0.9882108 1.002861 114788\n[14] {applicant_race:White}                     =&gt; {approve}                                 0.5349397  0.8764727 0.6103324 1.024774 108765\n[15] {approve}                                  =&gt; {applicant_race:White}                    0.5349397  0.6254529 0.8552837 1.024774 108765\n\n\n\n\nTable 6.1 outlines the overall top 15 mined rules by support, or relative frequency. One can see some relatively frequently occurring occurences here, however, this doesn’t mean they are useful or meaningful associations.\nFor instance, examining rule #14 with an untrained eye would be immediately concerning, as it seems 53% of the time, White applicants are simply approved because they are White. However, examining the lift of this rule being quite close to 1, this is actually a weak association within this data. Recalling that a lift value equal to 1 means that A and B are independent. While this value is greater than one, as are all values in Table 6.1, all of them are very close to 1. As such, every rule in this table is a weak association and simply a result of frequency of presence in the data.\n\n\n\nTable 6.2: Top 15 Associations by Confidence\n\n\n\n     lhs                                         rhs            support confidence   coverage     lift count\n[1]  {interest_rate:&gt;80}                      =&gt; {approve}   0.18578904          1 0.18578904 1.169203 37775\n[2]  {aus:Loan Prospector/Product Advisor,                                                                  \n      aus:Other}                              =&gt; {JP Morgan} 0.15580213          1 0.15580213 4.971320 31678\n[3]  {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:61-80}              =&gt; {approve}   0.04241056          1 0.04241056 1.169203  8623\n[4]  {interest_rate:&gt;80,                                                                                    \n      tract_minority_population_percent:0-20} =&gt; {approve}   0.04197283          1 0.04197283 1.169203  8534\n[5]  {debt_to_income_ratio:61-80,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04571074          1 0.04571074 1.169203  9294\n[6]  {interest_rate:&gt;80,                                                                                    \n      tract_to_msa_income_percentage:21-40}   =&gt; {approve}   0.04036454          1 0.04036454 1.169203  8207\n[7]  {debt_to_income_ratio:21-40,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04220891          1 0.04220891 1.169203  8582\n[8]  {Female,                                                                                               \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04209579          1 0.04209579 1.169203  8559\n[9]  {2.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04509104          1 0.04509104 1.169203  9168\n[10] {1.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.05827702          1 0.05827702 1.169203 11849\n[11] {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:21-40}              =&gt; {approve}   0.05279802          1 0.05279802 1.169203 10735\n[12] {interest_rate:&gt;80,                                                                                    \n      Joint}                                  =&gt; {approve}   0.06351993          1 0.06351993 1.169203 12915\n[13] {interest_rate:&gt;80,                                                                                    \n      Male}                                   =&gt; {approve}   0.06214773          1 0.06214773 1.169203 12636\n[14] {aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.08140290          1 0.08140290 1.169203 16551\n[15] {interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                        =&gt; {approve}   0.10214832          1 0.10214832 1.169203 20769\n\n\n\n\nIn Table 6.2, we have a bit more of a mixed bag of results. The strongest rule is actually the assocation that JP Morgan uses Loan Prospector/Product Advisor and Other Automated underwriting systems for their loan applications, moreseo than any other lender, for 2023 data. This rule (rule #2) has a substantially high value for lift (close to 5) and a confidence of 1, whereas nearly all other rules in this top-15 list are much closer to 1 in terms of lift.\nAll of these rules have greater lift than the rules mined in Table 6.1, and thus have more utility. However, most are still very close to 1 and not incredibly strong, less rule #2. As such, many of these rules are frequent, but not necessarily meaningful beyond their relative frequency.\n\n\n\nTable 6.3: Top 15 Associations by Lift\n\n\n\n     lhs                                                rhs                                      support confidence   coverage     lift count\n[1]  {applicant_ethnicity:Mexican}                   =&gt; {applicant_ethnicity:Hispanic/Latino} 0.04056128  0.9169446 0.04423525 8.207934  8247\n[2]  {applicant_ethnicity:Hispanic/Latino}           =&gt; {applicant_ethnicity:Mexican}         0.04056128  0.3630800 0.11171442 8.207934  8247\n[3]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07532387  0.7362273 0.10231062 7.834365 15315\n[4]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07603703  0.7353151 0.10340740 7.824658 15460\n[5]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06595941  0.7333224 0.08994600 7.803453 13411\n[6]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06651026  0.7322395 0.09083129 7.791930 13523\n[7]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05100776  0.7264131 0.07021867 7.729930 10371\n[8]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05134712  0.7252518 0.07079903 7.717572 10440\n[9]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04512547  0.7246663 0.06227068 7.711341  9175\n[10] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04538122  0.7233459 0.06273792 7.697291  9227\n[11] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04939456  0.6718625 0.07351885 7.149444 10043\n[12] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07602227  0.6709350 0.11330795 7.139574 15457\n[13] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04969457  0.6703377 0.07413364 7.133218 10104\n[14] {applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07674034  0.6700880 0.11452278 7.130562 15603\n[15] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06656437  0.6685768 0.09956129 7.114480 13534\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Visualization of top 15 rules (by lift)\n\n\n\n\nIn Table 6.3 and Figure 6.1, one can begin to see some interesting patterns. The rules in this list are quite interesting. Of particular interest are rules #11 and #13, establishing assoications between approval and the absence of ethnic, racial, and gender information on a loan application. While other features are include (e.g. 1 bedroom home, specific underwriting systems), the association of these things together within the overarching dataset could be used to tell a compelling story.\nNamely, with 66-70% confidence, it’s possible that you may boost your chances to have a loan approval if you omit your demographic information, as such omissions and exclusions are associated with loan approvals. Similarly speaking, if the former is a true statement, it may also be true that one may reduce their chances for approval when including their personal demographic information on a loan application. And by further examining these rules, both of these claims may best hold true if the loan has been processed by Rocket Mortgage.\nFrom here, it’s of interest to examine association rules mined when the transactions are filtered to specific organizations. The reason for this is that, for a set threshold of support and confidence, certain interesting rules for a given organization may not be available for mining simply due to a lower volume of transactions processed via that organization. As such, by first filtering down transactions to a set organization and then exploring the rules mined for that organization at set common thresholds for support and confidence, more interesting information may arise.\n\n\n\n\n\n\n\n\n\nFigure 6.2: top 10 NFCU Denial Rules by Lift\n\n\n\n\nNFCU has rules including debt-to-income ratio being above 80th percentile and loan interest rate being between 41 and 60th percentile as common features for all of its top 10 rules by lift. A total of 4 of the top ten rules include race or ethnicity (Non-hispanic/latino and black/African American applicants).\n\n\n\n\n\n\n\n\nFigure 6.3: top 10 JP Morgan Denial Rules by Lift\n\n\n\n\nJP Morgan has the same common features of debt-to-income ratio being above 80th percentile and loan interest rate being between 41 and 60th percentile for its top 10 rules. JP Morgan has a single rule covering ethnicity for denial (non-hispanic/latino applicants with the other common criteria).\n\n\n\n\n\n\n\n\nFigure 6.4: top 10 Bank of America Denial Rules by Lift\n\n\n\n\nBank of America appears as an oddity here. The main common traits are a moderate interest rate (in the range of 21st to 40th percentile) and the lack of use of an underwriting system (e.g. they didn’t use any underwriting).\nSurprisingly, there are strong rules for BoA for denial of White and Non-hispanic/latino applicants.\n\n\n\n\n\n\n\n\nFigure 6.5: top 10 Wells Fargo Denial Rules by Lift\n\n\n\n\nWells Fargo corporation has no presence of association rules tied to ethnic, racial, gender, or age in its top-10 rules. Their denials seem to be predominantly mapped to lower proposed interest rates married with a high debt-to-income ratio.\n\n\n\n\n\n\n\n\nFigure 6.6: top 10 Rocket Mortgage Denial Rules by Lift\n\n\n\n\nWells Fargo has 3 rules in its top 10 by lift tied to ethnicity (Denial of non-Hispanic/Latino applicants with moderate interest rates).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#conclusions",
    "href": "arm.html#conclusions",
    "title": "6  Association Rule Mining",
    "section": "6.6 Conclusions",
    "text": "6.6 Conclusions\n\nThese association rules support this research going a degree beyond basic statistical analyses of source variables. In particular, cases of lift being over 1 are of interest, as it suggests that the antecedents contribute more probabilistically to the precedents. Once again, not necessarily tending to causation, but instead establishing a probabilistic and associative connection between the variables.\nExamining the results for the top 15 rules by lift, one sees an interesting occurence. Namely, the following set of items is frequent and strongly associated (examining the 11th rule):\n{ 1 rooms, applicant_ethnicity:Information Not Provided, applicant_race:Information not provided approve, aus:Desktop Underwriter} =&gt; {Sex Not Available}\nNamely, that mortgage approval is strongly associated with not having protected class information (sex, ethnicity, race) available or listed on a mortgage application.\nWith lift values exceeding 7 and confidence of 66%. Moreover, this 66% confidence corresponds to the concept that when all items in the antecedent are met, 66% of the time it is followed by the sex not being listed or available in the application.\nThis suggests, then, that it is quite likely to see applications where no demographic protected class information is provided or available for the applicant, and the application is not denied. This finding appears connected and linked to those of Figure 2.9, Figure 2.10, and Figure 2.11. What is further interesting is that the findings for each individual chart in initial exploration appear to merge together as rules within this association rule mining. While ARM does not establish or produce causal relationsips, the further depth of the relationships between these protected class variables is intriguing.\nHowever, some potentially concerning rules did arise for rocket mortgage (male or White or non-hispanic/latino), Wells Fargo (non-hispanic/latino ethnicity), Bank of America (White or non-hispanic/latino), and JP Morgan (non-Hispanic/Latino). While these rules appear to include other relevant financial or risk-based lending information (high debt to income ratio and insufficient interest rate on the loan, and other similar financial indicators that the applicant’s ability to repay may be at risk), these rules suggest that, at least on a frequentist basis, all lenders are more likely to deny loans to non-Hispanic/Latino applicants when they fall within these financial categories.\nThe findings in denial for NFCU rules #7 and #8 in Figure 6.2 high confidence and lift are consistent with the findings of CNN’s report from the end of 2023, when taking the organization by itself and when not comparing to other institutions. To compare all institutions for such a rule of denial of black applicants, here, ARM is run once more, focused across the entirety of the dataset with minimum support = 0.04, confidence = 0.01, and setting loan denial as the consequent.\n\n\n    lhs                                           rhs       support confidence  coverage      lift count\n[1] {applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04213514  0.3407446 0.1236561 2.3546500  8567\n[2] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04138263  0.3381153 0.1223921 2.3364812  8414\n[3] {applicant_race:White}                     =&gt; {deny} 0.07539273  0.1235273 0.6103324 0.8536119 15329\n[4] {1 rooms,                                                                                           \n     applicant_race:White}                     =&gt; {deny} 0.07414348  0.1224923 0.6052911 0.8464593 15075\n[5] {applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04111213  0.1185674 0.3467406 0.8193371  8359\n[6] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04052193  0.1176227 0.3445077 0.8128092  8239\n[7] {applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05616707  0.1111652 0.5052577 0.7681857 11420\n[8] {1 rooms,                                                                                           \n     applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05537030  0.1103444 0.5017952 0.7625140 11258\n\n\nIn the above table, all rules from the totality of the dataset that include race are listed. Examining these rules, one can clearly see that at the selected minimum confidence and support levels, there are no stand-outs in terms of specific organizations having high-lift high-confidence associations between a particular protected class and denial of loan applications. One can also see, however, that with limited confidence and moderate lift in rules #1 and #2, White applicants whose interest rates would be in the 41-60th percentile of 2023 interest rates tended to be denied. The remainder of the rules are not useful as they have lift values less than 1 and thus have negative assoications with one another.\n\n\n     lhs                                           rhs          support confidence   coverage     lift count\n[1]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11876236          1 0.11876236 1.169203 24147\n[2]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04705836          1 0.04705836 1.169203  9568\n[3]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Male}                                     =&gt; {approve} 0.04324667          1 0.04324667 1.169203  8793\n[4]  {applicant_race:White,                                                                                 \n      aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                        =&gt; {approve} 0.05488339          1 0.05488339 1.169203 11159\n[5]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                          =&gt; {approve} 0.06145424          1 0.06145424 1.169203 12495\n[6]  {applicant_race:White,                                                                                 \n      aus:Desktop Underwriter,                                                                              \n      interest_rate:&gt;80}                        =&gt; {approve} 0.08051760          1 0.08051760 1.169203 16371\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.09885305          1 0.09885305 1.169203 20099\n[8]  {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11780329          1 0.11780329 1.169203 23952\n[9]  {applicant_race:White,                                                                                 \n      interest_rate:0-20,                                                                                   \n      Rocket Mortgage}                          =&gt; {approve} 0.05066840          1 0.05066840 1.169203 10302\n[10] {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04677802          1 0.04677802 1.169203  9511\n\n\nSince metrics such as confidence and lift originate from somewhat Bayesian probability measurements, the performance of naive Bayes and Bernoulli Naive Bayes classification methods on the data could potentially be effective in terms of accuracy, recall, and precision for cases when age, gender, or race are not listed for an application. What would still remain in question is the degree to which predictive strength for those models is impacted by the presence of specific protected classes (instead of their absence) for establishing a link to the outcome of approval or denial of the application.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "nb.html",
    "href": "nb.html",
    "title": "7  Naive Bayes",
    "section": "",
    "text": "7.1 Overview\nThe Naive Bayes method of classification provides a fairly simple and accessible framework under which to estimate the probability that a subject is a member of a certain class, given prior evidence within one’s dataset. The methodology is commonly used in recommendation systems (e.g. streaming services, online gaming services, e-commerce, and many more) to quickly identify and suggest new actions, items, or activity to a user based on their past actions or activity.\n The simplicity of the algorithm makes it an excellent candidate for assessing categorical data (in the case of multinomial Naive Bayes), as well as normally-distributed numerical data (for Gaussian Naive Bayes), and Bernoulli distributed or binary data (as for Bernoulli Naive Bayes). The assumptions made in the algorithm render it simple to implement, and to produce models with a reliable degree of performance.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "nb.html#overview",
    "href": "nb.html#overview",
    "title": "7  Naive Bayes",
    "section": "",
    "text": "7.1.1 What makes it Naive?\nThe following equations outline different formulations of Bayes’ Rule. In these formulations, let X represent the Data, and Y represent the Outcome or Label for the data.\n\\[\nP(X|Y) = \\frac{P(Y|X)\\cdot P(X)}{P(Y)}\n\\tag{7.1}\\]\n\\[\nP(X|Y) = \\frac{P(X\\cap Y)}{P(Y)}\n\\tag{7.2}\\]\n\\[\nP(Y|X) = \\frac{P(X|Y)\\cdot P(X)}{P(Y)}\n\\tag{7.3}\\]\n\\[\nP(Y|X) = \\frac{P(X\\cap Y)}{P(X)}\n\\tag{7.4}\\]\nBayes’ rule has no stipulation that X and Y be independent from one another in the source data. The requirement is that the input probability for \\(P(X)\\), \\(P(Y)\\), and \\(P(X\\cap Y)\\) be the actual probability for each variable or combination thereof.\nThe algorithm is Naive because it assumes all features or input variables are independent of one another, or that each features’ probability does not directly impact one another. In this assumption, it means one can leverage the probability rule for independence between two variables:\n\\[\nP(X\\cap Y) = P(X)\\cdot P(Y)\\iff \\text{X,Y are independent}\n\\tag{7.5}\\]\nIn the world of statistics, it can be quite rare to encounter a collected dataset in which all features are truly independent of one another.\nMany novice, introductory statistics students sometimes make the assumption that variables are independent while working out problems in their homeworks, quizzes, or exams. This leads to incorrect responses, but makes the problem doable in relatively short order. In doing so, those students demonstrate their naivety. Since it makes the same assumption and performs no work to confirm or refute the claim, Naive Bayes is similarly naive.\nThis allows for the transformation of Equation 7.3, Equation 7.4, and Equation 7.5 to calculate probabilites when fitting data to a model and predicting the class of a new record:\n\\[\nP(Y|X) = \\frac{P(X_1|Y)\\cdot P(X_2|Y)\\cdot ...\\cdot P(X_n|Y)}{P(Y)}\n\\tag{7.6}\\]\n\\[\nP(Y|X) = \\frac{P(X_1\\cap Y)\\cdot P(X_2 \\cap Y)\\cdot ...\\cdot P(X_n\\cap Y)}{P(Y)}\n\\tag{7.7}\\]\n\\[\nP(Y|X) = \\frac{\\prod\\limits_{i=1}^n P(X_i\\cap Y)}{P(X)}\n\\tag{7.8}\\]\nIn Equation 7.8, one can see that the probability of belonging to a certain class, given an input record, is the product of the probabilities of each individual feature holding the same category for \\(X_j\\) when the class is \\(Y\\).\nBecause of the product formulation in Equation 7.8 above, it can be benefical to represent the equation using logarithms, rendering a simpler and more easily calculated implementation:\n\\[\n\\text{log}(P(Y|X)) = \\frac{\\text{log}\\sum\\limits_{i=1}^n P(X_i\\cap Y)}{P(X)}\n\\tag{7.9}\\]\n\n\n7.1.2 How does it work?\nIn short, prior probabilities are calculated as follows from a training dataset:\n\\[\nP(X|Y) = \\frac{P(X\\cap Y)}{P(Y)}\n\\]\nThe above is calculated for all features \\(X_i\\) in \\(X\\).\nFrom there, these probabilities are applied in Equation 7.8 for every possible class outcome \\(y_i\\). The maximum amongst these probabilites is selected as the predicted class.\nA simple way to understand Naive Bayes is as a calculation of the combined relative frequency for all features or variables in consideration when records belong to a specific class. These relative frequencies are treated as overall probabities, and are pre-calculated for a Naive Bayes model using a training dataset. When new records are introduced to the model for classification, Naive Bayes outputs the class that has the greatest probability using the pre-computed prior probabilites in conjunction with the new records’ feature data.\nConsider the below example:\nFor this example, a dummy dataset is generated with 400 records. There are 4 classifications - R, G, Y, and B. There are 3 features for evaluation:\n\nsize, which takes on possible values of “S”, “M”, or “L”\ncost, which takes on possible values of “free”,“low”,‘medium’, or ‘high’\ntested, which is a boolean - True/False value\n\nBelow are the first 10 records of this dummy dataset:\n\n\n\n\nTable 7.1: randomly generated dataset, first 10 records\n\n\n\n\n\n\n\n\nclass\nsize\ncost\ntested\n\n\n\n\nR\nS\nlow\nFalse\n\n\nR\nS\nlow\nFalse\n\n\nR\nS\nhigh\nFalse\n\n\nR\nM\nfree\nFalse\n\n\nR\nM\nlow\nFalse\n\n\nR\nL\nfree\nTrue\n\n\nR\nL\nhigh\nFalse\n\n\nR\nM\nfree\nFalse\n\n\nR\nS\nfree\nFalse\n\n\nR\nS\nfree\nFalse\n\n\n\n\n\n\n\n\nTo produce the probability tables, we must first calculate the prior probabilities for each of the classes in our label column, classes.\nFrom here, we can iterate on the remaining columns with respect to the label column. In our case, we have the columns of cost, size, and tested.\nThe general process is as follows:\n\nselect the feature column\nfor each unique value within the feature column, \\(X_j\\):\n\nfor each unique class in our labels:\n\nget the raw count of the number of records where the feature column is equal to the unique value and the class is equal to the current class. This is a proxy for \\(P(X_i|Y)\\)\ncalculate the total raw of each class in the resulting filter, analogous to \\(P(Y)\\)\nuse the above calulations to calculate \\(P(Y_i|X)\\) as listed in Equation 7.2\nselect the highest value for \\(P(Y_i|X)\\) to select the class prediction\n\n\n\nUsing this, we can precompute relative frequencies and probabilities \\(P(Y|X_i)\\) for each value, given that the record is a member of the target class:\n\n\n\n\nTable 7.2: relative frequency of size, given class\n\n\n\n\n\n\n\n\nsize\nclass\ncount\nprob\n\n\n\n\nS\nB\n34\n0.272000\n\n\nM\nB\n43\n0.344000\n\n\nL\nB\n48\n0.384000\n\n\nS\nG\n31\n0.269565\n\n\nM\nG\n47\n0.408696\n\n\nL\nG\n37\n0.321739\n\n\nS\nR\n25\n0.333333\n\n\nM\nR\n27\n0.360000\n\n\nL\nR\n23\n0.306667\n\n\nS\nY\n36\n0.423529\n\n\nM\nY\n23\n0.270588\n\n\nL\nY\n26\n0.305882\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7.3: relative frequency of cost, given class\n\n\n\n\n\n\n\n\ncost\nclass\ncount\nprob\n\n\n\n\nlow\nB\n24\n0.192000\n\n\nhigh\nB\n26\n0.208000\n\n\nfree\nB\n38\n0.304000\n\n\nmedium\nB\n37\n0.296000\n\n\nlow\nG\n33\n0.286957\n\n\nhigh\nG\n22\n0.191304\n\n\nfree\nG\n28\n0.243478\n\n\nmedium\nG\n32\n0.278261\n\n\nlow\nR\n18\n0.240000\n\n\nhigh\nR\n24\n0.320000\n\n\nfree\nR\n19\n0.253333\n\n\nmedium\nR\n14\n0.186667\n\n\nlow\nY\n20\n0.235294\n\n\nhigh\nY\n19\n0.223529\n\n\nfree\nY\n19\n0.223529\n\n\nmedium\nY\n27\n0.317647\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7.4: relative frequency of tested, given class\n\n\n\n\n\n\n\n\ntested\nclass\ncount\nprob\n\n\n\n\nFalse\nB\n56\n0.448000\n\n\nTrue\nB\n69\n0.552000\n\n\nFalse\nG\n64\n0.556522\n\n\nTrue\nG\n51\n0.443478\n\n\nFalse\nR\n43\n0.573333\n\n\nTrue\nR\n32\n0.426667\n\n\nFalse\nY\n46\n0.541176\n\n\nTrue\nY\n39\n0.458824\n\n\n\n\n\n\n\n\nThese probability tables are pre-computed and used for future classification tasks. We directly treat these as independent probabilities and on introduction of a new record, we simply perform the following tasks:\n\nFor every possible result classification, \\(y_i\\) (in our case, G, B, Y, and R):\n\nTake the new input record\nStart with prob = prior probability for each class (the relative frequency of each class \\(y_i\\) in the source data)\nIterate through the pre-computed probability tables for size, cost, and tested:\n\nget the value of the probability column where the class is the current possible result classification \\(y_i\\) and the record feature value is equal to the table feature value.\nmultiply the current value for prob by the selected value. This is analogous to Equation 7.8 for the current class \\(y_i\\)\n\nafter completion we have \\(P(Y=y_i|X)\\)\n\nnow that we’ve collected all \\(P(Y|X)\\) for every \\(y_i\\), one selects the maximum value amongst all \\(P(Y=y_i|X)\\) and set the predicted class as \\(y_i\\)\n\nExamining this process for the below example record:\n\n\n\n\nTable 7.5: New Record for Testing the Model\n\n\n\n\n\n\n\n\nsize\ncost\ntested\n\n\n\n\nL\nmedium\nTrue\n\n\n\n\n\n\n\n\nTo perform the calculation, one can initialize the data with the prior probability \\(P(Y)\\) for each potential output class.\n\n\n\n\nTable 7.6: Prior probabilites (P(Y))\n\n\n\n\n\n\n\n\nB\nG\nY\nR\n\n\n\n\n0.312500\n0.287500\n0.212500\n0.187500\n\n\n\n\n\n\n\n\nFrom here, one must examine the values in each table, where the column in Table 7.5 column is equal to the same column in each of our pre-computed tables, Table 7.2,\n\n\n\n\nTable 7.7: Probabilites from Size Frequencies where size=L\n\n\n\n\n\n\n\n\nB\nG\nY\nR\n\n\n\n\n0.384000\n0.321739\n0.305882\n0.306667\n\n\n\n\n\n\n\n\nfor Table 7.3,\n\n\n\n\nTable 7.8: Probabilites from Size Frequencies where cost=medium\n\n\n\n\n\n\n\n\nB\nG\nY\nR\n\n\n\n\n0.296000\n0.278261\n0.317647\n0.186667\n\n\n\n\n\n\n\n\nand for Table 7.4\n\n\n\n\nTable 7.9: Probabilites from Tested Frequencies where tested=True\n\n\n\n\n\n\n\n\nB\nG\nY\nR\n\n\n\n\n0.552000\n0.443478\n0.458824\n0.426667\n\n\n\n\n\n\n\n\nTaking each of these (the prior probabilities and the extracted probabilities), one can calculate the probability \\(P(Y_i|X))\\) for the input record\n\n\n\n\nTable 7.10: Calculation of P(Y|X)\n\n\n\n\n\n\n\n\n\nclass\nB\nG\nY\nR\n\n\n\n\nprob\n0.019607\n0.011415\n0.009473\n0.00458\n\n\n\n\n\n\n\n\n\n\nThe final result, one can see, is that the classification of the record will be “B”, as it has the highest probability amongst all the potential classes.\n\n\n7.1.3 The Zero-Frequency Problem\nOne challenge with the Naive Bayes algorithm is when one or more of the feature spaces has zero occurences within a given output class. When this occurs, it sets the probability for a record being a member of that class to zero, meaning that no newly introduced data or records can ever be classified as a member of that class by the algorithm. Without additional data and training, the model will never adapt to these new inputs, because the calculation is a running product of the relative frequencies. Thus, a single relative frequency of zero will result in an overall probability of zero, and the output class will not be predicted.\n\nWhat if no records of class Y in the source data were ever tested?\n\n\n\n\n\n\n\ntested\nclass\ncount\nprob\n\n\n\n\nFalse\nB\n56\n0.448000\n\n\nTrue\nB\n69\n0.552000\n\n\nFalse\nG\n64\n0.556522\n\n\nTrue\nG\n51\n0.443478\n\n\nFalse\nR\n43\n0.573333\n\n\nTrue\nR\n32\n0.426667\n\n\nFalse\nY\n85\n1.000000\n\n\nTrue\nY\n0\n0.000000\n\n\n\n\n\nOne can see that the change of all records where class=Y and tested=True have a probability of zero.\nThis has a substatial impact on predictions, namely - whenever a record has tested=True, the trained model will produce a zero probability result for the class of a new record belonging to Y, and thus no new records will ever be classified as Y. Here’s the predicted outcome for the record in the previous example:\n\n\n\n\n\n\n\nsize\ncost\ntested\n\n\n\n\nL\nmedium\nTrue\n\n\n\n\n\nWe see the record has tested = True, so the outcome will never be class Y:\n\n\n\n\n\n\n\nR\nG\nY\nB\n\n\n\n\n0.004580\n0.011415\n0.000000\n0.019607\n\n\n\n\n\nThe absence of a probability is, well, problematic. If the new record truly did belong to class Y, the model can never predict it as belonging to the class, due to the absence of data. There are methods and means of handling this issue, however.\nOne option includes updating an existing Naive Bayes model in relatively short order with new records, new training set information, and recomputing the prior probabilities for classification of future records. Without such additional data and retraining/updating, however, the classification challenge will remain.\nAnother option to rectify the zero-frequency issue is via smoothing methods. These methods allow for any possibility to occur, and assign minute, non-zero probabilities to any cases which have zero frequency within the training dataset. In doing this it ensures that, for every value of every considered feature, there is some probability \\(p\\) strictly greater than zero assigned, thus allowing potential predictions into the appropriate class for any new input tuple.\nA common smoothing technique for Multinomial Naive Bayes is Laplace smoothing. The technique adapts the calculation of the probabilites for each feature \\(P(X_i|Y)\\) in the following manner:\n\\[\nP(X|Y) = \\frac{P(Y|X) + \\alpha}{P(Y)+\\alpha n}\n\\tag{7.10}\\]\nWhere \\(\\alpha\\) is equal to 1, and n is the number of total categories \\(Y\\) in the dataset. There are The additive non-zero values to the numerator \\(\\alpha\\) and the denominator \\(\\alpha n\\) ensures that the probabilities for all \\(P(X_i|Y)\\) are greater than zero, thus enabling them to (potentially) be predicted by the model. It’s possible that a single, and potentially less important, feature could be the difference between whether or not the correct class can be predicted absent such smoothing, but the inclusion of Laplace or other smoothing techniques can support better modeling.\nThus far, everything explained above examines Categorical naive bayes. There are several other versions of the Naive Bayes algorithm that operate under the same independence assumption, but algorithmic performance differs from what has been explained thus far.\n\n\n7.1.4 Bernoulli Naive Bayes\nThis algorithm performs similarly to the Multinomial Naive Bayes algorithm, but on binary encoded (0/1) data. Every category in each feature needs to be transferred into a column, and then each column is set to a zero if a record does not have the specified category value for that feature, and a one otherwise.\nHowever, the probabilities are calculated differently from that of the Multinomial Naive Bayes Algorithm. For calculating the values of the prior probabilities in the training data, the formula is repaced as follows:\n\\[\nP(x_i|y) = P(x_i=1|y)\\cdot x_i + (1-P(x_i=1|y))\\cdot(1-x_i)\n\\]\nThis is similar to the construct of the binomial distribution (chance for k successes in n trials), and is repeated for each feature value \\(x_i\\) in the dataset. This is of benefit to Bernoulli naive bayes over Multinomial, as it will include and penalize non-occuring combinations of \\(x_i\\) and \\(y\\) together in the dataset.\nSome advantages of Bernoulli naive bayes are that it is relatively simple (for small datasets) to implement and that it performs well on tasks such as task classification (e.g. detecting spam emails, for instance). However, it is only able to categorize or predict on a binary outcome, similar to logistic regression. This is applicable for this research, but may not fit all use cases.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "nb.html#data-and-code",
    "href": "nb.html#data-and-code",
    "title": "7  Naive Bayes",
    "section": "7.2 Data and Code",
    "text": "7.2 Data and Code\n\nMultinomialNB (With Protected Classes)\nMultinomialNB (Without Protected Classes)\nCategoricalNB (With Protected Classes)\nCategoricalNB (Without Protected Classes)\nBernoulliNB (With Protected Classes)\nBernoulliNB (Without Protected Classes)\n\nTo prepare the data, several steps were necessary. Namely, the data for this research effort is of mixed (categorical and quantitative) types. Different data transformation techniques and algorithms were required for application to source data to place it in a usable format for each Naive Bayes algorithm.\nFor all model code, in addition to building 2 models with and without protected class information, an exploration into statistically significant differences in model performance was performed. The experiment is constructed under the following parameters:\n\nFor each naive bayes model type, repeat the following steps, 500 times:\n\ninitialize a random seed\nsample the records from both datasets (with and without protected class information) using the same random seed so as to pull the same records, on an 80/20 train/test split. This ensures the same subjects are present in the training and testing data for each random sampling.\ntrain two models using the training data\n\nin the event of an error in fitting or predicting under the current train/test split, decrement the loop counter so that we ensure 500 measurements and restart the loop.\n\npredict the outcomes using the testing data\ncapture model performance metrics (accuracy, precision, recall, F1, ROC-AUC) for the two models trained with and without protected class information (age/gender/race).\n\nAfter metrics for 500 models are captured:\n\nConstruct a paired-t test for difference in means between the two models’ performance metrics, on a per-metric basis.\n\n\\(H_0\\): There is no difference in the mean performance metric for models when trained with and without protected class information\n\\(H_A\\): One of the models has a higher mean performance metric when protected class information is included\n\\(\\alpha=0.003\\) or a \\(3\\sigma\\) confidence\n\nVisualize the distribution of performance metrics for each model\nConclude on any statistically significant differences between the models’ performance\n\n\nThe output for these tests are located as follows:\n\nMultinomialNB Randomization Testing\nCategoricalNB Randomization Testing\nBernoulliNB Randomization Testing\n\n\n7.2.1 Multinomial Naive Bayes\nThe data prepration and code were executed natively in Appendix G, sourcing from the final clean dataset.\nMultinomial Naive Bayes requires count data. Since the native format of the initially cleaned dataset is purely record data, there are challenges to convert this to count information. Namely, each feature and feature value either occurs, or it doesn’t. As such, the data had to be transformed into a one-hot encoded dataset with either 1 if the feature value occurred in the record, and 0 otherwise. This same data is leveraged in Bernoulli Naive Bayes for this portion of the research.\n\n\n\n\nTable 7.11: Initial Data Used\n\n\n\n\n\n\n\n\n\n\nstate_code\ncounty_code\nderived_sex\naction_taken\npurchaser_type\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\n...\ntract_median_age_of_housing_units\napplicant_race\nco-applicant_race\napplicant_ethnicity\nco-applicant_ethnicity\naus\ndenial_reason\noutcome\ncompany\nincome_from_median\n\n\n\n\n0\nOH\n39153.0\nSex Not Available\n1\n0\n2\n2\n665000.0\n85.000\n4.250\n...\n36\n32768\n131072\n32\n128\n64\n512\n1.0\nJP Morgan\nTrue\n\n\n1\nNY\n36061.0\nMale\n1\n0\n2\n2\n755000.0\n21.429\n4.250\n...\n0\n32768\n262144\n64\n256\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n2\nNY\n36061.0\nSex Not Available\n1\n0\n1\n2\n965000.0\n80.000\n5.250\n...\n0\n65536\n262144\n64\n256\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n3\nFL\n12011.0\nMale\n1\n0\n2\n2\n705000.0\n92.175\n5.125\n...\n12\n32768\n262144\n32\n256\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n4\nMD\n24031.0\nJoint\n1\n0\n2\n2\n1005000.0\n65.574\n5.625\n...\n69\n66\n32768\n32\n32\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n5\nNC\n37089.0\nJoint\n1\n0\n1\n2\n695000.0\n85.000\n6.000\n...\n39\n32768\n32768\n32\n32\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n6\nCA\n6073.0\nJoint\n2\n0\n2\n2\n905000.0\n75.000\n6.250\n...\n44\n2\n2\n32\n32\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n7\nNY\n36061.0\nSex Not Available\n2\n0\n1\n2\n355000.0\n15.909\n5.625\n...\n63\n65536\n65536\n64\n64\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n8\nNY\n36061.0\nJoint\n1\n0\n1\n2\n1085000.0\n90.000\n5.625\n...\n75\n32768\n32768\n32\n32\n64\n512\n1.0\nJP Morgan\nTrue\n\n\n9\nMO\n29189.0\nSex Not Available\n2\n0\n1\n2\n405000.0\n53.333\n5.750\n...\n0\n65536\n65536\n64\n64\n64\n512\n1.0\nJP Morgan\nTrue\n\n\n\n\n10 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.12: MultinomialNB Training Data (With protected classes)\n\n\n\n\n\n\n\n\n\n\nderived_sex_Female\nderived_sex_Joint\nderived_sex_Male\nderived_sex_Sex Not Available\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n149746\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n105015\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n29094\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n101082\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n77750\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1.0\n\n\n197336\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0.0\n\n\n137650\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n136772\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n41734\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n10710\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 244 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.13: MultinomialNB Testing Data (With protected classes)\n\n\n\n\n\n\n\n\n\n\nderived_sex_Female\nderived_sex_Joint\nderived_sex_Male\nderived_sex_Sex Not Available\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n30860\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n126890\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n28730\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n31244\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n56105\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n5443\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n95702\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1.0\n\n\n83812\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0.0\n\n\n84338\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n124545\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 244 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.14: MultinomialNB Training Data (No protected classes)\n\n\n\n\n\n\n\n\n\n\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\npurchaser_type_71\npreapproval_1\npreapproval_2\nopen-end_line_of_credit_1\n...\ntract_median_age_of_housing_units_H\ntract_median_age_of_housing_units_M\ntract_median_age_of_housing_units_MH\ntract_median_age_of_housing_units_ML\ncompany_Bank of America\ncompany_JP Morgan\ncompany_Navy Federal Credit Union\ncompany_Rocket Mortgage\ncompany_Wells Fargo\noutcome\n\n\n\n\n149746\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n105015\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n29094\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n101082\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n77750\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n197336\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n137650\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n136772\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n41734\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10710\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n10 rows × 176 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.15: MultinomialNB Training Data (No protected classes)\n\n\n\n\n\n\n\n\n\n\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\npurchaser_type_71\npreapproval_1\npreapproval_2\nopen-end_line_of_credit_1\n...\ntract_median_age_of_housing_units_H\ntract_median_age_of_housing_units_M\ntract_median_age_of_housing_units_MH\ntract_median_age_of_housing_units_ML\ncompany_Bank of America\ncompany_JP Morgan\ncompany_Navy Federal Credit Union\ncompany_Rocket Mortgage\ncompany_Wells Fargo\noutcome\n\n\n\n\n30860\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n126890\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n28730\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n31244\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n56105\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n5443\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n95702\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n83812\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n84338\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n124545\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n10 rows × 176 columns\n\n\n\n\n\n\nAlso of note: the record indexes in Table 9.6 and Table 9.8 match one another, meaning that the two models are trained on the same subjects. Similarly, the indexes between Table 9.7 and Table 9.9 match, so they are tested on the same subjects between the two models.\nAlso note that the indexes between Table 9.6 and Table 9.7 are disjoint - that means that the model has disjoint training and testing data. Similarly, Table 9.8 and Table 9.9 are disjoint.\nBy achieving these splits, the two models evaluated with and without protected class information will avoid unnecessary biases in the results. When a model is tested on data with which it has already been trained, the model has already optimized to the best of its ability to correctly classify the training data. As such, the outcome of an evaluation of a model using the same data in training and testing will artificially inflate its performance metrics (accuracy, precision, recall, F1, ROC-AUC). As such, it is paramount to have a disjoint training and testing dataset.\n\n\n7.2.2 Bernoulli Naive Bayes\nThe data preparation was executed in Appendix G, sourced from the final clean dataset. To prepare the data for two different models, 2 copies were made. The second copy dropped all columns that included protected class information, and the first retained all source columns.\nTo perform an MCA, the data must be in a one-hot-encoded (or binary 0/1) format per category and feature. Since this work was previously done, the steps were seamless to include for this purpose.\nExamples of the two dataset samples used for Bernoulli Naive Bayes are summarized in Table 9.6, Table 9.7, Table 9.8, and Table 9.9.\n\n\n7.2.3 Categorical Naive Bayes\nThe data for categorical naive bayes was sourced from the final clean dataset.\nIn Appendix D, the data is transformed into label-encoded format to perform the categorical naive bayes analysis. In some cases, binary data was captured (mostly in terms of protected class information). These binary caputred data was parsed back out into featurename:featurevalue columns with a 1 if the record met the condition, and zero otherwise. This format still meets the needs of categorical naive bayes.\n\n\n\n\nTable 7.16: CategoricalNB Training Data (With protected classes)\n\n\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n149746\n0\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n105015\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n29094\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n101082\n2\n1\n1\n1\n2\n2\n0\n0\n0\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n77750\n0\n1\n1\n2\n1\n2\n1\n1\n1\n1\n...\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1.0\n\n\n197336\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0.0\n\n\n137650\n3\n1\n1\n2\n2\n0\n3\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n136772\n0\n1\n1\n1\n4\n2\n1\n2\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n41734\n0\n1\n1\n1\n1\n2\n1\n1\n1\n1\n...\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n10710\n0\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n0\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 101 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.17: CategoricalNB Training Data (With protected classes)\n\n\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n30860\n2\n1\n1\n2\n2\n2\n3\n1\n1\n1\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n126890\n3\n1\n1\n3\n2\n3\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n28730\n2\n1\n1\n1\n2\n3\n3\n1\n1\n1\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n31244\n2\n1\n1\n2\n2\n2\n0\n0\n0\n1\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n56105\n2\n1\n1\n3\n1\n2\n1\n1\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n5443\n2\n1\n1\n3\n2\n2\n1\n1\n1\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n95702\n1\n1\n1\n1\n3\n0\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1.0\n\n\n83812\n1\n1\n1\n2\n2\n2\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0.0\n\n\n84338\n1\n1\n1\n1\n1\n2\n3\n1\n3\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n124545\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 101 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.18: CategoricalNB Training Data (No protected classes)\n\n\n\n\n\n\n\n\n\n\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\n...\ncompany\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n149746\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n105015\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n29094\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n101082\n1\n1\n1\n2\n2\n0\n0\n0\n1\n2\n...\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n77750\n1\n1\n2\n1\n2\n1\n1\n1\n1\n2\n...\n4\n0\n0\n0\n0\n0\n1\n0\n0\n1.0\n\n\n197336\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.0\n\n\n137650\n1\n1\n2\n2\n0\n3\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n136772\n1\n1\n1\n4\n2\n1\n2\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n41734\n1\n1\n1\n1\n2\n1\n1\n1\n1\n2\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n10710\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n1\n0\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\n\n\n\n\n\nTable 7.19: CategoricalNB Training Data (No protected classes)\n\n\n\n\n\n\n\n\n\n\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\n...\ncompany\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n30860\n1\n1\n2\n2\n2\n3\n1\n1\n1\n2\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n126890\n1\n1\n3\n2\n3\n1\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n28730\n1\n1\n1\n2\n3\n3\n1\n1\n1\n2\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n31244\n1\n1\n2\n2\n2\n0\n0\n0\n1\n2\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n56105\n1\n1\n3\n1\n2\n1\n1\n1\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n5443\n1\n1\n3\n2\n2\n1\n1\n1\n1\n2\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n95702\n1\n1\n1\n3\n0\n1\n1\n1\n1\n2\n...\n2\n0\n0\n0\n0\n0\n0\n1\n0\n1.0\n\n\n83812\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n...\n4\n0\n0\n0\n0\n0\n1\n0\n0\n0.0\n\n\n84338\n1\n1\n1\n1\n2\n3\n1\n3\n1\n2\n...\n4\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n124545\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\n\nJust as for MultinomialNB, the data for CategoricalNB has the similar combination of same indexes (between datasets) and disjoint indexes (between train test splits).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "nb.html#results",
    "href": "nb.html#results",
    "title": "7  Naive Bayes",
    "section": "7.3 Results",
    "text": "7.3 Results\n\n7.3.1 Categorical Naive Bayes\n\n\n\n\n\n\n\n\nFigure 7.1: Confusion Matrices (CategoricalNB, Single-Run)\n\n\n\n\n\n\n\n\n\nTable 7.20: Model Performance Metrics (CategoricalNB, Single Run)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nCategoricalNB\nWith Protected Classes\n0.882012\n0.943416\n0.917050\n0.930046\n0.795993\n\n\nCategoricalNB\nWithout Protected Classes\n0.912455\n0.934819\n0.964922\n0.949632\n0.783651\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: Metric Kernel Density Estimates (500 randomizations, CategoricalNB)\n\n\n\n\n\n\n\n\n\nTable 7.21: Statistical Significance Tests (Model Performance Metrics, CategoricalNB)\n\n\n\n\n\n\n\n\nModel\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nCategoricalNB\nAccuracy\n-8.271750\n0.000000\nWithout Protected Classes\n0.883466\n0.031069\n\n\nCategoricalNB\nPrecision\n2.768951\n0.005624\nWith Protected Classes\n0.924105\n0.006065\n\n\nCategoricalNB\nRecall\n-21.365216\n0.000000\nWithout Protected Classes\n0.948421\n0.046966\n\n\nCategoricalNB\nF1\n-9.276604\n0.000000\nWithout Protected Classes\n0.932983\n0.020344\n\n\nCategoricalNB\nROC-AUC\n1.049461\n0.293966\nWith Protected Classes\n0.731962\n0.007958\n\n\n\n\n\n\n\n\n\n\n7.3.2 Multinomial Naive Bayes\n\n\n\n\n\n\n\n\nFigure 7.3: Confusion Matrices (MultinomialNB, Single-Run)\n\n\n\n\n\n\n\n\n\nTable 7.22: Model Performance Metrics (MultinomialNB, Single Run)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nMultinomialNB\nWith Protected Classes\n0.936358\n0.967880\n0.957361\n0.962591\n0.884798\n\n\nMultinomialNB\nWithout Protected Classes\n0.939653\n0.969309\n0.959833\n0.964548\n0.890112\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Metric Kernel Density Estimates (500 randomizations, MultinomialNB)\n\n\n\n\n\n\n\n\n\nTable 7.23: Statistical Significance Tests (Model Performance Metrics, MultinomialNB)\n\n\n\n\n\n\n\n\nModel\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nMultinomialNB\nAccuracy\n-1.242399\n0.214090\nWithout Protected Classes\n0.804602\n0.000175\n\n\nMultinomialNB\nPrecision\n12.735062\n0.000000\nWith Protected Classes\n0.914013\n0.000778\n\n\nMultinomialNB\nRecall\n-7.701771\n0.000000\nWithout Protected Classes\n0.852538\n0.001103\n\n\nMultinomialNB\nF1\n-2.489032\n0.012809\nWithout Protected Classes\n0.881842\n0.000228\n\n\nMultinomialNB\nROC-AUC\n10.578913\n0.000000\nWith Protected Classes\n0.689026\n0.002104\n\n\n\n\n\n\n\n\n\n\n7.3.3 Bernoulli Naive Bayes\n\n\n\n\n\n\n\n\nFigure 7.5: Confusion Matrices (BernoulliNB, Single-Run)\n\n\n\n\n\n\n\n\n\nTable 7.24: Model Performance Metrics (BernoulliNB, Single Run)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nBernoulliNB\nWith Protected Classes\n0.937514\n0.973560\n0.952818\n0.963077\n0.899943\n\n\nBernoulliNB\nWithout Protected Classes\n0.939875\n0.977551\n0.951553\n0.964377\n0.911205\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.6: Metric Kernel Density Estimates (500 randomizations, BernoulliNB)\n\n\n\n\n\n\n\n\n\nTable 7.25: Statistical Significance Tests (Model Performance Metrics, BernoulliNB)\n\n\n\n\n\n\n\n\nModel\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nBernoulliNB\nAccuracy\n-53.447073\n0.000000\nWithout Protected Classes\n0.940754\n0.003477\n\n\nBernoulliNB\nPrecision\n-44.319309\n0.000000\nWithout Protected Classes\n0.969658\n0.002109\n\n\nBernoulliNB\nRecall\n-31.047924\n0.000000\nWithout Protected Classes\n0.960794\n0.001971\n\n\nBernoulliNB\nF1\n-52.811800\n0.000000\nWithout Protected Classes\n0.965205\n0.002040\n\n\nBernoulliNB\nROC-AUC\n-49.602472\n0.000000\nWithout Protected Classes\n0.891554\n0.007172\n\n\n\n\n\n\n\n\n\n\n7.3.4 Overall\n\n\n\n\nTable 7.26: Summary of single-run model performance outcomes\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nCategoricalNB\nWith Protected Classes\n0.882012\n0.943416\n0.917050\n0.930046\n0.795993\n\n\nCategoricalNB\nWithout Protected Classes\n0.912455\n0.934819\n0.964922\n0.949632\n0.783651\n\n\nMultinomialNB\nWith Protected Classes\n0.936358\n0.967880\n0.957361\n0.962591\n0.884798\n\n\nMultinomialNB\nWithout Protected Classes\n0.939653\n0.969309\n0.959833\n0.964548\n0.890112\n\n\nBernoulliNB\nWith Protected Classes\n0.937514\n0.973560\n0.952818\n0.963077\n0.899943\n\n\nBernoulliNB\nWithout Protected Classes\n0.939875\n0.977551\n0.951553\n0.964377\n0.911205\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7.27: Summary of 500 Randomization Test Outcomes (All Model Types)\n\n\n\n\n\n\n\n\nModel\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nBernoulliNB\nPrecision\n-44.319309\n0.000000\nWithout Protected Classes\n0.969658\n0.002109\n\n\nBernoulliNB\nF1\n-52.811800\n0.000000\nWithout Protected Classes\n0.965205\n0.002040\n\n\nBernoulliNB\nRecall\n-31.047924\n0.000000\nWithout Protected Classes\n0.960794\n0.001971\n\n\nCategoricalNB\nRecall\n-21.365216\n0.000000\nWithout Protected Classes\n0.948421\n0.046966\n\n\nBernoulliNB\nAccuracy\n-53.447073\n0.000000\nWithout Protected Classes\n0.940754\n0.003477\n\n\nCategoricalNB\nF1\n-9.276604\n0.000000\nWithout Protected Classes\n0.932983\n0.020344\n\n\nCategoricalNB\nPrecision\n2.768951\n0.005624\nWith Protected Classes\n0.924105\n0.006065\n\n\nMultinomialNB\nPrecision\n12.735062\n0.000000\nWith Protected Classes\n0.914013\n0.000778\n\n\nBernoulliNB\nROC-AUC\n-49.602472\n0.000000\nWithout Protected Classes\n0.891554\n0.007172\n\n\nCategoricalNB\nAccuracy\n-8.271750\n0.000000\nWithout Protected Classes\n0.883466\n0.031069\n\n\nMultinomialNB\nF1\n-2.489032\n0.012809\nWithout Protected Classes\n0.881842\n0.000228\n\n\nMultinomialNB\nRecall\n-7.701771\n0.000000\nWithout Protected Classes\n0.852538\n0.001103\n\n\nMultinomialNB\nAccuracy\n-1.242399\n0.214090\nWithout Protected Classes\n0.804602\n0.000175\n\n\nCategoricalNB\nROC-AUC\n1.049461\n0.293966\nWith Protected Classes\n0.731962\n0.007958\n\n\nMultinomialNB\nROC-AUC\n10.578913\n0.000000\nWith Protected Classes\n0.689026\n0.002104\n\n\n\n\n\n\n\n\nExamining the above figures and tables for each model type, performance metrics, and the distribution of performance metrics across 500 random trials, several findings are evident:\n\nBernoulli Naive Bayes is the best performing across all performance metrics at 89.1-96.5% as the mean metric score. In Table 7.27, the performance is sorted by the top mean metric in descending order. For all stats, BernoulliNB outperforms Categorical and Multinomial naive bayes models.\nThere are statistically significant differences in model performance for nearly all performance metrics, across all naive bayes model types (exceptions: CategoricalNB Precision, CategoricalNB ROC-AUC)\nThe difference in mean performance metrics, where significant, is less than 4% across all hypothesis tests.\n\nThese findings are revealing. First and foremost, if a naive bayes model were to be employed, it should be the Bernoulli format, regardless of what data is included. This can be of use in future modeling efforts. It effectively examines whether or not a record meets a specific set of conditions that support a conclusion of approval or denial of a loan.\nFurthermore, in the case of Bernoulli naive bayes, there is a statistically significant outcome in favor of the exclusion of protected class information to deliver higher model performance (with metrics ranging from 0.07%-1% higher).\nWere a financial institution to instead employ CategoricalNB or MultinomialNB, they would be doing themselves a disservice in terms of model performance. However, they do have access to additional variables and information not available to the general public, and may find that either of these models serve their predictive needs better than BernoulliNB.\nThat being the case - for each of these models, there are cases with statistically significant outcomes where the models perform better when protected class information is included in the training and testing data:\n\n\n\n\n\n\n\n\n\nModel\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\n1\nCategoricalNB\nPrecision\n2.768951\n5.623710e-03\nWith Protected Classes\n0.924105\n0.006065\n\n\n4\nCategoricalNB\nROC-AUC\n1.049461\n2.939661e-01\nWith Protected Classes\n0.731962\n0.007958\n\n\n1\nMultinomialNB\nPrecision\n12.735062\n3.775460e-37\nWith Protected Classes\n0.914013\n0.000778\n\n\n4\nMultinomialNB\nROC-AUC\n10.578913\n3.732618e-26\nWith Protected Classes\n0.689026\n0.002104\n\n\n\n\n\n\n\nWhen the better performing model includes protected class information, one can clearly see that, while statistically significant, the difference in means between the models is less than 1%. With such a slight difference, performance gain by incorporating these features is not justified, and from an operational standpoint, the features can be excluded from models with minimal impact to effective predictions.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "dt.html",
    "href": "dt.html",
    "title": "8  Decision Tree Modeling",
    "section": "",
    "text": "8.1 Overview\nDecision trees, when constructed and visualized are one of the machine learning models that are most easily understood by humans on direct examination. A decision tree provides a root node, multiple decision nodes, and branches to follow based upon the current decision node. Each node provides some sort of decision based upon a feature or variable for an input record.\nThe decision tree is, then, quite akin to a flowchart. Employers around the globe use flowcharts to explain processes and procedures to their employees to simplify workflows and provide a frame of reference to get things done. As such, decision tree machine learning modeling is user-friendly, easily-understood, and readily consumed. Decision trees are be high-utitlity, both to human users as well as well as for computers with their very simple branching logic.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "dt.html#overview",
    "href": "dt.html#overview",
    "title": "8  Decision Tree Modeling",
    "section": "",
    "text": "8.1.1 How Does One Build a Decision Tree?\nThe process of building a decision tree has some math behind it, but is fairly simple to explain without the math. To proceed in processing, one needs data in the appropriate format (in Python, the requirement is numeric data only, using available packages). Once the data is in hand, it must be split into a training and testing dataset. From here, we can use the following step(s) to proceed building the tree.\n\nSelect a heuristic that will measure the how impure the current state of the data is, at the current node.\nSelect a maximum depth to which your tree will grow, so as to prevent over-fitting. Too small a value can result in under-fitting (poor model performance)\nWhile we haven’t reached the maximum depth:\n\nfor each feature in the current partition of the dataset (starting with the full dataset):\n\ncalculate the heuristic for each value of the current feature if it were to be split on the value\n\nusing the heuristic, calculate the information gain if the data were to be split on the feature\n\n\nfor the feature value with the highest information gain, split the data on the feature’s value for instance, if the feature were “cost”, the condition could be “cost &lt;= 100?”, and divide the data into two pieces, one where the condtion is true, the and the other where it is false. Increment the depth of our tree.\nrepeat process for the two new partitions if the heuristic is greater than zero\n\nif it is zero, then the partition is considered pure. This signifies that all records in the partition are members of the same class, and this node can be used to clearly make a decision of the class of a future input object.\n\n\n\nThis is all fine and good in terms of understanding steps to produce the tree - but what are heuristics and information gain? How can one calculate them and leverage them to execute this process?\n\n\n8.1.2 Heuristics\nCommon heuristics for measuring the current state of a data partition include Gini indexing and Entropy measures. These values give a sense of how impure the current state of the data is. These heuristics support the splitting of the data at nodes in the tree to reduce the impurity.\n\n8.1.2.1 Gini Indexing\n\\[\n\\text{Gini} =1-\\sum\\limits_{i=1}^{n} p_i^2\n\\tag{8.1}\\]\nFor calculating gini, one iterates over the current split of the data and calculates the sum total members in each class for the current node out of the total number of records in the current node, squares them, and takes the sum total thereof, and subtracts it from one.\nGini being 0 signifies that the node is pure and the algorithm can stop executing on this branch. Mathematically speaking, this is only possible when the records in the current node all belong to one class (e.g. pure). One seeks with each split to reduce the Gini index by the maximum value possible with each node split.\n\n\n8.1.2.2 Entropy\n\\[\n\\text{Entropy} = -\\sum\\limits_{1}^{n}p\\cdot \\text{log}_2(p_i)\n\\tag{8.2}\\]\nEntropy takes the same calculation as done in gini - the number of records in the current data split of a specific class out of the total number of records in the split. What entropy does differently is that it multiplies that value by it’s base 2 logarithm, sums the total values, and takes the negative (which must be done as logarithms with fractional inputs produce negative numbers). Calculating by hand gets hairy if you’re not paying attention. Technically speaking, a logarithm with an input of 0 is undefined. However, when the input to the log is zero, so is its external multiplier. The external multiplier is used as the discriminating factor and the result is set to zero.\nEntropy being 0 signifies that the node is pure and the algorithm can stop executing on this branch. One seeks with each split to reduce the entropy by the maximum value possible with each node split.\nFor both metrics, the best case scenario is a pure node with heuristic = 0. The worst case, mathematically, is 0.5 for Gini and 1 for Entropy.\n\n\n\n8.1.3 Information Gain\nThe following equation outlines the calculation for information gain at each split of a decision tree:\n\\[\nG_i = h_{i-1} - \\sum\\limits_{j=1}^{N}\\text{len}(D_j)\\cdot h_i\n\\tag{8.3}\\]\n\\(h_{i-1}\\) is the heuristic (Gini or Entropy) of the previous node (i.e. impurity). \\(h_i\\) is the current proposed split, \\(N\\) is the number of available splits to perform with the current data, \\(D_j\\) is a proposed split of the data for building a new node on column \\(j\\in N\\), and \\(G_i\\) is the information gain for the current proposed split.\nThe algorithm selects the highest \\(G_i\\) as the winner for where and how to split the current state of the data, and repeats to a certain specified depth, or until the algorithm has pure nodes as leaves.\n\n\n8.1.4 Summary of Metrics\nUsing a heuristic like gini or entropy provides a measure of pureness for the current state of the dataset. When leveraging information gain, it tells one how much a proposed split would reduce the impurity in the data (or conversly, how much additional information one would have available in the data if one were to split on the current feature value). By choosing the maximum value for information gain we are selecting the option which most greatly reduces the impurities present in the data.\n\n\n8.1.5 What challenges exist with decision trees?\nThere are several considerations when building a decision tree. The intent as discussed in the overview of this section is to provide an easily understood and accessible model that delivers reasonably good peformance. As such, a decision tree should be readable and not overwhelming. To meet this need, the tuning of the tree’s maximum depth is necessary so as to ensure the resulting tree is reasonably sized and can be followed by its users.\nSimilarly, tree depth also assists one in overcoming a decision tree’s challenges with over and underfitting. Higher depth will improve the accuracy of future predictions by the tree, but at the cost of readability. Having too high of a depth is problematic as well. Since the algorithm seeks to have all leaf nodes at the base of the tree be as pure as possible, an unconstrained algorithm will continue splitting the data until this condition is met. This can result in leaves on the tree containing single node outcomes, which is an overfitting condition. Having too small of a depth can provide a simple tree, but at the cost of performative accuracy.\nDecision trees can be fast, however, building the tree when using a large volume of training data can be computationally expensive.\nTrees can also have challenges with performing updates. The trees are pre-trained, and as new records are added to training data, the addition thereof changes the hueristic value from what it was previously for each variable value under consideration. As such, the tree would need to be reconstructed when new data is made available for training.\nDecision trees also, in nearly every case, can have infinitely many possible tree outcomes. Specifically, any dataset that contains numeric data (continuous), the possibilities are endless.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "dt.html#data-and-code",
    "href": "dt.html#data-and-code",
    "title": "8  Decision Tree Modeling",
    "section": "8.2 Data and Code",
    "text": "8.2 Data and Code\nMultiple data formats were leveraged to produced multiple decision trees, with and without protected class information.\nOne version of the tree production leveraged the multiple correspondence analysis outputs produced within Appendix G, with and without protected class information.\nAnother version of tree production leveraged label encoded data that was previously produced in Appendix D. Since label-encoded data was necessary to perform Categorical Naive Bayes, this same data, as a numeric, has utility for decision tree building.\n\nSource data (pre-transformation)\nLabel Encoded Data - With Protected Classes\nLabel Encoded Data - Without Protected Classes\nRandomization Test Results\n\nSummarized views for the training and testing data are in the below tables:\n\n\n\n\nTable 8.1: Decision Tree Training Data (With Protected Class)\n\n\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n142289\n3\n0\n1\n1\n2\n3\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n174168\n0\n0\n1\n2\n2\n3\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n106229\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n30766\n0\n1\n1\n3\n2\n2\n3\n1\n1\n1\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n34556\n0\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n60130\n0\n1\n0\n1\n1\n2\n1\n1\n1\n1\n...\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0.0\n\n\n114480\n0\n0\n1\n1\n4\n2\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n61174\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n131619\n2\n0\n1\n1\n4\n2\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n77029\n2\n1\n1\n2\n2\n2\n1\n1\n1\n1\n...\n1\n1\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n\n\n10 rows × 101 columns\n\n\n\n\n\n\n\n\n\n\nTable 8.2: Decision Tree Testing Data (With Protected Class)\n\n\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n89944\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n71083\n1\n1\n1\n1\n1\n4\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n44430\n1\n0\n1\n2\n2\n2\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1.0\n\n\n33808\n2\n1\n1\n1\n2\n1\n0\n0\n1\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1.0\n\n\n101468\n1\n1\n1\n1\n3\n2\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n96435\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n64375\n2\n1\n1\n3\n2\n2\n3\n1\n1\n1\n...\n1\n0\n1\n0\n0\n0\n1\n0\n0\n1.0\n\n\n107749\n0\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n40569\n3\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0.0\n\n\n149685\n3\n1\n1\n1\n2\n2\n2\n2\n2\n1\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 101 columns\n\n\n\n\n\n\n\n\n\n\nTable 8.3: Decision Tree Training Data (Without Protected Class)\n\n\n\n\n\n\n\n\n\n\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\n...\ncompany\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n142289\n0\n1\n1\n2\n3\n1\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n174168\n0\n1\n2\n2\n3\n1\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n106229\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0.0\n\n\n30766\n1\n1\n3\n2\n2\n3\n1\n1\n1\n2\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n34556\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n60130\n1\n0\n1\n1\n2\n1\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0.0\n\n\n114480\n0\n1\n1\n4\n2\n1\n1\n1\n1\n1\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n61174\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n131619\n0\n1\n1\n4\n2\n1\n1\n1\n1\n1\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n77029\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n...\n4\n1\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\n\n\n\n\n\nTable 8.4: Decision Tree Testing Data (Without Protected Class)\n\n\n\n\n\n\n\n\n\n\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\n...\ncompany\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n89944\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n4\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n71083\n1\n1\n1\n1\n4\n1\n1\n1\n1\n1\n...\n4\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n44430\n0\n1\n2\n2\n2\n1\n1\n1\n2\n2\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1.0\n\n\n33808\n1\n1\n1\n2\n1\n0\n0\n1\n0\n2\n...\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1.0\n\n\n101468\n1\n1\n1\n3\n2\n1\n1\n1\n1\n2\n...\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n96435\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n64375\n1\n1\n3\n2\n2\n3\n1\n1\n1\n2\n...\n0\n0\n1\n0\n0\n0\n1\n0\n0\n1.0\n\n\n107749\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n3\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n40569\n1\n1\n1\n2\n2\n1\n1\n1\n1\n1\n...\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0.0\n\n\n149685\n1\n1\n1\n2\n2\n2\n2\n2\n1\n2\n...\n3\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\n\nAs done in Chapter 7 for train test splits - the indexes are disjoint between training and testing datasets for each parent dataset (with and without protected classes), and each model is trained with the same records for comparison and evaluation against one another (same records for training and testing allow for direct comparison of the models).\nBy achieving these splits, the two models evaluated with and without protected class information will avoid unnecessary biases in the results. When a model is tested on data with which it has already been trained, the model has already optimized to the best of its ability to correctly classify the training data. As such, the outcome of an evaluation of a model using the same data in training and testing will artificially inflate its performance metrics (accuracy, precision, recall, F1, ROC-AUC). As such, it is paramount to have a disjoint training and testing dataset.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "dt.html#results",
    "href": "dt.html#results",
    "title": "8  Decision Tree Modeling",
    "section": "8.3 Results",
    "text": "8.3 Results\n\n\n\n\n\n\n\n\nFigure 8.1: Decision Tree (With Protected Classes)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Decision Tree (Without Protected Classes)\n\n\n\n\n\n\n\n\n\nTable 8.5: Model Performance Scores (Single Run; D=4)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nDecision Tree\nWith Protected Classes\n0.909013\n0.911677\n0.989477\n0.948985\n0.711476\n\n\nDecision Tree\nWithout Protected Classes\n0.909013\n0.911677\n0.989477\n0.948985\n0.711476\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Decision Tree Confusion Matrices (With Protected Class Information; D=4)\n\n\n\n\n\nBy far and large, the most interesting result from this research are the decision trees produced for the data with and without protected classes\n\nOne can clearly see that the initially crafted trees are identical. While the numbers in terms of the value of each variable may not be clear (as they are a re-mapping of the source data from categories to integers), the gini index of each variable, and the variable names leveraged in decision making for building each tree, are incredibly clear.\nThe decision tree models for both datasets came to a concensus on how loan decisions could / should be made based upon highly relevant variables. Factors such as debt to income ratio, the loan’s proposed interest rate, the loan term, the automated underwriting system type, income, and whether or not the loan’s purpose was for an open end line of credit were the factors that most greatly partitioned each dataset so as to minimize the Gini index.\nMany of the leaf nodes (where the decisions are made for classification) had remarkably low values for Gini index, within a tree depth of 6. The worst amongst these was with regard to the leaf nodes connected to automated underwriting systems, with Gini nearing the worst case scenario of 0.5. Separate of those, the decision trees were able to, fairly substantially, reduce the index closer to zero. An additional 1 or two layers to these trees may improve upon the result, but could also potentially render a decision-making process for loan approval or denial more complex.\nRegardless, the result is astounding - for machine learning using decision trees, there is absolutely no impact to performance of the models when protected classes are included or excluded. These features do not provide sufficient enough information gain over other options to partition the dataset in a for the purpose of classification.\nFurthermore - the performance of the decision tree is reasonably accurate as well, at or above 90%. If using ROC-AUC as a discriminating factor, however, this model could certainly benefit from additional tuning and adaptations. That being said, the other scores - depending on metric importance for decision making, all exceed the 90% watermark and are fairly good in terms of performance.\nWhat if we prune some of the higher echelon features within the tree? What changes then?\nFirst - let’s look at the tree by removing the root node. One can do this by removing the feature from the dataset and reconstructing the tree with the pruned data. In this case, that feature will be the debt to income ratio on the application.\n\n\n\n\n\n\n\n\nFigure 8.4: Decision Tree (With Protected Class Information; Feature Removed; D=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.5: Decision Tree (Without Protected Class Information; Feature Removed; D=4)\n\n\n\n\n\n\n\n\n\nTable 8.6: Model Performance Scores (Single Run; Features Removed; D=4)\n\n\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nDecision Tree\nWith Protected Classes\n0.889389\n0.897131\n0.983439\n0.938304\n0.658499\n\n\n1\nDecision Tree\nWithout Protected Classes\n0.889389\n0.897131\n0.983439\n0.938304\n0.658499\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6: Decision Tree Confusion Matrices (With Protected Class Information; Feature Removed; D=4)\n\n\n\n\n\nWe can see that after removing the debt_to_income_ratio feature from the data, that the decision tree with protected classes to begin including race and ethnic information as part of the decision making process. This includes Filipino co-applicants, and when co-applicant ethnicity is not provided on the application.\nDespite that fact, however, the predicted outcomes (metric scores) are equivalent for each model.\nDoes this change any further when dropping automated underwriting systems from the training datasets (e.g. the root node of these new trees)?\n\n\n\n\n\n\n\n\nFigure 8.7: Decision Tree (With Protected Class Information; Features Removed; D=4)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.8: Decision Tree (Without Protected Class Information; Features Removed; D=4)\n\n\n\n\n\n\n\n\n\nTable 8.7: Model Performance Scores (Single Run; Features Removed; D=4)\n\n\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nDecision Tree\nWith Protected Classes\n0.863544\n0.863488\n0.998275\n0.926002\n0.532782\n\n\n1\nDecision Tree\nWithout Protected Classes\n0.863593\n0.863495\n0.998332\n0.926031\n0.532811\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.9: Decision Tree Confusion Matrices (With Protected Class Information; Features Removed; D=4)\n\n\n\n\n\nHere, we can see that the decision tree including protected class information picked up another deciding factor for race, substituting for the non-provided ethnic information. In this case, it has selected applicant race information unavailable as a splitting point in the tree. Examining the model performance metrics, we see that the decision tree without protected classes has improved performance over that of the tree that includes them.\nThese trees were built off of a single iteration of training and testing splits. How do the performances of the two tree types (with and without protected class information) compare when performing the hypothesis testing outlined in Chapter 7.\n\n\n\n\n\n\n\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nAccuracy\n-0.063229\n0.949584\nWithout Protected Classes\n0.909842\n0.000004\n\n\nPrecision\n-0.048038\n0.961686\nWithout Protected Classes\n0.912724\n0.000002\n\n\nRecall\n-0.045748\n0.963511\nWithout Protected Classes\n0.989174\n0.000001\n\n\nF1\n-0.064089\n0.948899\nWithout Protected Classes\n0.949412\n0.000002\n\n\nROC-AUC\n-0.049822\n0.960264\nWithout Protected Classes\n0.715086\n0.000009\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOver 500 randomizations, there isn’t sufficient evidence to reject \\(H_0\\) - there is no significant difference between the two models. Surprisingly, the higher mean amongst all these insignificant test results had an insignificantly higher mean for each metric when protected class information was excluded. The p-values are incredibly high in these models, signifying a near total overlap in the mean and the variance of the data for both models’ performance metrics.\nIt appears the initial finding of the first two trees trained on the full dataset is clear: it is ideal to exclude protected class information of any kind when constructing decision tree models for mortgage application predictions. The accuracy of the decision tree without hyperparameter tuning is not a strong as what was found for Bernoulli Naive Bayes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html",
    "href": "reg.html",
    "title": "9  Regression Modeling",
    "section": "",
    "text": "9.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#overview",
    "href": "reg.html#overview",
    "title": "9  Regression Modeling",
    "section": "",
    "text": "9.1.1 Linear Regression\n\n9.1.1.1 What is Linear Regression?\nLinear regression is a special case of generalized linear modeling that seeks to minimize the mean squared error on a continuous spectrum between each all datapoints in the dataset and the predicted output value. It makes an assumption that the underlying data can be modeled using a weighted linear combination of sums:\n\\[\n\\hat{y}=w_1x_1+w_2x_2+...+w_kx_k + b\n\\tag{9.1}\\]\nThe optimization function is the mean squared error, or the distance between each datapoint in the dataset and the prediction line:\n\\[\n\\text{MSE}_i = \\frac{1}{n}\\cdot\\sum\\limits_{i=1}^{n}(y_i-\\hat{y}_i)^2\n\\tag{9.2}\\]\nThe act of identifying the weights allows a user to make a prediction on the interval \\((-\\infty,\\infty)\\) to make a numeric prediction for an output or dependent variable \\(Y\\), based upon a vector of input \\(X\\).\n\n\n\n9.1.2 Logistic Regression\n\n9.1.2.1 What is Logistic Regression?\nLogistic regression is a method to predict a probability of a boolean outcome on the interval from \\([0,1]\\). To perform this action, logistic regression uses linear regression in combination with the sigmoid function \\(\\hat{y}=\\frac{1}{1+e^{-z}}\\) where \\(z = w_1x_1+w_2x_2+...+w_kx_k + b\\), and gradient ascent (or descent, depending on implementation), to link the linear equation from all real numbers to the interval [0,1].\n\n\n9.1.2.2 How do Logistic and Linear Regression Compare?\nThe key similarity between linear and logisitic regression is the use of a linear combination of weighted sums as in Equation 9.1 above, and each leverage an optimization problem to maximize performance of their predictive outcomes. However, this is about where their similarities end. The difference in the output space (\\((-\\infty,\\infty)\\) vs \\([0,1]\\)), the type of optimization problem used (mean squared error minimization vs. maximum likelihood estimation), and their applications (continuous numeric prediction vs. probability prediction), truly set these two regression methods apart.\n\n\n9.1.2.3 How Does Logistic Regression Predict Probability?\nThe sigmoid function is paramount in the implementation of logisitic regression as it remaps the output space of a linear regression to have limited bounds on the interval \\([0,1]\\). Since linear regression has an infinite output space, it is less useful in the prediction of an outcome or a class. The sigmoid function, and derivations from it, are used to train logistic regression models and to make predictions with them once they are trained.\n\n\n9.1.2.4 How is Logistic Regression Trained?\nMaximum likelihood estimation is used as part of the optimization effort for a logisitic regression. Each weight \\(w_i\\) in the equation for \\(z\\) above needs to be adapted and adjusted so as to maximize the likelihood that the input dataset is correctly classified. To calculate the change in probability, the algorithm leverages partial derivatives to calculate how each weight should be adjusted while pursuing a local maximum (gradient ascent) or minimum (gradient descent) using\n\nthe log likelihood function \\[\n\\text{log(p(y|x)} = \\sum\\limits_{i=1}^ny_i\\cdot \\text{log}(\\hat{y}_i)+(1-y_i)\\cdot\\text{log}(1-\\hat{y}_i)\n\\tag{9.3}\\]\npartial derivatives of the likelihood with respect to each feature \\[\n\\frac{\\delta L}{\\delta w_i} = (\\hat{y_i}-y_i)x_i = \\frac{\\delta L}{\\delta \\hat{y}_i}\\frac{\\delta\\hat{y}_i}{\\delta z}\\frac{\\delta z}{\\delta w_i}\n\\tag{9.4}\\]\n\nto iteratively move the needle in the right direction.\n\n\n9.1.2.5 How Is Maximum Likelihood Used in Logisitic Regression?\nUsing the above, each iteration of a logistic regression seeks to increase the likelihood, or probability to that a given input vector \\(X\\) will be classified as the appropriate category \\(Y\\) by adjusting the weights to bring the output probability closer to zero when \\(X\\) is not a member of \\(Y\\), and closer to 1 when \\(X\\) is a member of \\(Y\\). This method can enter the pitfall of solely achieving a local minimum or maximum vs. a global minimum or maximum, as one cannot directly know or infer a parametric equation for \\(\\text{log(p(y|x))}\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#data",
    "href": "reg.html#data",
    "title": "9  Regression Modeling",
    "section": "9.2 Data",
    "text": "9.2 Data\nThe data used for logistic regression is output from the multiple correspondence analysis (MCA) section. The code will leverage two outputs - the MCA that includes the protected classes, and the MCA that does not.\nThe process to generate these data is contained and described in Appendix G.\n\nSource Data\nMultiple Correspondence Analysis - With Protected Classes\nMultiple Correspondence Analysis - Without Protected Classes\nMultinomialNB - With Protected Classes\nMultinomialNB - Without Protected Classes\n\n\n9.2.1 Logistic Regression Data\nAs done in Chapter 7 for train test splits - the indexes are disjoint between training and testing datasets for each parent dataset (with and without protected classes), and each model is trained with the same records for comparison and evaluation against one another (same records for training and testing allow for direct comparison of the models).\nBy achieving these splits, the two models evaluated with and without protected class information will avoid unnecessary biases in the results. When a model is tested on data with which it has already been trained, the model has already optimized to the best of its ability to correctly classify the training data. As such, the outcome of an evaluation of a model using the same data in training and testing will artificially inflate its performance metrics (accuracy, precision, recall, F1, ROC-AUC). As such, it is paramount to have a disjoint training and testing dataset.\nHere are the first few rows of the training and testing data (excluding protected classes):\n\n\n\n\nTable 9.1: MCA Training Data (With Protected Classes)\n\n\n\n\n\n\n\n\n\n\nMC1\nMC2\nMC3\nMC4\nMC5\nMC6\nMC7\nMC8\nMC9\nMC10\n...\nMC173\nMC174\nMC175\nMC176\nMC177\nMC178\nMC179\nMC180\nMC181\noutcome\n\n\n\n\n149746\n-0.387299\n-0.001936\n-0.003167\n-0.003736\n-0.024160\n-0.058848\n-0.161641\n-0.163059\n0.051339\n-0.009136\n...\n0.003101\n-0.001341\n0.000861\n0.004199\n-0.000527\n-0.002532\n-0.008719\n-0.000873\n-0.002317\n1.0\n\n\n105015\n-0.386534\n0.007586\n0.005638\n-0.149832\n0.037838\n0.251784\n0.098772\n-0.104072\n0.013961\n-0.418258\n...\n0.000377\n0.001873\n0.001106\n-0.027093\n0.005149\n0.003389\n0.007496\n-0.003416\n0.003757\n1.0\n\n\n29094\n0.584290\n-0.011996\n-0.005619\n0.162373\n-0.135637\n-0.251829\n-0.104188\n-0.168750\n-0.177008\n0.118405\n...\n0.022515\n-0.007842\n-0.002415\n-0.049875\n0.001095\n0.072832\n-0.005292\n0.078073\n0.014257\n1.0\n\n\n101082\n-0.334664\n0.002222\n0.000164\n-0.101558\n0.021665\n-0.211819\n0.279624\n0.695377\n0.687991\n-0.076981\n...\n0.026285\n0.001354\n-0.002768\n0.006803\n0.007875\n0.004828\n0.006313\n-0.007748\n0.009507\n1.0\n\n\n77750\n-0.281561\n0.009739\n0.007302\n0.294964\n-0.119251\n0.159063\n0.758784\n-0.039995\n-0.195625\n0.002697\n...\n-0.052733\n-0.000610\n-0.008253\n0.117513\n0.000083\n0.007967\n-0.010744\n-0.000727\n0.000273\n1.0\n\n\n197336\n0.486499\n-0.010249\n-0.000090\n-0.015523\n-0.006435\n0.167655\n-0.198908\n0.150687\n-0.162005\n0.046811\n...\n-0.012404\n-0.000997\n-0.002124\n0.001909\n0.000212\n-0.007067\n0.015649\n0.006640\n-0.000096\n0.0\n\n\n137650\n0.467874\n0.001350\n-0.009676\n-1.188243\n0.363408\n0.128250\n0.223274\n-0.317145\n0.196431\n-0.287222\n...\n-0.019506\n-0.000767\n-0.000612\n-0.005116\n0.001205\n-0.005075\n0.000100\n-0.000287\n-0.002095\n1.0\n\n\n136772\n-0.367590\n-0.001345\n-0.000256\n-0.156002\n0.020358\n-0.132456\n0.018576\n-0.093998\n0.103849\n0.001622\n...\n-0.106859\n0.004671\n0.000376\n0.009465\n-0.000201\n0.007182\n-0.011129\n-0.008761\n-0.001996\n1.0\n\n\n41734\n-0.330008\n0.011018\n0.006242\n0.272821\n-0.054578\n0.325838\n0.372732\n0.030138\n-0.028335\n0.063026\n...\n0.011053\n0.001163\n0.004312\n-0.062842\n0.000968\n0.007450\n-0.009850\n-0.003101\n0.003397\n0.0\n\n\n10710\n-0.426415\n-0.004343\n-0.003192\n0.108570\n-0.054307\n-0.125460\n-0.280849\n0.185723\n-0.081365\n0.132101\n...\n0.020289\n-0.005505\n0.000400\n0.092377\n0.001436\n0.006528\n-0.011197\n0.000699\n-0.019417\n1.0\n\n\n\n\n10 rows × 182 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.2: MCA Testing Data (With Protected Classes)\n\n\n\n\n\n\n\n\n\n\nMC1\nMC2\nMC3\nMC4\nMC5\nMC6\nMC7\nMC8\nMC9\nMC10\n...\nMC173\nMC174\nMC175\nMC176\nMC177\nMC178\nMC179\nMC180\nMC181\noutcome\n\n\n\n\n30860\n-0.377629\n-0.003778\n-0.000284\n0.123173\n-0.082610\n-0.252011\n-0.083817\n0.137802\n-0.059589\n0.146982\n...\n0.033447\n-0.004992\n0.012373\n-0.043422\n-0.000974\n0.003553\n0.006232\n0.001033\n-0.018414\n1.0\n\n\n126890\n-0.467556\n0.004930\n-0.005359\n-0.641571\n0.206492\n0.242741\n-0.226606\n-0.014591\n-0.472724\n0.021588\n...\n0.010695\n-0.000696\n0.000022\n0.016918\n0.003402\n-0.003975\n-0.003803\n0.001092\n-0.000510\n1.0\n\n\n28730\n-0.413508\n-0.005233\n0.001155\n0.121581\n-0.085867\n-0.230894\n-0.106145\n0.189636\n-0.061267\n0.069930\n...\n0.015688\n0.000374\n-0.005150\n-0.031315\n-0.003929\n0.005044\n0.008720\n0.012316\n-0.079917\n1.0\n\n\n31244\n-0.287660\n-0.003570\n0.003981\n0.050584\n-0.079588\n-0.564932\n0.493067\n0.565763\n0.294100\n0.394073\n...\n0.071348\n0.005973\n0.002630\n-0.031388\n0.001923\n-0.059667\n0.008618\n0.053812\n0.008861\n1.0\n\n\n56105\n0.475915\n0.012787\n-0.010241\n0.304675\n-0.048947\n0.467680\n-0.300472\n-0.138536\n0.078400\n0.050529\n...\n0.021655\n-0.007473\n0.009129\n-0.029546\n0.005734\n-0.003265\n-0.093248\n0.005869\n-0.020469\n1.0\n\n\n5443\n-0.532447\n-0.000680\n-0.000100\n0.149901\n-0.052502\n0.037784\n-0.165450\n0.385730\n-0.450069\n0.097568\n...\n-0.175609\n0.002486\n0.005582\n-0.060549\n-0.006316\n-0.007500\n0.009540\n0.004287\n0.003089\n1.0\n\n\n95702\n0.488661\n0.002546\n0.001658\n0.293300\n-0.075460\n0.567510\n-0.399183\n0.208255\n0.236614\n-0.575200\n...\n0.051268\n-0.003152\n-0.000812\n0.022663\n-0.002750\n-0.011304\n0.013903\n0.001937\n0.017053\n1.0\n\n\n83812\n0.618010\n-0.008268\n0.003282\n0.208434\n-0.083836\n0.068062\n0.195019\n0.212166\n0.059184\n-0.127586\n...\n-0.056974\n-0.001156\n-0.027070\n0.117950\n0.000061\n0.008795\n-0.008416\n0.003242\n-0.017692\n0.0\n\n\n84338\n0.655969\n-0.004918\n-0.006596\n0.294970\n-0.113992\n0.117014\n-0.038303\n-0.000417\n0.119094\n-0.173938\n...\n-0.025098\n-0.003851\n-0.015221\n0.000430\n0.005239\n0.005173\n-0.009246\n0.003599\n-0.017117\n1.0\n\n\n124545\n0.570068\n-0.019301\n-0.010219\n0.132395\n-0.125488\n-0.262735\n0.254719\n-0.193122\n-0.455048\n0.200752\n...\n0.018821\n0.002103\n0.000520\n0.011330\n-0.000257\n0.012774\n0.019057\n0.005942\n0.000166\n1.0\n\n\n\n\n10 rows × 182 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.3: MCA Training Data (Without Protected Classes)\n\n\n\n\n\n\n\n\n\n\nMC1\nMC2\nMC3\nMC4\nMC5\nMC6\nMC7\nMC8\nMC9\nMC10\n...\nMC92\nMC93\nMC94\nMC95\nMC96\nMC97\nMC98\nMC99\nMC100\noutcome\n\n\n\n\n149746\n-0.022191\n-0.265491\n-0.089926\n0.030741\n-0.203565\n0.045974\n0.036976\n-0.158538\n-0.089747\n-0.118819\n...\n0.018080\n-0.010963\n-0.029876\n-0.003884\n-0.001979\n0.007441\n-0.007867\n0.005483\n-0.000806\n1.0\n\n\n105015\n0.290036\n0.178485\n-0.157600\n-0.271749\n-0.245192\n0.415834\n0.123361\n0.055042\n-0.280792\n-0.105106\n...\n0.032420\n0.008380\n-0.046675\n-0.011824\n0.044305\n0.096120\n0.266568\n0.009842\n0.006172\n1.0\n\n\n29094\n-0.095327\n-0.086033\n-0.071726\n-0.077756\n-0.175656\n-0.205904\n-0.027044\n-0.228361\n-0.037807\n-0.112685\n...\n0.011820\n-0.023923\n0.035317\n0.019936\n-0.090864\n0.017211\n0.003241\n-0.006380\n-0.000049\n1.0\n\n\n101082\n-0.654622\n0.371221\n0.433967\n0.351343\n0.378372\n1.075594\n-0.389695\n-0.187974\n-0.091264\n-0.120999\n...\n-0.048364\n0.117014\n-0.080124\n0.017038\n0.005769\n0.113322\n0.239627\n0.041617\n0.008369\n1.0\n\n\n77750\n-0.011772\n0.645262\n0.057628\n-0.176639\n-0.285778\n-0.331408\n-0.273163\n0.149750\n-0.234999\n0.106382\n...\n0.236344\n-0.074790\n-0.100306\n-0.338227\n-0.034114\n0.028328\n-0.038353\n-0.014300\n0.001131\n1.0\n\n\n197336\n0.211278\n-0.030244\n0.227029\n-0.003941\n0.369867\n-0.046471\n0.319009\n-0.191355\n-0.301365\n0.207057\n...\n0.031491\n-0.015596\n-0.012795\n-0.014921\n-0.018947\n0.004192\n-0.022371\n0.006839\n-0.000022\n0.0\n\n\n137650\n-0.358538\n0.051337\n-0.836959\n-0.183025\n0.827005\n-0.153045\n-0.300371\n0.067741\n-0.113371\n-0.321046\n...\n0.335969\n-0.153317\n0.006543\n0.153396\n0.313536\n-0.124046\n0.047060\n-0.003220\n0.003096\n1.0\n\n\n136772\n-0.224883\n-0.192682\n-0.084809\n0.117577\n-0.313160\n-0.083558\n0.224909\n0.148168\n-0.065917\n-0.105496\n...\n0.182218\n0.384711\n0.246450\n-0.027070\n-0.039627\n-0.072453\n0.029784\n-0.224192\n-0.000603\n1.0\n\n\n41734\n0.126939\n0.621233\n0.144349\n0.053672\n-0.309407\n-0.342110\n-0.120858\n0.063892\n-0.205557\n-0.164335\n...\n0.072717\n-0.012892\n-0.102817\n-0.445682\n-0.024483\n0.166019\n-0.124861\n-0.005165\n0.002151\n0.0\n\n\n10710\n0.163013\n-0.275794\n0.117153\n-0.011200\n-0.019967\n0.048655\n0.044069\n-0.165311\n0.018394\n-0.103244\n...\n-0.030015\n0.019834\n-0.002256\n-0.022811\n0.042606\n-0.007783\n-0.012869\n-0.010702\n-0.000356\n1.0\n\n\n\n\n10 rows × 101 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.4: MCA Testing Data (Without Protected Classes)\n\n\n\n\n\n\n\n\n\n\nMC1\nMC2\nMC3\nMC4\nMC5\nMC6\nMC7\nMC8\nMC9\nMC10\n...\nMC92\nMC93\nMC94\nMC95\nMC96\nMC97\nMC98\nMC99\nMC100\noutcome\n\n\n\n\n30860\n0.021908\n-0.107937\n0.160667\n-0.096196\n0.018201\n-0.101238\n0.033902\n-0.304192\n0.099209\n-0.167334\n...\n-0.041614\n0.049508\n0.024640\n0.119880\n0.038063\n-0.152665\n0.053603\n-0.017657\n-0.001590\n1.0\n\n\n126890\n0.480633\n-0.445114\n0.321141\n-0.039117\n0.011562\n-0.032186\n-0.610614\n0.551665\n0.076726\n0.035808\n...\n-0.031216\n0.024834\n-0.065209\n-0.075517\n0.059589\n-0.056023\n0.004988\n0.007007\n0.003624\n1.0\n\n\n28730\n-0.008824\n-0.143329\n0.122037\n-0.081711\n0.180048\n-0.110025\n0.093520\n-0.358804\n0.058229\n-0.195015\n...\n-0.070911\n0.049980\n-0.013948\n-0.019076\n0.031768\n-0.006892\n-0.030458\n-0.016992\n-0.001599\n1.0\n\n\n31244\n-0.929934\n0.290919\n0.916922\n0.393976\n0.653189\n0.575340\n-0.367097\n-0.433499\n-0.010454\n0.096068\n...\n0.139571\n0.046301\n-0.099484\n-0.084230\n-0.004915\n0.071326\n-0.032964\n0.019019\n0.000421\n1.0\n\n\n56105\n0.414472\n0.449156\n-0.207480\n-0.003224\n-0.150305\n0.032234\n-0.191760\n0.153439\n1.021892\n-0.041131\n...\n-0.055936\n0.036294\n0.038325\n0.232323\n0.003879\n0.034577\n0.006583\n0.002948\n0.006281\n1.0\n\n\n5443\n0.523671\n-0.283900\n0.655485\n-0.123366\n0.718045\n-0.163190\n-0.157075\n0.188473\n0.113406\n0.062999\n...\n-0.058962\n0.001311\n0.019947\n-0.076058\n-0.048344\n-0.037792\n-0.004104\n-0.015193\n-0.001536\n1.0\n\n\n95702\n0.473073\n0.236969\n-0.363466\n-0.336683\n-0.106912\n1.000458\n0.259367\n0.206744\n-0.271422\n0.230963\n...\n-0.101931\n0.052594\n-0.061589\n0.005126\n-0.186917\n-0.079407\n-0.302814\n-0.002668\n-0.006435\n1.0\n\n\n83812\n-0.140662\n0.424349\n0.083509\n-0.163135\n-0.410733\n-0.206931\n-0.126389\n-0.014700\n-0.278051\n-0.149923\n...\n0.003635\n0.030724\n-0.095295\n-0.276234\n0.016694\n0.262435\n-0.092800\n-0.011479\n0.001361\n0.0\n\n\n84338\n0.060723\n0.581497\n-0.263134\n-0.246079\n-0.336327\n-0.090347\n-0.142133\n-0.116463\n0.962056\n0.254970\n...\n-0.010415\n0.008387\n-0.065012\n-0.232432\n0.236782\n0.015017\n0.039840\n-0.005290\n0.007312\n1.0\n\n\n124545\n-0.019499\n-0.310502\n-0.078770\n0.035220\n-0.240221\n0.032627\n-0.037473\n-0.112962\n-0.098873\n-0.104411\n...\n0.036539\n-0.009720\n-0.027945\n-0.016062\n0.043209\n0.004422\n0.017510\n0.006984\n0.000563\n1.0\n\n\n\n\n10 rows × 101 columns\n\n\n\n\n\n\nNotice that each has a different number of components. Each is structured to a number of components required to explain at least approximately 99.99% of the variance in each dataset.\nSimilarly as in Chapter 7 and Chapter 8, a 500 iteration randomization of source data test was performed against both models to compare a difference in means.\n\n\n9.2.2 Multinomial Naive Bayes\nThe description for MultinomialNB data and code can be found in Chapter 7.\n\n\n\n\nTable 9.5: Initial Data Used\n\n\n\n\n\n\n\n\n\n\nstate_code\ncounty_code\nderived_sex\naction_taken\npurchaser_type\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\n...\ntract_median_age_of_housing_units\napplicant_race\nco-applicant_race\napplicant_ethnicity\nco-applicant_ethnicity\naus\ndenial_reason\noutcome\ncompany\nincome_from_median\n\n\n\n\n0\nOH\n39153.0\nSex Not Available\n1\n0\n2\n2\n665000.0\n85.000\n4.250\n...\n36\n32768\n131072\n32\n128\n64\n512\n1.0\nJP Morgan\nTrue\n\n\n1\nNY\n36061.0\nMale\n1\n0\n2\n2\n755000.0\n21.429\n4.250\n...\n0\n32768\n262144\n64\n256\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n2\nNY\n36061.0\nSex Not Available\n1\n0\n1\n2\n965000.0\n80.000\n5.250\n...\n0\n65536\n262144\n64\n256\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n3\nFL\n12011.0\nMale\n1\n0\n2\n2\n705000.0\n92.175\n5.125\n...\n12\n32768\n262144\n32\n256\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n4\nMD\n24031.0\nJoint\n1\n0\n2\n2\n1005000.0\n65.574\n5.625\n...\n69\n66\n32768\n32\n32\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n5\nNC\n37089.0\nJoint\n1\n0\n1\n2\n695000.0\n85.000\n6.000\n...\n39\n32768\n32768\n32\n32\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n6\nCA\n6073.0\nJoint\n2\n0\n2\n2\n905000.0\n75.000\n6.250\n...\n44\n2\n2\n32\n32\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n7\nNY\n36061.0\nSex Not Available\n2\n0\n1\n2\n355000.0\n15.909\n5.625\n...\n63\n65536\n65536\n64\n64\n64\n512\n1.0\nJP Morgan\nFalse\n\n\n8\nNY\n36061.0\nJoint\n1\n0\n1\n2\n1085000.0\n90.000\n5.625\n...\n75\n32768\n32768\n32\n32\n64\n512\n1.0\nJP Morgan\nTrue\n\n\n9\nMO\n29189.0\nSex Not Available\n2\n0\n1\n2\n405000.0\n53.333\n5.750\n...\n0\n65536\n65536\n64\n64\n64\n512\n1.0\nJP Morgan\nTrue\n\n\n\n\n10 rows × 51 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.6: MultinomialNB Training Data (With protected classes)\n\n\n\n\n\n\n\n\n\n\nderived_sex_Female\nderived_sex_Joint\nderived_sex_Male\nderived_sex_Sex Not Available\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n149746\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n105015\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n29094\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n...\n0\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n101082\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n77750\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n1\n0\n0\n1.0\n\n\n197336\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0.0\n\n\n137650\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n136772\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n41734\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n...\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0.0\n\n\n10710\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 244 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.7: MultinomialNB Testing Data (With protected classes)\n\n\n\n\n\n\n\n\n\n\nderived_sex_Female\nderived_sex_Joint\nderived_sex_Male\nderived_sex_Sex Not Available\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\n...\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\noutcome\n\n\n\n\n30860\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n126890\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n...\n1\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n28730\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n31244\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n1\n1\n0\n0\n1\n0\n0\n0\n1.0\n\n\n56105\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n5443\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n...\n1\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n95702\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n1.0\n\n\n83812\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0.0\n\n\n84338\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1.0\n\n\n124545\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n0\n1.0\n\n\n\n\n10 rows × 244 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.8: MultinomialNB Training Data (No protected classes)\n\n\n\n\n\n\n\n\n\n\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\npurchaser_type_71\npreapproval_1\npreapproval_2\nopen-end_line_of_credit_1\n...\ntract_median_age_of_housing_units_H\ntract_median_age_of_housing_units_M\ntract_median_age_of_housing_units_MH\ntract_median_age_of_housing_units_ML\ncompany_Bank of America\ncompany_JP Morgan\ncompany_Navy Federal Credit Union\ncompany_Rocket Mortgage\ncompany_Wells Fargo\noutcome\n\n\n\n\n149746\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n105015\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n29094\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n101082\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n77750\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n197336\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n137650\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n136772\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n41734\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n10710\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n10 rows × 176 columns\n\n\n\n\n\n\n\n\n\n\nTable 9.9: MultinomialNB Training Data (No protected classes)\n\n\n\n\n\n\n\n\n\n\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\npurchaser_type_71\npreapproval_1\npreapproval_2\nopen-end_line_of_credit_1\n...\ntract_median_age_of_housing_units_H\ntract_median_age_of_housing_units_M\ntract_median_age_of_housing_units_MH\ntract_median_age_of_housing_units_ML\ncompany_Bank of America\ncompany_JP Morgan\ncompany_Navy Federal Credit Union\ncompany_Rocket Mortgage\ncompany_Wells Fargo\noutcome\n\n\n\n\n30860\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n126890\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n28730\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n31244\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n56105\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n5443\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n95702\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n83812\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n84338\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n124545\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n10 rows × 176 columns",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#code",
    "href": "reg.html#code",
    "title": "9  Regression Modeling",
    "section": "9.3 Code",
    "text": "9.3 Code\nThe code for both logistic regression can be found in Appendix F and multinomial naive bayes in Appendix D.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#results",
    "href": "reg.html#results",
    "title": "9  Regression Modeling",
    "section": "9.4 Results",
    "text": "9.4 Results\n\n9.4.1 Logistic Regression\n\n\n\n\n\n\n\n\nFigure 9.1: Logistic Regression Modeling Results\n\n\n\n\n\n\n\n\n\nTable 9.10: Logistic Regression Model Metrics\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nLogistic Regression\nWith Protected Classes\n0.961859\n0.983022\n0.972197\n0.977579\n0.936481\n\n\nLogistic Regression\nWithout Protected Classes\n0.958859\n0.981304\n0.970385\n0.975814\n0.930562\n\n\n\n\n\n\n\n\nThere is a chance that the difference in performance of the models trained with and without protected class data could be due to random chance, namely the random splitting of training and testing data from the source datasets.\nTo examine the potential for the difference (albeit minute) between both modeling methods, one can conduct a randomization test on the source data itself to examine for a statistically significant difference in means.\nTo do so, one can shuffle the data multiple times into new training and testing datasets, re-train the models, and capture the performance scores of each model (one trained with, and one trained without, protected classes). From here, one achieve’s a distribution of the performance scores (such as Accuracy, Precision, Recall, F1, and ROC-AUC scores) of each model; when the number of randomizations increase, the distribution of each should approach a normal distribution.\nTo perform the randomization, the data was shuffled 500 times, and two models were trained on each shuffle, capturing the aforementioned scores. When these shuffles were executed, the following outcomes were achieved:\n\n\n\n\n\n\n\n\nFigure 9.2\n\n\n\n\n\n\n\n\n\nTable 9.11: Per Metric Paired Z-test (Logistic Regression, 500 Iterations)\n\n\n\n\n\n\n\n\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nAccuracy\n44.648798\n0.000000\nWith Protected Classes\n0.963586\n0.002457\n\n\nPrecision\n47.173034\n0.000000\nWith Protected Classes\n0.983867\n0.001985\n\n\nRecall\n16.219786\n0.000000\nWith Protected Classes\n0.973386\n0.000889\n\n\nF1\n43.977874\n0.000000\nWith Protected Classes\n0.978598\n0.001431\n\n\nROC-AUC\n50.590439\n0.000000\nWith Protected Classes\n0.939526\n0.006305\n\n\n\n\n\n\n\n\nOne can see that over 500 iterations of shuffling nearly 200k records, that the model trained on the multiple correspondence analysis that included protected class information had statistically better performance across all metrics.\nThis is a substantial finding. Namely, these significant difference signifies that including protected class information in logisitic regression confidently improves its predictive performance, better than if it were excluded as part of the model training data.\nFurthermore, in comparison to all Naive Bayes models and all decision tree models, including their randomization testing, the Logistic Regression model outperforms them all in every metric. The most discerning factor is the fact that the ROC-AUC score for Logistic Regression is in the 90s, whereas few if any other ROC-AUCs for other models exceeded 89%.\nWhat does this mean? It means that, if using logistic regression modeling to assess whether or not a loan should be approved, that a company could choose to include protected class data when building a linear regression model if they’re concerned about model performance.\nEthically speaking, however, it should be excluded outright. This is further evidenced by the overall difference in performance between two models trained on the exact same data, less protected class variable presence. The difference, while statistically significant, is not operationally significant, as the maximum difference in means for each performance metric is less than 0.6%.\nLeveraging that ethical perspective, while the difference is significantly different, from a mathematical standpoint, that models leveraging protected class information outperform those that exclude it, the cost is minimal. Exclustion of protected class information still achieves incredibly high accuracy, precision, Recall, F1, and ROC-AUC, all ranging from 93%-98% to make predictions. Such modeling can be leveraged to inform one as to the likely outcome of the loan application, and can be leveraged in conjunction with other available relevant information to make an informed decision.\n\n\n9.4.2 Multinomial Naive Bayes\nThe results here are the same as included and described within Chapter 7. The findings are the same.\n\n\n\n\n\n\n\n\nFigure 9.3: Confusion Matrices (MultinomialNB, Single-Run)\n\n\n\n\n\n\n\n\n\nTable 9.12: Model Performance Metrics (MultinomialNB, Single Run)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nMultinomialNB\nWith Protected Classes\n0.936358\n0.967880\n0.957361\n0.962591\n0.884798\n\n\nMultinomialNB\nWithout Protected Classes\n0.939653\n0.969309\n0.959833\n0.964548\n0.890112\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.4: Metric Kernel Density Estimates (500 randomizations, MultinomialNB)\n\n\n\n\n\n\n\n\n\nTable 9.13: Statistical Significance Tests (Model Performance Metrics, MultinomialNB)\n\n\n\n\n\n\n\n\nModel\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nMultinomialNB\nAccuracy\n-1.242399\n0.214090\nWithout Protected Classes\n0.804602\n0.000175\n\n\nMultinomialNB\nPrecision\n12.735062\n0.000000\nWith Protected Classes\n0.914013\n0.000778\n\n\nMultinomialNB\nRecall\n-7.701771\n0.000000\nWithout Protected Classes\n0.852538\n0.001103\n\n\nMultinomialNB\nF1\n-2.489032\n0.012809\nWithout Protected Classes\n0.881842\n0.000228\n\n\nMultinomialNB\nROC-AUC\n10.578913\n0.000000\nWith Protected Classes\n0.689026\n0.002104\n\n\n\n\n\n\n\n\nExamining MultinomialNB’s performance, it seems to have fairly low performance in terms of accuracy. This is likely caused by the fact that it was provided with Bernoulli data, as it was the only way to obtain count data from the source records. That being said, the performance is quite precise.\nAcross 500 iterations of MultinomialNB, there was not a significant difference in accuracy or F1 score of the models when they were and when they weren’t trained with protected class information. For other cases, the difference, while statistically significant, was within 1 percentage point of the top performer. Operationally speaking, there’s not a substantial need for that level of performance. The protected class information inclusion or exclusion makes a statistically significant, but not operationally impactful, difference in model performance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#overall",
    "href": "reg.html#overall",
    "title": "9  Regression Modeling",
    "section": "9.5 Overall",
    "text": "9.5 Overall\nThe performance of Logistic Regression, with and without protected classes, far outshined the performance of Multinomial Naive Bayes. Multinomial Naive Bayes may have had better performance had the data provided the opportunity to better be count-vectorized, and as such is better suited for assessing document classification than individual record classification.\nAnother difference in the models is what they do and how they do it. Logisitic Regression is discriminative, meaning that we know the potential output classes and one crafts the model to maximize the likelihood of predicting a correct probability for a class, given new input data. MultinomialNB, however, is generative and seeks to identify the probability of the data, given a class.\nThus far, Logistic Regression’s accuracy (along with other metrics) makes it a top contender for modeling. This, however, comes at the cost of substantial dimensionality to explain approximately 99.99% of the variance in the data with the MCA execution. If dimensionality reduction, computational memory and time were a constraint, it may render the execution of future predictions with logisitic regression infeasible. This especially becomes the case when the model needs to be refit with additional new data; the sheer volume of features and row vectors produces a tremendous amount of data (near 1 Gb), and the execution of gradient descent upon said data to produce a well-performing logisitic regression is also computationally expensive and time consuming.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "app2.html",
    "href": "app2.html",
    "title": "Appendix B — Component Analysis Code (PCA and MCA)",
    "section": "",
    "text": "B.1 Module and Data Imports\nimport pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.cluster import KMeans\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom prince import MCA\n\nfr = pd.read_csv('../data/final_clean.csv')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Component Analysis Code (PCA and MCA)</span>"
    ]
  },
  {
    "objectID": "app2.html#module-and-data-imports",
    "href": "app2.html#module-and-data-imports",
    "title": "Appendix B — Component Analysis Code (PCA and MCA)",
    "section": "",
    "text": "B.1.1 Principal Component Analysis\n\nB.1.1.1 Select & Scale Numeric Columns\n\ncolumns = [\n    #'loan_amount',\n    #'property_value',\n    'income',\n    'interest_rate',\n    'total_loan_costs',\n    'loan_to_value_ratio',\n    #'origination_costs',\n    #'discount_points',\n    #'lender_credits',\n    'loan_term',\n    'intro_rate_period',\n    'total_units',\n    'tract_minority_population_percent',\n    'tract_population',\n    'tract_to_msa_income_percentage',\n    'tract_owner_occupied_units',\n    'tract_one_to_four_family_homes',\n    'tract_median_age_of_housing_units',\n    #'debt_to_income_ratio'\n]\n\nX = fr[columns]\n\nX = StandardScaler().fit_transform(X)\n\n\n\nB.1.1.2 Perform 2D PCA\n\npca2d = PCA(n_components=2)\nresult2d = pd.DataFrame(pca2d.fit_transform(X))\nresult2d['outcome']  = fr['outcome'].astype(bool)\n\ndisplay(\n    np.cumsum(pca2d.explained_variance_) #eigenvalues\n)\n\nsns.scatterplot(\n    data=result2d,\n    x=0,y=1,hue='outcome'\n)\nnp.cumsum(pca2d.explained_variance_ratio_)\n\narray([2.71077093, 4.35879384])\n\n\narray([0.20851998, 0.33529045])\n\n\n\n\n\n\n\n\n\n\n\nB.1.1.3 Perform 3D PCA\n\npca3d = PCA(n_components=3)\nresult3d = pd.DataFrame(pca3d.fit_transform(X))\nresult3d['outcome']  = fr['outcome'].astype(bool)\ndisplay(\n    np.cumsum(pca3d.explained_variance_) #eigenvalues\n)\nresult3d\nnp.cumsum(pca3d.explained_variance_ratio_)\n\narray([2.71077093, 4.35879384, 5.61886437])\n\n\narray([0.20851998, 0.33529045, 0.43221855])\n\n\n\nfig = plt.figure(figsize=(12,12))\nax = Axes3D(fig,rect=[0,0,.9,1],elev=5,azim=225)\n\nfig.add_axes(ax)\n\nx=result3d[0]\ny=result3d[1]\nz=result3d[2]\n\nax.scatter(x,y,z, cmap=\"RdYlGn\", edgecolor='k', s=40, c=fr['outcome'].astype(int))\n\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Component Analysis Code (PCA and MCA)</span>"
    ]
  },
  {
    "objectID": "app4.html",
    "href": "app4.html",
    "title": "Appendix C — Association Rule Mining Code",
    "section": "",
    "text": "C.1 Module and Data Imports\nimport pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.cluster import KMeans\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom mlxtend.frequent_patterns import apriori\n\nfrom mlxtend.frequent_patterns import association_rules",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Association Rule Mining Code</span>"
    ]
  },
  {
    "objectID": "app4.html#transform-to-basket-transactions",
    "href": "app4.html#transform-to-basket-transactions",
    "title": "Appendix C — Association Rule Mining Code",
    "section": "C.2 Transform to Basket Transactions",
    "text": "C.2 Transform to Basket Transactions\n\nmapper = {\n    'applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    # 'co-applicant_race':{\n    #     'American Indian/Alaska Native':0b0000000000000000001,\n    #     'Asian':0b0000000000000000010,\n    #     'Asian Indian':0b0000000000000000100,\n    #     'Chinese':0b0000000000000001000,\n    #     'Filipino':0b0000000000000010000,\n    #     'Japanese':0b0000000000000100000,\n    #     'Korean':0b0000000000001000000,\n    #     'Vietnamese':0b0000000000010000000,\n    #     'Other Asian':0b0000000000100000000,\n    #     'Black/African American':0b0000000001000000000,\n    #     'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n    #     'Native Hawaiian':0b0000000100000000000,\n    #     'Guamanian/Chamorro':0b0000001000000000000,\n    #     'Samoan':0b0000010000000000000,\n    #     'Other Pacific Islander':0b0000100000000000000,\n    #     'White':0b0001000000000000000,\n    #     'Information not provided':0b0010000000000000000,\n    #     'Not Applicable':0b0100000000000000000,\n    #     'No Co-applicant':0b1000000000000000000\n    # },\n    'applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    # 'co-applicant_ethnicity':{\n    #     'Hispanic/Latino':0b000000001,\n    #     'Mexican':0b000000010,\n    #     'Puerto Rican':0b000000100,\n    #     'Cuban':0b000001000,\n    #     'Other Hispanic/Latino':0b000010000,\n    #     'Not Hispanic/Latino':0b000100000,\n    #     'Information Not Provided':0b001000000,\n    #     'Not Applicable':0b010000000,\n    #     'No Co-applicant':0b100000000\n    # },\n    'aus':{\n        'Desktop Underwriter':0b00000001,\n        'Loan Prospector/Product Advisor':0b00000010,\n        'TOTAL Scorecard':0b00000100,\n        'GUS':0b00001000,\n        'Other':0b00010000,\n        'Internal Proprietary':0b00100000,\n        'Not applicable':0b01000000,\n        'Exempt':0b10000000,\n    }, \n    'denial_reason':{\n        'DTI':0b0000000001,\n        'Employment History':0b0000000010,\n        'Credit History':0b0000000100,\n        'Collateral':0b0000001000,\n        'Insufficient Cash':0b0000010000,\n        'Unverifiable Information':0b0000100000,\n        'Credit Application Incomplete':0b0001000000,\n        'Mortgage Insurance Denied':0b0010000000,\n        'Other':0b0100000000,\n        'Not Applicable':0b1000000000\n    }\n}\n\nnew_mapper = {}\nfor k,v in mapper.items():\n    new_mapper[k] = {}\n    #print(k)\n    for j,w in v.items():\n        #print(w,j)\n        new_mapper[k][w] = j\n\n\nfr2 = pd.read_csv('../data/final_clean_r2.csv')#fr.copy()\n\npct = [20,40,60,80]\n\nlevels = ['0-20','21-40','41-60','61-80','&gt;80']\n\npct_cols = [\n    'income',\n    'debt_to_income_ratio',\n    'loan_to_value_ratio',\n    'tract_minority_population_percent',\n    'tract_to_msa_income_percentage',\n    'tract_median_age_of_housing_units',\n    'interest_rate'\n]\n\nfor col in pct_cols:\n    p = list(map(lambda x: np.percentile(fr2[col],x),pct))\n    p = [-np.inf] + p + [np.inf]\n    fr2[col] = pd.cut(fr2[col],bins=p,labels=levels)\n\n\nbasket = []\nbc = [\n    'income','debt_to_income_ratio','loan_to_value_ratio',\n    'tract_minority_population_percent','tract_to_msa_income_percent',\n    'derived_sex'\n]\nb1c = ['interest_rate','company','applicant_race']\nb2c = ['interest_rate','company','outcome']\nb3c = ['interest_rate','applicant_race','outcome']\n\nb1,b2,b3 = [],[],[]\n\nfor i, row in fr2.iterrows():\n    curr = []\n    for k,v in new_mapper.items():\n        for j,w in v.items():\n            #print(row[k],type(row[k]))\n            if row[k] & j &gt; 0:\n                curr.append(\"{}:{}\".format(k,w))\n\n    if row['balloon_payment'] == 1:\n        curr.append('balloon')\n    \n    if row['interest_only_payment'] == 1:\n        curr.append('interest only')\n    \n    curr.append(\"{} rooms\".format(row['total_units']))\n\n    for col in pct_cols:\n        curr.append(\"{}:{}\".format(col,row[col]))\n    \n    # curr.append(row['company'])\n\n    curr.append(row['derived_sex'])\n\n    curr.append(\"age_category:{}\".format(row['applicant_age']))\n\n    basket.append(curr)\n\n\nitems = set()\nfor trans in basket:\n    for item in trans:\n        items.add(item)\n\nresult = []\nfor record in basket:\n    rowset = set(record)\n    labels = {}\n    uncommons = list(items-rowset)\n    commons = list(items.intersection(rowset))\n    for uc in uncommons:\n        labels[uc] = False\n    for com in commons:\n        labels[com] = True\n    result.append(labels)\n\nohe_df = pd.DataFrame(result)\n\nsingle_basket = ohe_df.replace(\n    [0],[np.nan]\n).reset_index()\n\nsingle_basket = single_basket.melt(\n    id_vars='index',\n    value_vars=single_basket.columns[1:]\n)\n\nsingle_basket.sort_values(by='index',inplace=True)\nsingle_basket.dropna(inplace=True)\n\n# display(\n#     ohe_df.head(),\n#     single_basket.head()\n# )\n\nsingle_basket.to_csv('../data/ARM_single_basket.csv',index=False)\n\n\nC.2.1 Construct Frequent Items\n\nfreq_items=apriori(ohe_df,min_support=0.05,use_colnames=True,verbose=1)\n\n\nProcessing 3306 combinations | Sampling itemset size 2\nProcessing 23013 combinations | Sampling itemset size 3\nProcessing 19952 combinations | Sampling itemset size 4\nProcessing 13885 combinations | Sampling itemset size 5\nProcessing 3780 combinations | Sampling itemset size 6\nProcessing 343 combinations | Sampling itemset size 7\n\nars = association_rules(freq_items,metric='support',min_threshold=0.5)\nars.sort_values(by='lift',ascending=False,inplace=True)\n\n\n\nC.2.2 Construct Association Rules (Python)\n\n\nC.2.3 Save the Top 15 Association Rules by Category\n\n\nC.2.4 Performing Rule Mining and Visualization in R\nRule mining is performed on data that was already transformed in Python from previous steps\n\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)\n\nFirst load the data into R for use in rule mining.\n\ndat &lt;- read.transactions(\n    'C:/Users/pconn/OneDrive/Desktop/Machine Learning/ML/data/single_basket.csv',\n    sep=',',\n    rm.duplicates=TRUE,\n    format='single',\n    cols=c(1,2)\n)\n\nNow that the data is loaded, R is leveraged to perform rule mining\n\nset.seed = 9001\n\na_rules &lt;- arules::apriori(\n    dat,\n    control=list(verbose=F),\n    parameter=list(support=0.04,confidence=0.01,minlen=2)\n)\n\nUsing the mined rules, the rules can now be printed and/or visualized.\n\nsorted_arules &lt;- sort(a_rules,by='support',decreasing = T)\narules::inspect(sorted_arules[1:15])\n\n     lhs                                           rhs                                         support confidence  coverage     lift  count\n[1]  {approve}                                  =&gt; {1 rooms}                                 0.8465587  0.9897986 0.8552837 1.001607 172124\n[2]  {1 rooms}                                  =&gt; {approve}                                 0.8465587  0.8566580 0.9882108 1.001607 172124\n[3]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {1 rooms}                                 0.7152005  0.9893591 0.7228928 1.001162 145416\n[4]  {1 rooms}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.7152005  0.7237327 0.9882108 1.001162 145416\n[5]  {applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6253676  0.8650905 0.7228928 1.011466 127151\n[6]  {approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6253676  0.7311815 0.8552837 1.011466 127151\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                                                             \n      approve}                                  =&gt; {1 rooms}                                 0.6195837  0.9907512 0.6253676 1.002571 125975\n[8]  {1 rooms,                                                                                                                             \n      applicant_ethnicity:Not Hispanic/Latino}  =&gt; {approve}                                 0.6195837  0.8663077 0.7152005 1.012889 125975\n[9]  {1 rooms,                                                                                                                             \n      approve}                                  =&gt; {applicant_ethnicity:Not Hispanic/Latino} 0.6195837  0.7318852 0.8465587 1.012439 125975\n[10] {applicant_race:White}                     =&gt; {1 rooms}                                 0.6052911  0.9917401 0.6103324 1.003571 123069\n[11] {1 rooms}                                  =&gt; {applicant_race:White}                    0.6052911  0.6125121 0.9882108 1.003571 123069\n[12] {aus:Desktop Underwriter}                  =&gt; {1 rooms}                                 0.5645626  0.9910383 0.5696678 1.002861 114788\n[13] {1 rooms}                                  =&gt; {aus:Desktop Underwriter}                 0.5645626  0.5712977 0.9882108 1.002861 114788\n[14] {applicant_race:White}                     =&gt; {approve}                                 0.5349397  0.8764727 0.6103324 1.024774 108765\n[15] {approve}                                  =&gt; {applicant_race:White}                    0.5349397  0.6254529 0.8552837 1.024774 108765\n\n\n\nsorted_arules &lt;- sort(a_rules,by='confidence',decreasing = T)\narules::inspect(sorted_arules[1:15])\n\n     lhs                                         rhs            support confidence   coverage     lift count\n[1]  {interest_rate:&gt;80}                      =&gt; {approve}   0.18578904          1 0.18578904 1.169203 37775\n[2]  {aus:Loan Prospector/Product Advisor,                                                                  \n      aus:Other}                              =&gt; {JP Morgan} 0.15580213          1 0.15580213 4.971320 31678\n[3]  {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:61-80}              =&gt; {approve}   0.04241056          1 0.04241056 1.169203  8623\n[4]  {interest_rate:&gt;80,                                                                                    \n      tract_minority_population_percent:0-20} =&gt; {approve}   0.04197283          1 0.04197283 1.169203  8534\n[5]  {debt_to_income_ratio:61-80,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04571074          1 0.04571074 1.169203  9294\n[6]  {interest_rate:&gt;80,                                                                                    \n      tract_to_msa_income_percentage:21-40}   =&gt; {approve}   0.04036454          1 0.04036454 1.169203  8207\n[7]  {debt_to_income_ratio:21-40,                                                                           \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04220891          1 0.04220891 1.169203  8582\n[8]  {Female,                                                                                               \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04209579          1 0.04209579 1.169203  8559\n[9]  {2.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.04509104          1 0.04509104 1.169203  9168\n[10] {1.0,                                                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.05827702          1 0.05827702 1.169203 11849\n[11] {interest_rate:&gt;80,                                                                                    \n      loan_to_value_ratio:21-40}              =&gt; {approve}   0.05279802          1 0.05279802 1.169203 10735\n[12] {interest_rate:&gt;80,                                                                                    \n      Joint}                                  =&gt; {approve}   0.06351993          1 0.06351993 1.169203 12915\n[13] {interest_rate:&gt;80,                                                                                    \n      Male}                                   =&gt; {approve}   0.06214773          1 0.06214773 1.169203 12636\n[14] {aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                      =&gt; {approve}   0.08140290          1 0.08140290 1.169203 16551\n[15] {interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                        =&gt; {approve}   0.10214832          1 0.10214832 1.169203 20769\n\n\n\nsorted_arules &lt;- sort(a_rules,by='lift',decreasing = T)\narules::inspect(sorted_arules[1:15])\n\n     lhs                                                rhs                                      support confidence   coverage     lift count\n[1]  {applicant_ethnicity:Mexican}                   =&gt; {applicant_ethnicity:Hispanic/Latino} 0.04056128  0.9169446 0.04423525 8.207934  8247\n[2]  {applicant_ethnicity:Hispanic/Latino}           =&gt; {applicant_ethnicity:Mexican}         0.04056128  0.3630800 0.11171442 8.207934  8247\n[3]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07532387  0.7362273 0.10231062 7.834365 15315\n[4]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07603703  0.7353151 0.10340740 7.824658 15460\n[5]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06595941  0.7333224 0.08994600 7.803453 13411\n[6]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06651026  0.7322395 0.09083129 7.791930 13523\n[7]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05100776  0.7264131 0.07021867 7.729930 10371\n[8]  {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.05134712  0.7252518 0.07079903 7.717572 10440\n[9]  {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04512547  0.7246663 0.06227068 7.711341  9175\n[10] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter,                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.04538122  0.7233459 0.06273792 7.697291  9227\n[11] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04939456  0.6718625 0.07351885 7.149444 10043\n[12] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07602227  0.6709350 0.11330795 7.139574 15457\n[13] {applicant_ethnicity:Information Not Provided,                                                                                          \n      applicant_race:Information not provided,                                                                                               \n      approve,                                                                                                                               \n      aus:Desktop Underwriter}                       =&gt; {Sex Not Available}                   0.04969457  0.6703377 0.07413364 7.133218 10104\n[14] {applicant_ethnicity:Information Not Provided,                                                                                          \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.07674034  0.6700880 0.11452278 7.130562 15603\n[15] {1 rooms,                                                                                                                               \n      applicant_ethnicity:Information Not Provided,                                                                                          \n      approve,                                                                                                                               \n      Rocket Mortgage}                               =&gt; {Sex Not Available}                   0.06656437  0.6685768 0.09956129 7.114480 13534\n\n\n\nsub &lt;- head(sort(a_rules,by='lift',decreasing = T),10)\nplot(sub,method=\"graph\",engine=\"html\")\n\n\n\n\n\n\n\nC.2.5 Examining Organization Specific Rules\n\nNFCU &lt;- subset(dat, subset = items %in% \"Navy Federal Credit Union\")\nJPM &lt;- subset(dat, subset = items %in% 'JP Morgan')\nBOA &lt;- subset(dat, subset = items %in% 'Bank of America')\nWF &lt;- subset(dat, subset = items %in% 'Wells Fargo')\nRM &lt;- subset(dat, subset = items %in% 'Rocket Mortgage')\n\nget_rules &lt;- function(trns,appear,sup,conf,len){\n    arules::apriori(\n        trns,parameter=list(support=sup,confidence=conf,minlen=len) ,\n        control=list(verbose=F),\n        appearance = appear\n    )\n}\n\nNFCU_app &lt;- get_rules(trns=NFCU,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nNFCU_den &lt;- get_rules(trns=NFCU,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nJPM_app &lt;- get_rules(trns=JPM,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nJPM_den &lt;- get_rules(trns=JPM,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nBOA_app &lt;- get_rules(trns=BOA,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nBOA_den &lt;- get_rules(trns=BOA,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nWF_app &lt;- get_rules(trns=WF,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nWF_den &lt;- get_rules(trns=WF,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\nRM_app &lt;- get_rules(trns=RM,appear=list(default='lhs',rhs='approve'),sup=0.04,conf=0.01,len=3)\nRM_den &lt;- get_rules(trns=RM,appear=list(default='lhs',rhs='deny'),sup=0.04,conf=0.01,len=3)\n\n\nplot(sort(NFCU_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(JPM_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(BOA_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(WF_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\nplot(sort(RM_den,by='lift')[1:10],engine='html',method='graph')\n\n\n\n\n\n\na_rules_den &lt;- arules::apriori(\n    dat,\n    control=list(verbose=F),\n    parameter=list(support=0.04,confidence=0.01,minlen=2),\n    appearance = list(default='lhs',rhs='deny')\n)\n\na_rules_app &lt;- arules::apriori(\n    dat,\n    control=list(verbose=F),\n    parameter=list(support=0.04,confidence=0.01,minlen=2),\n    appearance = list(default='lhs',rhs='approve')\n)\ninspect(sort(\n    subset(\n        a_rules_den,lhs %pin% 'race:'\n    ), by='lift'\n))\n\n    lhs                                           rhs       support confidence  coverage      lift count\n[1] {applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04213514  0.3407446 0.1236561 2.3546500  8567\n[2] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     interest_rate:41-60}                      =&gt; {deny} 0.04138263  0.3381153 0.1223921 2.3364812  8414\n[3] {applicant_race:White}                     =&gt; {deny} 0.07539273  0.1235273 0.6103324 0.8536119 15329\n[4] {1 rooms,                                                                                           \n     applicant_race:White}                     =&gt; {deny} 0.07414348  0.1224923 0.6052911 0.8464593 15075\n[5] {applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04111213  0.1185674 0.3467406 0.8193371  8359\n[6] {1 rooms,                                                                                           \n     applicant_race:White,                                                                              \n     aus:Desktop Underwriter}                  =&gt; {deny} 0.04052193  0.1176227 0.3445077 0.8128092  8239\n[7] {applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05616707  0.1111652 0.5052577 0.7681857 11420\n[8] {1 rooms,                                                                                           \n     applicant_ethnicity:Not Hispanic/Latino,                                                           \n     applicant_race:White}                     =&gt; {deny} 0.05537030  0.1103444 0.5017952 0.7625140 11258\n\ninspect(sort(\n    subset(\n        a_rules_app,lhs %pin% 'race:'\n    ), by='lift'\n)[1:10])\n\n     lhs                                           rhs          support confidence   coverage     lift count\n[1]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11876236          1 0.11876236 1.169203 24147\n[2]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04705836          1 0.04705836 1.169203  9568\n[3]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Male}                                     =&gt; {approve} 0.04324667          1 0.04324667 1.169203  8793\n[4]  {applicant_race:White,                                                                                 \n      aus:Loan Prospector/Product Advisor,                                                                  \n      interest_rate:&gt;80}                        =&gt; {approve} 0.05488339          1 0.05488339 1.169203 11159\n[5]  {applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Rocket Mortgage}                          =&gt; {approve} 0.06145424          1 0.06145424 1.169203 12495\n[6]  {applicant_race:White,                                                                                 \n      aus:Desktop Underwriter,                                                                              \n      interest_rate:&gt;80}                        =&gt; {approve} 0.08051760          1 0.08051760 1.169203 16371\n[7]  {applicant_ethnicity:Not Hispanic/Latino,                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.09885305          1 0.09885305 1.169203 20099\n[8]  {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80}                        =&gt; {approve} 0.11780329          1 0.11780329 1.169203 23952\n[9]  {applicant_race:White,                                                                                 \n      interest_rate:0-20,                                                                                   \n      Rocket Mortgage}                          =&gt; {approve} 0.05066840          1 0.05066840 1.169203 10302\n[10] {1 rooms,                                                                                              \n      applicant_race:White,                                                                                 \n      interest_rate:&gt;80,                                                                                    \n      Joint}                                    =&gt; {approve} 0.04677802          1 0.04677802 1.169203  9511",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Association Rule Mining Code</span>"
    ]
  },
  {
    "objectID": "app6.html",
    "href": "app6.html",
    "title": "Appendix D — Naive Bayes Code",
    "section": "",
    "text": "D.1 Data PreProcessing for CategoricalNB\n#prep the data - turn binary back to true/false\nmapper = {\n    'applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    'co-applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    'applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    'co-applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    'aus':{\n        'Desktop Underwriter':0b00000001,\n        'Loan Prospector/Product Advisor':0b00000010,\n        'TOTAL Scorecard':0b00000100,\n        'GUS':0b00001000,\n        'Other':0b00010000,\n        'Internal Proprietary':0b00100000,\n        'Not applicable':0b01000000,\n        'Exempt':0b10000000,\n    }, \n}\n\nnew_mapper = {}\nfor k,v in mapper.items():\n    new_mapper[k] = {}\n    #print(k)\n    for j,w in v.items():\n        #print(w,j)\n        new_mapper[k][w] = j\n\n#drop columns that will not be leveraged in MCA\nfr.drop(\n    labels = [\n        'balloon_payment', \n        'interest_only_payment', \n        'other_nonamortizing_features',\n        'income_from_median',\n        'state_code',\n        'county_code'\n    ],\n    axis=1,inplace=True\n)\n\n#identify numeric columns to convert to categorical\nnumerics = [\n    'income',\n    'loan_amount',\n    'interest_rate',\n    'total_loan_costs',\n    'origination_charges',\n    'discount_points',\n    'lender_credits',\n    'loan_term',\n    'intro_rate_period',\n    'property_value',\n    'total_units',\n    'tract_population',\n    'tract_minority_population_percent',\n    'ffiec_msa_md_median_family_income',\n    'tract_to_msa_income_percentage',\n    'tract_owner_occupied_units',\n    'tract_one_to_four_family_homes',\n    'tract_median_age_of_housing_units',\n    'loan_to_value_ratio'\n]\n\n#set the cutting boundaries\nbounds = [i/5 for i in range(1,5)]\n\nfor col in numerics:\n    #income had some errors, for some reason\n    if col == 'income':\n        fr.loc[fr[col]&lt;=0,col] = 0.01\n        fr[col] = np.log(fr[col])\n\n    s = fr[col].std()\n\n    m = fr[col].mean()\n\n    #cut everything based on standard deviations\n    cut_level = [\n        m-2*s,\n        m-s,\n        m+s,\n        m+2*s\n    ]\n    \n    cut_level = [-np.inf] + cut_level + [np.inf]\n\n    #assign value based on cut boundaries\n    fr[col] = pd.cut(\n        fr[col],\n        bins=cut_level,\n        labels=[\"L\",\"ML\",\"M\",\"MH\",\"H\"]\n    )\n\n    #convert to categorical\n    fr[col] = fr[col].astype('category')\n\nfr[numerics].head(10)\n\n\n\n\n\n\n\n\nincome\nloan_amount\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\nintro_rate_period\nproperty_value\ntotal_units\ntract_population\ntract_minority_population_percent\nffiec_msa_md_median_family_income\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\nloan_to_value_ratio\n\n\n\n\n0\nM\nMH\nL\nH\nH\nM\nH\nM\nH\nM\nM\nM\nM\nM\nM\nMH\nMH\nM\nM\n\n\n1\nL\nMH\nL\nM\nM\nM\nM\nM\nH\nH\nM\nM\nML\nM\nH\nM\nL\nML\nL\n\n\n2\nMH\nH\nML\nM\nM\nM\nM\nM\nH\nH\nM\nH\nM\nM\nH\nM\nL\nML\nM\n\n\n3\nM\nMH\nML\nM\nM\nM\nM\nM\nH\nM\nM\nH\nM\nM\nH\nH\nH\nML\nM\n\n\n4\nM\nH\nML\nM\nM\nM\nM\nM\nH\nH\nM\nM\nM\nH\nMH\nM\nM\nMH\nM\n\n\n5\nMH\nMH\nM\nMH\nM\nM\nM\nM\nM\nMH\nM\nM\nML\nM\nM\nM\nM\nM\nM\n\n\n6\nMH\nH\nM\nM\nM\nM\nM\nM\nMH\nH\nM\nM\nMH\nM\nM\nM\nM\nM\nM\n\n\n7\nMH\nM\nML\nM\nM\nM\nM\nM\nM\nH\nM\nM\nML\nM\nH\nMH\nML\nMH\nL\n\n\n8\nM\nH\nML\nM\nM\nM\nM\nM\nH\nH\nM\nH\nM\nM\nMH\nMH\nML\nH\nM\n\n\n9\nM\nM\nML\nM\nM\nM\nM\nM\nH\nM\nM\nM\nML\nM\nMH\nMH\nM\nML\nML\nfr_bin = fr[[\n    'applicant_race',\n    'applicant_ethnicity',\n    'co-applicant_race',\n    'co-applicant_ethnicity',\n    'aus'\n]].copy()\n\nfor k,v in new_mapper.items():\n    for l,w in v.items():\n        fr_bin[k+'_'+w] = (fr_bin[k]&l &gt; 0).astype(int)\n\nfr_bin.drop(\n    labels=[    \n        'applicant_race',\n        'applicant_ethnicity',\n        'co-applicant_race',\n        'co-applicant_ethnicity',\n        'aus'\n    ],\n    inplace=True,\n    axis=1\n)\n\nfr.drop(\n    labels=[\n        'applicant_race',\n        'applicant_ethnicity',\n        'co-applicant_race',\n        'co-applicant_ethnicity',\n        'denial_reason',\n        'aus',\n        # 'outcome',\n        'action_taken'\n    ],\n    inplace=True,\n    axis=1\n)\ndisplay(\n    fr.head(10),\n    fr_bin.head(10)\n)\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\napplicant_age\nco-applicant_age\ntract_population\ntract_minority_population_percent\nffiec_msa_md_median_family_income\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\ncompany\n\n\n\n\n0\nSex Not Available\n2\n2\nMH\nM\nL\nH\nH\nM\nH\n...\n3.0\n7.0\nM\nM\nM\nM\nMH\nMH\nM\nJP Morgan\n\n\n1\nMale\n2\n2\nMH\nL\nL\nM\nM\nM\nM\n...\n1.0\n8.0\nM\nML\nM\nH\nM\nL\nML\nJP Morgan\n\n\n2\nSex Not Available\n1\n2\nH\nM\nML\nM\nM\nM\nM\n...\n3.0\n8.0\nH\nM\nM\nH\nM\nL\nML\nJP Morgan\n\n\n3\nMale\n2\n2\nMH\nM\nML\nM\nM\nM\nM\n...\n4.0\n8.0\nH\nM\nM\nH\nH\nH\nML\nJP Morgan\n\n\n4\nJoint\n2\n2\nH\nM\nML\nM\nM\nM\nM\n...\n1.0\n1.0\nM\nM\nH\nMH\nM\nM\nMH\nJP Morgan\n\n\n5\nJoint\n1\n2\nMH\nM\nM\nMH\nM\nM\nM\n...\n2.0\n3.0\nM\nML\nM\nM\nM\nM\nM\nJP Morgan\n\n\n6\nJoint\n2\n2\nH\nM\nM\nM\nM\nM\nM\n...\n2.0\n1.0\nM\nMH\nM\nM\nM\nM\nM\nJP Morgan\n\n\n7\nSex Not Available\n1\n2\nM\nL\nML\nM\nM\nM\nM\n...\n1.0\n1.0\nM\nML\nM\nH\nMH\nML\nMH\nJP Morgan\n\n\n8\nJoint\n1\n2\nH\nM\nML\nM\nM\nM\nM\n...\n1.0\n1.0\nH\nM\nM\nMH\nMH\nML\nH\nJP Morgan\n\n\n9\nSex Not Available\n1\n2\nM\nML\nML\nM\nM\nM\nM\n...\n1.0\n1.0\nM\nML\nM\nMH\nMH\nM\nML\nJP Morgan\n\n\n\n\n10 rows × 36 columns\n\n\n\n\n\n\n\n\n\n\napplicant_race_American Indian/Alaska Native\napplicant_race_Asian\napplicant_race_Asian Indian\napplicant_race_Chinese\napplicant_race_Filipino\napplicant_race_Japanese\napplicant_race_Korean\napplicant_race_Vietnamese\napplicant_race_Other Asian\napplicant_race_Black/African American\n...\nco-applicant_ethnicity_Not Applicable\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n10 rows × 64 columns\n#perform label encoding for categorical columns...\nle = LabelEncoder()\nmaps = []\noutdf = fr.copy()\nfor col in outdf.columns:\n    le.fit(outdf[col])\n    d = dict(zip(le.classes_,le.transform(le.classes_)))\n    outdf[col] = le.transform(outdf[col]) #le.fit(outdf[col])\n    maps.append({col:d})\n# outdf.head(10)\noutdf.head(10)\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\napplicant_age\nco-applicant_age\ntract_population\ntract_minority_population_percent\nffiec_msa_md_median_family_income\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\ncompany\n\n\n\n\n0\n3\n1\n1\n2\n2\n1\n0\n0\n1\n0\n...\n3\n7\n2\n1\n2\n2\n3\n3\n1\n1\n\n\n1\n2\n1\n1\n2\n1\n1\n1\n1\n1\n1\n...\n1\n8\n2\n3\n2\n0\n2\n1\n3\n1\n\n\n2\n3\n0\n1\n0\n2\n4\n1\n1\n1\n1\n...\n3\n8\n0\n1\n2\n0\n2\n1\n3\n1\n\n\n3\n2\n1\n1\n2\n2\n4\n1\n1\n1\n1\n...\n4\n8\n0\n1\n2\n0\n0\n0\n3\n1\n\n\n4\n1\n1\n1\n0\n2\n4\n1\n1\n1\n1\n...\n1\n1\n2\n1\n0\n3\n2\n2\n2\n1\n\n\n5\n1\n0\n1\n2\n2\n2\n2\n1\n1\n1\n...\n2\n3\n2\n3\n2\n2\n2\n2\n1\n1\n\n\n6\n1\n1\n1\n0\n2\n2\n1\n1\n1\n1\n...\n2\n1\n2\n2\n2\n2\n2\n2\n1\n1\n\n\n7\n3\n0\n1\n1\n1\n4\n1\n1\n1\n1\n...\n1\n1\n2\n3\n2\n0\n3\n4\n2\n1\n\n\n8\n1\n0\n1\n0\n2\n4\n1\n1\n1\n1\n...\n1\n1\n0\n1\n2\n3\n3\n4\n0\n1\n\n\n9\n3\n0\n1\n1\n4\n4\n1\n1\n1\n1\n...\n1\n1\n2\n3\n2\n3\n3\n2\n3\n1\n\n\n\n\n10 rows × 36 columns\n#rejoin the binary columns to the main frame\noutdf = outdf.join(fr_bin,how='outer')\n# for col in fr_bin.columns:\n#     outdf[col] = fr_bin[col].copy()\n\n#convert all columns to integers\n# for col in outdf.columns:\n#     outdf[col] = outdf[col].astype(int)\noutdf_nr = outdf.copy()\noutdf.head(10)\n\n\n\n\n\n\n\n\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\nco-applicant_ethnicity_Not Applicable\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\n\n\n\n\n0\n3\n1\n1\n2\n2\n1\n0\n0\n1\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n2\n1\n1\n2\n1\n1\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n3\n0\n1\n0\n2\n4\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n2\n1\n1\n2\n2\n4\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n1\n1\n1\n0\n2\n4\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n5\n1\n0\n1\n2\n2\n2\n2\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n1\n1\n1\n0\n2\n2\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n7\n3\n0\n1\n1\n1\n4\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n8\n1\n0\n1\n0\n2\n4\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n9\n3\n0\n1\n1\n4\n4\n1\n1\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n10 rows × 100 columns\n#drop columns that have protected classes...\noutdf_nr.drop(columns=\n    [\n        'derived_sex',\n        'applicant_ethnicity_observed',\n        'co-applicant_ethnicity_observed',\n        'applicant_race_observed',\n        'co-applicant_race_observed',\n        'applicant_sex',\n        'co-applicant_sex',\n        'applicant_sex_observed',\n        'co-applicant_sex_observed',\n        'applicant_age',\n        'co-applicant_age',\n        'applicant_race_American Indian/Alaska Native',\n        'applicant_race_Asian',\n        'applicant_race_Asian Indian',\n        'applicant_race_Chinese',\n        'applicant_race_Filipino',\n        'applicant_race_Japanese',\n        'applicant_race_Korean',\n        'applicant_race_Vietnamese',\n        'applicant_race_Other Asian',\n        'applicant_race_Black/African American',\n        'applicant_race_Native Hawaiian/Pacific Islander',\n        'applicant_race_Native Hawaiian',\n        'applicant_race_Guamanian/Chamorro',\n        'applicant_race_Samoan',\n        'applicant_race_Other Pacific Islander',\n        'applicant_race_White',\n        'applicant_race_Information not provided',\n        'applicant_race_Not Applicable',\n        'applicant_race_No Co-applicant',\n        'co-applicant_race_American Indian/Alaska Native',\n        'co-applicant_race_Asian',\n        'co-applicant_race_Asian Indian',\n        'co-applicant_race_Chinese',\n        'co-applicant_race_Filipino',\n        'co-applicant_race_Japanese',\n        'co-applicant_race_Korean',\n        'co-applicant_race_Vietnamese',\n        'co-applicant_race_Other Asian',\n        'co-applicant_race_Black/African American',\n        'co-applicant_race_Native Hawaiian/Pacific Islander',\n        'co-applicant_race_Native Hawaiian',\n        'co-applicant_race_Guamanian/Chamorro',\n        'co-applicant_race_Samoan',\n        'co-applicant_race_Other Pacific Islander',\n        'co-applicant_race_White',\n        'co-applicant_race_Information not provided',\n        'co-applicant_race_Not Applicable',\n        'co-applicant_race_No Co-applicant',\n        'applicant_ethnicity_Hispanic/Latino',\n        'applicant_ethnicity_Mexican',\n        'applicant_ethnicity_Puerto Rican',\n        'applicant_ethnicity_Cuban',\n        'applicant_ethnicity_Other Hispanic/Latino',\n        'applicant_ethnicity_Not Hispanic/Latino',\n        'applicant_ethnicity_Information Not Provided',\n        'applicant_ethnicity_Not Applicable',\n        'applicant_ethnicity_No Co-applicant',\n        'co-applicant_ethnicity_Hispanic/Latino',\n        'co-applicant_ethnicity_Mexican',\n        'co-applicant_ethnicity_Puerto Rican',\n        'co-applicant_ethnicity_Cuban',\n        'co-applicant_ethnicity_Other Hispanic/Latino',\n        'co-applicant_ethnicity_Not Hispanic/Latino',\n        'co-applicant_ethnicity_Information Not Provided',\n        'co-applicant_ethnicity_Not Applicable',\n        'co-applicant_ethnicity_No Co-applicant'\n    ],\n    axis=1,\n    inplace=True\n)\n#write to csv...\noutdf.to_csv('../data/cnb_pc.csv',index=False)\noutdf_nr.to_csv('../data/cnb_npc.csv',index=False)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Naive Bayes Code</span>"
    ]
  },
  {
    "objectID": "app6.html#categoricalnb",
    "href": "app6.html#categoricalnb",
    "title": "Appendix D — Naive Bayes Code",
    "section": "D.2 CategoricalNB",
    "text": "D.2 CategoricalNB\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nCategoricalNB\nWith Protected Classes\n0.882012\n0.943416\n0.917050\n0.930046\n0.795993\n\n\nCategoricalNB\nWithout Protected Classes\n0.912455\n0.934819\n0.964922\n0.949632\n0.783651\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# are the results significantly different? do a randomization test...\n\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nnp.random.seed(2089)\nfor i in range(500):\n    r = np.random.randint(0,5000,1)\n    # display(r)\n    X_train,X_test,y_train,y_test = train_test_split(\n        outdf,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r[0]\n    )\n    X_train_npc,X_test_npc,y_train,y_test = train_test_split(\n        outdf_nr,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r[0]\n    )\n    try:\n        cnb_pc = CategoricalNB()\n        cnb_npc = CategoricalNB()\n        cnb_pc.fit(X_train,y_train)\n        cnb_npc.fit(X_train_npc,y_train)\n        y_pred = cnb_pc.predict(X_test)\n        y_pred_npc = cnb_npc.predict(X_test_npc)\n    except IndexError:\n        i-=1\n        print(\"Couldn't predict for iteration {}\".format(i+1))\n        pass\n\n    results.loc[len(results)] = {\n        'Model':'Categorical Naive Bayes',\n        'Data':'With Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred),\n        'Precision':precision_score(y_test,y_pred),\n        'Recall':recall_score(y_test,y_pred),\n        'F1':f1_score(y_test,y_pred),\n        'ROC-AUC':roc_auc_score(y_test,y_pred)\n    }\n    results.loc[len(results)] = {\n        'Model':'Categorical Naive Bayes',\n        'Data':'Without Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred_npc),\n        'Precision':precision_score(y_test,y_pred_npc),\n        'Recall':recall_score(y_test,y_pred_npc),\n        'F1':f1_score(y_test,y_pred_npc),\n        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n    }\n\n\nresults.to_csv('../data/cnbRandTest.csv',index=False)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Naive Bayes Code</span>"
    ]
  },
  {
    "objectID": "app6.html#multinomialnb-new",
    "href": "app6.html#multinomialnb-new",
    "title": "Appendix D — Naive Bayes Code",
    "section": "D.3 MultinomialNB (New)",
    "text": "D.3 MultinomialNB (New)\nSince the data can’t be transformed to counts, binary/bernoulli must be leveraged.\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data,\n    labels,\n    stratify=labels,\n    test_size=0.2,\n    random_state=8808\n)\n\nX_train_npc, X_test_npc, y_train, y_test = train_test_split(\n    data_npc,\n    labels,\n    stratify=labels,\n    test_size=0.2,\n    random_state=8808\n)\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nmnb_pc=MultinomialNB()\nmnb_pc.fit(X_train,y_train)\ny_pred = mnb_pc.predict(X_test)\nresults.loc[len(results)] = {\n    'Model':'MultinomialNB',\n    'Data':'With Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'ROC-AUC':roc_auc_score(y_test,y_pred)\n}\nmnb_npc=MultinomialNB()\nmnb_npc.fit(X_train_npc,y_train)\ny_pred_npc=mnb_npc.predict(X_test_npc)\nresults.loc[len(results)] = {\n    'Model':'MultinomialNB',\n    'Data':'Without Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred_npc),\n    'Precision':precision_score(y_test,y_pred_npc),\n    'Recall':recall_score(y_test,y_pred_npc),\n    'F1':f1_score(y_test,y_pred_npc),\n    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n}\n\n\nimport matplotlib.pyplot as plt\ndisplay(results)\nfig,axes=plt.subplots(nrows=1,ncols=2)\n\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred,y_true=y_test\n    )\n).plot(ax=axes[0])\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred_npc,y_true=y_test\n    )\n).plot(ax=axes[1])\naxes[0].set_title('With Protected\\nClasses')\naxes[1].set_title('Without Protected\\nClasses')\nplt.suptitle(\"Confusion Matrices - Multinomial Naive Bayes\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nMultinomialNB\nWith Protected Classes\n0.936358\n0.967880\n0.957361\n0.962591\n0.884798\n\n\n1\nMultinomialNB\nWithout Protected Classes\n0.939653\n0.969309\n0.959833\n0.964548\n0.890112\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# are the results significantly different? do a randomization test...\n\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nnp.random.seed(2049)\nfor i in range(500):\n    r = np.random.randint(0,5000,1)\n    X_train,X_test,y_train,y_test = train_test_split(\n        outdf,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r[0]\n    )\n    X_train_npc,X_test_npc,y_train,y_test = train_test_split(\n        outdf_nr,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r[0]\n    )\n    mnb_pc.fit(X_train,y_train)\n    mnb_npc.fit(X_train_npc,y_train)\n    y_pred = mnb_pc.predict(X_test)\n    y_pred_npc = mnb_npc.predict(X_test_npc)\n    results.loc[len(results)] = {\n        'Model':'Multinomial Naive Bayes',\n        'Data':'With Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred),\n        'Precision':precision_score(y_test,y_pred),\n        'Recall':recall_score(y_test,y_pred),\n        'F1':f1_score(y_test,y_pred),\n        'ROC-AUC':roc_auc_score(y_test,y_pred)\n    }\n    results.loc[len(results)] = {\n        'Model':'Multinomial Naive Bayes',\n        'Data':'Without Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred_npc),\n        'Precision':precision_score(y_test,y_pred_npc),\n        'Recall':recall_score(y_test,y_pred_npc),\n        'F1':f1_score(y_test,y_pred_npc),\n        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n    }\n\n\nresults.to_csv('../data/mnbRandTest.csv',index=False)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Naive Bayes Code</span>"
    ]
  },
  {
    "objectID": "app6.html#bernoulli-naive-bayes",
    "href": "app6.html#bernoulli-naive-bayes",
    "title": "Appendix D — Naive Bayes Code",
    "section": "D.4 Bernoulli Naive Bayes",
    "text": "D.4 Bernoulli Naive Bayes\n\n#prep the data - done\n\n\n#split the data in to training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data,\n    labels,\n    stratify=labels,\n    test_size=0.2,\n    random_state=8808\n)\n\nX_train_npc, X_test_npc, y_train, y_test = train_test_split(\n    data_npc,\n    labels,\n    stratify=labels,\n    test_size=0.2,\n    random_state=8808\n)\n\n\n#train the model\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nbnb=BernoulliNB()\nbnb.fit(X_train,y_train)\ny_pred = bnb.predict(X_test)\nresults.loc[len(results)] = {\n    'Model':'BernoulliNB',\n    'Data':'With Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'ROC-AUC':roc_auc_score(y_test,y_pred)\n}\nbnb_nr=BernoulliNB()\nbnb_nr.fit(X_train_npc,y_train)\ny_pred_npc=bnb_nr.predict(X_test_npc)\nresults.loc[len(results)] = {\n    'Model':'BernoulliNB',\n    'Data':'Without Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred_npc),\n    'Precision':precision_score(y_test,y_pred_npc),\n    'Recall':recall_score(y_test,y_pred_npc),\n    'F1':f1_score(y_test,y_pred_npc),\n    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n}\n\n\n#display summarized results (confusion matrix)\nimport matplotlib.pyplot as plt\ndisplay(results)\nfig,axes=plt.subplots(nrows=1,ncols=2)\n\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred,y_true=y_test\n    )\n).plot(ax=axes[0])\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred_npc,y_true=y_test\n    )\n).plot(ax=axes[1])\naxes[0].set_title('With Protected\\nClasses')\naxes[1].set_title('Without Protected\\nClasses')\nplt.suptitle(\"Confusion Matrices - Bernoulli Naive Bayes\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nBernoulliNB\nWith Protected Classes\n0.937514\n0.973560\n0.952818\n0.963077\n0.899943\n\n\n1\nBernoulliNB\nWithout Protected Classes\n0.939875\n0.977551\n0.951553\n0.964377\n0.911205\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nD.4.1 Check for Difference in Performance Due to Random Chance\n\n# are the results significantly different? do a randomization test...\n\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nnp.random.seed(2034)\nfor i in range(500):\n    r = np.random.randint(0,5000,1)\n    X_train, X_test, y_train, y_test = train_test_split(\n        data,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r[0]\n    )\n    X_train_npc, X_test_npc, y_train, y_test = train_test_split(\n        data_npc,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r[0]\n    )\n    mnb_pc.fit(X_train,y_train)\n    mnb_npc.fit(X_train_npc,y_train)\n    y_pred = mnb_pc.predict(X_test)\n    y_pred_npc = mnb_npc.predict(X_test_npc)\n    results.loc[len(results)] = {\n        'Model':'BernoulliNB',\n        'Data':'With Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred),\n        'Precision':precision_score(y_test,y_pred),\n        'Recall':recall_score(y_test,y_pred),\n        'F1':f1_score(y_test,y_pred),\n        'ROC-AUC':roc_auc_score(y_test,y_pred)\n    }\n    results.loc[len(results)] = {\n        'Model':'BernoulliNB',\n        'Data':'Without Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred_npc),\n        'Precision':precision_score(y_test,y_pred_npc),\n        'Recall':recall_score(y_test,y_pred_npc),\n        'F1':f1_score(y_test,y_pred_npc),\n        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n    }\n\n\nresults.to_csv('../data/bnbRandTest.csv',index=False)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Naive Bayes Code</span>"
    ]
  },
  {
    "objectID": "app5.html",
    "href": "app5.html",
    "title": "Appendix E — Decision Tree Code",
    "section": "",
    "text": "E.1 Label Encoded Data\n#build the training and test data\nX_train,X_test,y_train,y_test = train_test_split(\n    clean,\n    labels,\n    stratify=labels,\n    test_size=0.2,\n    random_state=9999\n)\n\nX_train_npc,X_test_npc,y_train,y_test = train_test_split(\n    clean_npc,\n    labels,\n    stratify=labels,\n    test_size=0.2,\n    random_state=9999\n)\nderived_sex\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\n...\nco-applicant_ethnicity_Not Applicable\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\n\n\n\n\n142289\n3\n0\n1\n1\n2\n3\n1\n1\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n174168\n0\n0\n1\n2\n2\n3\n1\n1\n1\n1\n...\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n106229\n1\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n30766\n0\n1\n1\n3\n2\n2\n3\n1\n1\n1\n...\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n34556\n0\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n1\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n60130\n0\n1\n0\n1\n1\n2\n1\n1\n1\n1\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n114480\n0\n0\n1\n1\n4\n2\n1\n1\n1\n1\n...\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n61174\n2\n1\n1\n1\n2\n2\n1\n1\n1\n1\n...\n0\n1\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n131619\n2\n0\n1\n1\n4\n2\n1\n1\n1\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n77029\n2\n1\n1\n2\n2\n2\n1\n1\n1\n1\n...\n0\n1\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n10 rows × 100 columns\n\n\n\n\n\n\n\n\n\n\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\n...\ntract_median_age_of_housing_units\ncompany\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\n\n\n\n\n142289\n0\n1\n1\n2\n3\n1\n1\n1\n1\n2\n...\n2\n3\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n174168\n0\n1\n2\n2\n3\n1\n1\n1\n1\n2\n...\n1\n3\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n106229\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n1\n2\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n30766\n1\n1\n3\n2\n2\n3\n1\n1\n1\n2\n...\n1\n1\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n34556\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n1\n1\n1\n1\n0\n0\n1\n0\n0\n0\n\n\n60130\n1\n0\n1\n1\n2\n1\n1\n1\n1\n2\n...\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n114480\n0\n1\n1\n4\n2\n1\n1\n1\n1\n1\n...\n1\n3\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n61174\n1\n1\n1\n2\n2\n1\n1\n1\n1\n2\n...\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n131619\n0\n1\n1\n4\n2\n1\n1\n1\n1\n1\n...\n3\n3\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n77029\n1\n1\n2\n2\n2\n1\n1\n1\n1\n2\n...\n1\n4\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n\n\n10 rows × 33 columns\ndt_pc,dt_npc = DecisionTreeClassifier(max_depth=4),DecisionTreeClassifier(max_depth=4)\n# re-fit using the new data on the previous models\ndt_pc.fit(X_train,y_train)\ndt_npc.fit(X_train_npc,y_train)\n#generate predictions\ny_pred = dt_pc.predict(X_test)\ny_pred_npc = dt_npc.predict(X_test_npc)\nimport graphviz\nfrom sklearn import tree\ndot_data = tree.export_graphviz(\n    dt_npc, \n    class_names = ['deny','approve'], #labels.unique(), #\n    feature_names = X_test_npc.columns, #the columns\n    out_file=None\n)\ngraph = graphviz.Source(dot_data)\ngraph\n#display summarized classification results\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nresults.loc[len(results)] = {\n    'Model':'Decision Tree',\n    'Data':'With Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'ROC-AUC':roc_auc_score(y_test,y_pred)\n}\nresults.loc[len(results)] = {\n    'Model':'Decision Tree',\n    'Data':'Without Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred_npc),\n    'Precision':precision_score(y_test,y_pred_npc),\n    'Recall':recall_score(y_test,y_pred_npc),\n    'F1':f1_score(y_test,y_pred_npc),\n    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n}\n\ndisplay(results)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nDecision Tree\nWith Protected Classes\n0.909013\n0.911677\n0.989477\n0.948985\n0.711476\n\n\n1\nDecision Tree\nWithout Protected Classes\n0.909013\n0.911677\n0.989477\n0.948985\n0.711476\n#display summarized classification results - confusion matrices\n\nimport matplotlib.pyplot as plt\nfig,axes=plt.subplots(nrows=1,ncols=2)\n\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred,y_true=y_test\n    ),\n        display_labels=['Deny','Approve']\n).plot(ax=axes[0])\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred_npc,y_true=y_test\n    ),\n        display_labels=['Deny','Approve']\n).plot(ax=axes[1])\naxes[0].set_title('With Protected\\nClasses')\naxes[1].set_title('Without Protected\\nClasses')\nplt.suptitle(\"Confusion Matrices - Decision Trees\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Decision Tree Code</span>"
    ]
  },
  {
    "objectID": "app5.html#label-encoded-data",
    "href": "app5.html#label-encoded-data",
    "title": "Appendix E — Decision Tree Code",
    "section": "",
    "text": "E.1.1 Dropping debt to income ratio\n\nimport graphviz\nfrom sklearn import tree\ndot_data = tree.export_graphviz(\n    dt_npc, \n    class_names = ['deny','approve'], #labels.unique(), #\n    feature_names = X_test_npc.columns, #the columns\n    out_file=None\n)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\n\n\n\n\n\n'imgs\\\\dt1npc.png'\n\n\n\n\n\n\n\n\n\n\n\n\n\n'imgs\\\\dt1pc.png'\n\n\n\n#display summarized classification results\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nresults.loc[len(results)] = {\n    'Model':'Decision Tree',\n    'Data':'With Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'ROC-AUC':roc_auc_score(y_test,y_pred)\n}\nresults.loc[len(results)] = {\n    'Model':'Decision Tree',\n    'Data':'Without Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred_npc),\n    'Precision':precision_score(y_test,y_pred_npc),\n    'Recall':recall_score(y_test,y_pred_npc),\n    'F1':f1_score(y_test,y_pred_npc),\n    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n}\n\ndisplay(results)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nDecision Tree\nWith Protected Classes\n0.889389\n0.897131\n0.983439\n0.938304\n0.658499\n\n\n1\nDecision Tree\nWithout Protected Classes\n0.889389\n0.897131\n0.983439\n0.938304\n0.658499\n\n\n\n\n\n\n\n\n#display summarized classification results - confusion matrices\n\nimport matplotlib.pyplot as plt\nfig,axes=plt.subplots(nrows=1,ncols=2)\n\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred,y_true=y_test\n    ),\n        display_labels=['Deny','Approve']\n).plot(ax=axes[0])\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred_npc,y_true=y_test\n    ),\n        display_labels=['Deny','Approve']\n).plot(ax=axes[1])\naxes[0].set_title('With Protected\\nClasses')\naxes[1].set_title('Without Protected\\nClasses')\nplt.suptitle(\"Confusion Matrices - Decision Trees\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nE.1.2 Dropping automated underwriting system columns…\n\nimport graphviz\nfrom sklearn import tree\ndot_data = tree.export_graphviz(\n    dt_npc, \n    class_names = ['deny','approve'], #labels.unique(), #\n    feature_names = X_test_npc.columns, #the columns\n    out_file=None\n)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#display summarized classification results\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nresults.loc[len(results)] = {\n    'Model':'Decision Tree',\n    'Data':'With Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'ROC-AUC':roc_auc_score(y_test,y_pred)\n}\nresults.loc[len(results)] = {\n    'Model':'Decision Tree',\n    'Data':'Without Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred_npc),\n    'Precision':precision_score(y_test,y_pred_npc),\n    'Recall':recall_score(y_test,y_pred_npc),\n    'F1':f1_score(y_test,y_pred_npc),\n    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n}\n\ndisplay(results)\n\n\n\n\n\n\n\n\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\n0\nDecision Tree\nWith Protected Classes\n0.863544\n0.863488\n0.998275\n0.926002\n0.532782\n\n\n1\nDecision Tree\nWithout Protected Classes\n0.863593\n0.863495\n0.998332\n0.926031\n0.532811\n\n\n\n\n\n\n\n\n#display summarized classification results - confusion matrices\n\nimport matplotlib.pyplot as plt\nfig,axes=plt.subplots(nrows=1,ncols=2)\n\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred,y_true=y_test\n    ),\n        display_labels=['Deny','Approve']\n).plot(ax=axes[0])\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred_npc,y_true=y_test\n    ),\n        display_labels=['Deny','Approve']\n).plot(ax=axes[1])\naxes[0].set_title('With Protected\\nClasses')\naxes[1].set_title('Without Protected\\nClasses')\nplt.suptitle(\"Confusion Matrices - Decision Trees\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport graphviz\nfrom sklearn import tree\ndot_data = tree.export_graphviz(\n    dt_npc, \n    class_names = ['deny','approve'], #labels.unique(), #\n    feature_names = X_test_npc.columns, #the columns\n    out_file=None\n)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\n\n\n\n\ndot_data = tree.export_graphviz(\n    dt_pc, \n    class_names = ['deny','approve'], #labels.unique(), #\n    feature_names = X_test.columns, #the columns\n    out_file=None\n)\ngraph = graphviz.Source(dot_data)\ngraph\n\n\n\n\n\n\n\n\n\n#randomization testing...\nclean = pd.read_csv('../data/cnb_pc.csv')\nclean_npc = pd.read_csv('../data/cnb_npc.csv')\n\nnp.random.seed(5505)\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nfor i in range(500):\n    r = np.random.randint(1,5000,1)[0]\n\n    X_train,X_test,y_train,y_test = train_test_split(\n        clean,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r\n    )\n\n    X_train_npc,X_test_npc,y_train,y_test = train_test_split(\n        clean_npc,\n        labels,\n        stratify=labels,\n        test_size=0.2,\n        random_state=r\n    )\n    dt_pc.fit(X_train,y_train)\n    dt_npc.fit(X_train_npc,y_train)\n    y_pred = dt_pc.predict(X_test)\n    y_pred_npc = dt_npc.predict(X_test_npc)\n    results.loc[len(results)] = {\n        'Model':'Decision Tree (D=4)',\n        'Data':'With Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred),\n        'Precision':precision_score(y_test,y_pred),\n        'Recall':recall_score(y_test,y_pred),\n        'F1':f1_score(y_test,y_pred),\n        'ROC-AUC':roc_auc_score(y_test,y_pred)\n    }\n    results.loc[len(results)] = {\n        'Model':'Decision Tree (D=4)',\n        'Data':'Without Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred_npc),\n        'Precision':precision_score(y_test,y_pred_npc),\n        'Recall':recall_score(y_test,y_pred_npc),\n        'F1':f1_score(y_test,y_pred_npc),\n        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n    }\n\n\nresults.to_csv('../data/dtRandTest.csv',index=False)\n\n\n#check for statistically significant differences in model performance\nfrom statsmodels.stats.weightstats import ztest\ntbl_pc = results[results['Data']=='With Protected Classes'].describe().T.reset_index()\ntbl_npc = results[results['Data']=='Without Protected Classes'].describe().T.reset_index()\n\nfrom scipy import stats\nsig = pd.DataFrame({\n    'Stat':[],\n    'z-score':[],\n    'p-value':[],\n    'top performer':[],\n    'top mean':[],\n    'difference in means':[]\n})\n\nz_stat,p_value = ztest(\n    results.loc[results['Data']=='With Protected Classes']['Accuracy'],\n    results.loc[results['Data']=='Without Protected Classes']['Accuracy'],\n)\nscores = tbl_pc['index'].unique()\nfor score in scores: \n    z_stat,p_value = ztest(\n        results.loc[results['Data']=='With Protected Classes'][score],\n        results.loc[results['Data']=='Without Protected Classes'][score],\n    )\n    mu_pc, mu_npc = (\n        tbl_pc.loc[tbl_pc['index']==score,'mean'].iloc[0],\n        tbl_npc.loc[tbl_npc['index']==score,'mean'].iloc[0]\n    )\n    winner = np.select(\n        [\n            mu_pc &lt; mu_npc,\n            mu_npc &lt; mu_pc\n        ],\n        [\n            'Without Protected Classes',\n            'With Protected Classes'\n        ],\n        default='tie'\n    )\n    diff = np.abs(mu_pc-mu_npc)\n    m = max(mu_pc,mu_npc)\n    sig.loc[len(sig)] = {\n        'Stat':score,\n        'z-score':z_stat,\n        'p-value':p_value,\n        'top performer':winner,\n        'top mean':m,\n        'difference in means':diff\n    }\n\ndisplay(sig.style.hide(axis='index'))\n\n\n\n\n\n\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nAccuracy\n-0.063229\n0.949584\nWithout Protected Classes\n0.909842\n0.000004\n\n\nPrecision\n-0.048038\n0.961686\nWithout Protected Classes\n0.912724\n0.000002\n\n\nRecall\n-0.045748\n0.963511\nWithout Protected Classes\n0.989174\n0.000001\n\n\nF1\n-0.064089\n0.948899\nWithout Protected Classes\n0.949412\n0.000002\n\n\nROC-AUC\n-0.049822\n0.960264\nWithout Protected Classes\n0.715086\n0.000009\n\n\n\n\n\n\n#visualize the output distributions\nres = results.copy()\nres = res.melt(id_vars=['Model','Data'],var_name='Metric',value_name='Score')\ng = sns.FacetGrid(data=res,col='Metric',hue='Data',col_wrap=3)\ng.map_dataframe(\n    sns.kdeplot,\n    x='Score'\n)\ng.add_legend(loc='lower right')\nplt.suptitle(\"Distributions for Randomization of Training/Testing Data\\nDecision Trees\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Decision Tree Code</span>"
    ]
  },
  {
    "objectID": "app7.html",
    "href": "app7.html",
    "title": "Appendix F — Regression Code",
    "section": "",
    "text": "F.1 Logistic Regression\nData was prepared in Appendix G.\nPerforming train-test split of 80/20, stratified on the outcome “approve/deny”.\n#build the training and test data\nX_train,X_test,y_train,y_test = train_test_split(\n    mca, #mca[mca.columns[:126]],\n    labels,\n    stratify=labels,\n    random_state=8808,\n    test_size=0.2\n)\nX_train_npc,X_test_npc,y_train,y_test = train_test_split(\n    mca_npc, #mca_npc[mca_npc.columns[:86]],\n    labels,\n    stratify=labels,\n    random_state=8808,\n    test_size=0.2\n)\ny_train_copy = y_train.copy()\n# X_train,y_train = # SMOTE(random_state=8808).fit_resample(X_train,y_train_copy.copy())\n# X_train_npc,y_train = SMOTE(random_state=8808).fit_resample(X_train_npc,y_train_copy.copy())\n\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\n#train the model\nlr = LogisticRegression(max_iter=300)\nlr_npc = LogisticRegression(max_iter=300)\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nresults.loc[len(results)] = {\n    'Model':'Logistic Regression',\n    'Data':'With Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred),\n    'Precision':precision_score(y_test,y_pred),\n    'Recall':recall_score(y_test,y_pred),\n    'F1':f1_score(y_test,y_pred),\n    'ROC-AUC':roc_auc_score(y_test,y_pred)\n}\n#run the model with the test dataset\nlr_npc.fit(X_train_npc,y_train)\ny_pred_npc = lr_npc.predict(X_test_npc)\nresults.loc[len(results)] = {\n    'Model':'Logistic Regression',\n    'Data':'Without Protected Classes',\n    'Accuracy':accuracy_score(y_test,y_pred_npc),\n    'Precision':precision_score(y_test,y_pred_npc),\n    'Recall':recall_score(y_test,y_pred_npc),\n    'F1':f1_score(y_test,y_pred_npc),\n    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n}\n# display_labels=My_BNB_Model.classes_)\n#display summarized classification results\nimport matplotlib.pyplot as plt\nfig,axes=plt.subplots(nrows=1,ncols=2)\n\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred,y_true=y_test\n    ),\n    display_labels=['Deny','Approve']\n).plot(ax=axes[0])\nConfusionMatrixDisplay(\n    confusion_matrix(\n        y_pred=y_pred_npc,y_true=y_test\n    ),\n    display_labels=['Deny','Approve']\n).plot(ax=axes[1])\naxes[0].set_title('With Protected\\nClasses')\naxes[1].set_title('Without Protected\\nClasses')\nplt.suptitle(\"Confusion Matrices - Logistic Regression\")\nplt.tight_layout()\nplt.show()\nModel\nData\nAccuracy\nPrecision\nRecall\nF1\nROC-AUC\n\n\n\n\nLogistic Regression\nWith Protected Classes\n0.961859\n0.983022\n0.972197\n0.977579\n0.936481\n\n\nLogistic Regression\nWithout Protected Classes\n0.958859\n0.981304\n0.970385\n0.975814\n0.930562\n# are the results significantly different? do a randomization test...\n\nresults = pd.DataFrame({\n    'Model':[],\n    'Data':[],\n    'Accuracy':[],\n    'Precision':[],\n    'Recall':[],\n    'F1':[],\n    'ROC-AUC':[]\n})\nnp.random.seed(2036)\nfor i in range(500):\n    print('iteration: {}'.format(i+1))\n    r = np.random.randint(0,5000,1)\n    X_train,X_test,y_train,y_test = train_test_split(\n        mca, #mca[mca.columns[:126]],\n        labels,\n        stratify=labels,\n        random_state=r[0],\n        test_size=0.2\n    )\n    X_train_npc,X_test_npc,y_train,y_test = train_test_split(\n        mca_npc, #mca_npc[mca_npc.columns[:86]],\n        labels,\n        stratify=labels,\n        random_state=r[0],\n        test_size=0.2\n    )\n    y_train_copy = y_train.copy()\n    # X_train,y_train = SMOTE(random_state=8808).fit_resample(X_train,y_train_copy.copy())\n    # X_train_npc,y_train = SMOTE(random_state=8808).fit_resample(X_train_npc,y_train_copy.copy())\n# lr = LogisticRegression(max_iter=300)\n# lr_npc = LogisticRegression(max_iter=300)\n    lr.fit(X_train,y_train)\n    lr_npc.fit(X_train_npc,y_train)\n    y_pred = lr.predict(X_test)\n    y_pred_npc = lr_npc.predict(X_test_npc)\n    results.loc[len(results)] = {\n        'Model':'Logistic Regression',\n        'Data':'With Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred),\n        'Precision':precision_score(y_test,y_pred),\n        'Recall':recall_score(y_test,y_pred),\n        'F1':f1_score(y_test,y_pred),\n        'ROC-AUC':roc_auc_score(y_test,y_pred)\n    }\n    results.loc[len(results)] = {\n        'Model':'Logistic Regression',\n        'Data':'Without Protected Classes',\n        'Accuracy':accuracy_score(y_test,y_pred_npc),\n        'Precision':precision_score(y_test,y_pred_npc),\n        'Recall':recall_score(y_test,y_pred_npc),\n        'F1':f1_score(y_test,y_pred_npc),\n        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)\n    }\nresults.to_csv('../data/logRegRandTest.csv',index=False)\n#visualize the output distributions\nres = results.copy()\nres = res.melt(id_vars=['Model','Data'],var_name='Metric',value_name='Score')\ng = sns.FacetGrid(data=res,col='Metric',hue='Data',col_wrap=3)\ng.map_dataframe(\n    sns.kdeplot,\n    x='Score'#,\n    # hue='Data'\n)\ng.add_legend(loc='lower right')\nplt.suptitle(\"Distributions for Randomization of Training/Testing Data\\nLogistic Regression\")\nplt.tight_layout()\nplt.show()\nStat\nz-score\np-value\ntop performer\ntop mean\ndifference in means\n\n\n\n\nAccuracy\n44.648798\n0.000000\nWith Protected Classes\n0.963586\n0.002457\n\n\nPrecision\n47.173034\n0.000000\nWith Protected Classes\n0.983867\n0.001985\n\n\nRecall\n16.219786\n0.000000\nWith Protected Classes\n0.973386\n0.000889\n\n\nF1\n43.977874\n0.000000\nWith Protected Classes\n0.978598\n0.001431\n\n\nROC-AUC\n50.590439\n0.000000\nWith Protected Classes\n0.939526\n0.006305",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Regression Code</span>"
    ]
  },
  {
    "objectID": "app7.html#multinomial-naive-bayes",
    "href": "app7.html#multinomial-naive-bayes",
    "title": "Appendix F — Regression Code",
    "section": "F.2 Multinomial Naive Bayes",
    "text": "F.2 Multinomial Naive Bayes\nMultinomial Naive Bayes code and data processing can be found in Appendix D",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Regression Code</span>"
    ]
  },
  {
    "objectID": "app8-mca.html",
    "href": "app8-mca.html",
    "title": "Appendix G — Multiple Correspondence Analysis",
    "section": "",
    "text": "G.1 Data Transformations\nFirst, to perform MCA, variables require separation, and transformation to categorical variables. Below, the binary encoded fields for applicant race, ethnicity, and underwriting system, are broken back out into corresponding binary True/False columns for each category.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Multiple Correspondence Analysis</span>"
    ]
  },
  {
    "objectID": "app8-mca.html#data-transformations",
    "href": "app8-mca.html#data-transformations",
    "title": "Appendix G — Multiple Correspondence Analysis",
    "section": "",
    "text": "G.1.1 Breakout Race, Ethnicity, and Underwriting System Columns\n\n##for use in splitting the collapsed columns \n##back out into their respective binary values\nmapper = {\n    'applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    'co-applicant_race':{\n        'American Indian/Alaska Native':0b0000000000000000001,\n        'Asian':0b0000000000000000010,\n        'Asian Indian':0b0000000000000000100,\n        'Chinese':0b0000000000000001000,\n        'Filipino':0b0000000000000010000,\n        'Japanese':0b0000000000000100000,\n        'Korean':0b0000000000001000000,\n        'Vietnamese':0b0000000000010000000,\n        'Other Asian':0b0000000000100000000,\n        'Black/African American':0b0000000001000000000,\n        'Native Hawaiian/Pacific Islander':0b0000000010000000000,\n        'Native Hawaiian':0b0000000100000000000,\n        'Guamanian/Chamorro':0b0000001000000000000,\n        'Samoan':0b0000010000000000000,\n        'Other Pacific Islander':0b0000100000000000000,\n        'White':0b0001000000000000000,\n        'Information not provided':0b0010000000000000000,\n        'Not Applicable':0b0100000000000000000,\n        'No Co-applicant':0b1000000000000000000\n    },\n    'applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    'co-applicant_ethnicity':{\n        'Hispanic/Latino':0b000000001,\n        'Mexican':0b000000010,\n        'Puerto Rican':0b000000100,\n        'Cuban':0b000001000,\n        'Other Hispanic/Latino':0b000010000,\n        'Not Hispanic/Latino':0b000100000,\n        'Information Not Provided':0b001000000,\n        'Not Applicable':0b010000000,\n        'No Co-applicant':0b100000000\n    },\n    'aus':{\n        'Desktop Underwriter':0b00000001,\n        'Loan Prospector/Product Advisor':0b00000010,\n        'TOTAL Scorecard':0b00000100,\n        'GUS':0b00001000,\n        'Other':0b00010000,\n        'Internal Proprietary':0b00100000,\n        'Not applicable':0b01000000,\n        'Exempt':0b10000000,\n    }, \n}\n\nnew_mapper = {}\nfor k,v in mapper.items():\n    new_mapper[k] = {}\n    #print(k)\n    for j,w in v.items():\n        #print(w,j)\n        new_mapper[k][w] = j\n\nNext, the numeric variables require transformation to categorical. In this case, the standard deviation was leveraged to produce the following categories:\n\nValue &lt; Mean - 2*standard deviation =&gt; L (low)\nMean - 2 * standard deviation &lt; Value &lt; Mean - standard deviation =&gt; ML (Mid-Low)\nMean - standard deviation &lt; Value &lt; Mean + standard deviation =&gt; M\nMean + standard deviation &lt; Value &lt; Mean + 2 * standard deviation =&gt; MH (Mid-High)\nValue &gt; Mean + 2 * standard deviation =&gt; H (High)\n\nThis categorization allowed for diversity in the source data prior to transforming with MCA\n\n#adjust numerics to categoricals\n\n#drop columns that will not be leveraged in MCA\nfr.drop(\n    labels = [\n        'balloon_payment', \n        'interest_only_payment', \n        'other_nonamortizing_features',\n        'income_from_median',\n        'state_code',\n        'county_code'\n    ],\n    axis=1,inplace=True\n)\n\n#identify numeric columns to convert to categorical\nnumerics = [\n    'income',\n    'loan_amount',\n    'interest_rate',\n    'total_loan_costs',\n    'origination_charges',\n    'discount_points',\n    'lender_credits',\n    'loan_term',\n    'intro_rate_period',\n    'property_value',\n    'total_units',\n    'tract_population',\n    'tract_minority_population_percent',\n    'ffiec_msa_md_median_family_income',\n    'tract_to_msa_income_percentage',\n    'tract_owner_occupied_units',\n    'tract_one_to_four_family_homes',\n    'tract_median_age_of_housing_units',\n    'loan_to_value_ratio'\n]\n\n#set the cutting boundaries\nbounds = [i/5 for i in range(1,5)]\n\nfor col in numerics:\n    #income had some errors, for some reason\n    if col == 'income':\n        fr.loc[fr[col]&lt;=0,col] = 0.01\n        fr[col] = np.log(fr[col])\n\n    s = fr[col].std()\n\n    m = fr[col].mean()\n\n    #cut everything based on standard deviations\n    cut_level = [\n        m-2*s,\n        m-s,\n        m+s,\n        m+2*s\n    ]\n    \n    cut_level = [-np.inf] + cut_level + [np.inf]\n\n    #assign value based on cut boundaries\n    fr[col] = pd.cut(\n        fr[col],\n        bins=cut_level,\n        labels=[\"L\",\"ML\",\"M\",\"MH\",\"H\"]\n    )\n\n    #convert to categorical\n    fr[col] = fr[col].astype('category')\n\nfr[numerics].head(10)\n\n\n\n\n\n\n\n\nincome\nloan_amount\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\nlender_credits\nloan_term\nintro_rate_period\nproperty_value\ntotal_units\ntract_population\ntract_minority_population_percent\nffiec_msa_md_median_family_income\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\nloan_to_value_ratio\n\n\n\n\n0\nM\nMH\nL\nH\nH\nM\nH\nM\nH\nM\nM\nM\nM\nM\nM\nMH\nMH\nM\nM\n\n\n1\nL\nMH\nL\nM\nM\nM\nM\nM\nH\nH\nM\nM\nML\nM\nH\nM\nL\nML\nL\n\n\n2\nMH\nH\nML\nM\nM\nM\nM\nM\nH\nH\nM\nH\nM\nM\nH\nM\nL\nML\nM\n\n\n3\nM\nMH\nML\nM\nM\nM\nM\nM\nH\nM\nM\nH\nM\nM\nH\nH\nH\nML\nM\n\n\n4\nM\nH\nML\nM\nM\nM\nM\nM\nH\nH\nM\nM\nM\nH\nMH\nM\nM\nMH\nM\n\n\n5\nMH\nMH\nM\nMH\nM\nM\nM\nM\nM\nMH\nM\nM\nML\nM\nM\nM\nM\nM\nM\n\n\n6\nMH\nH\nM\nM\nM\nM\nM\nM\nMH\nH\nM\nM\nMH\nM\nM\nM\nM\nM\nM\n\n\n7\nMH\nM\nML\nM\nM\nM\nM\nM\nM\nH\nM\nM\nML\nM\nH\nMH\nML\nMH\nL\n\n\n8\nM\nH\nML\nM\nM\nM\nM\nM\nH\nH\nM\nH\nM\nM\nMH\nMH\nML\nH\nM\n\n\n9\nM\nM\nML\nM\nM\nM\nM\nM\nH\nM\nM\nM\nML\nM\nMH\nMH\nM\nML\nML\n\n\n\n\n\n\n\nAfter transforming numerics, the one-hot encoded columns extracted from race, ethnicity, and underwriting system required separation from the rest of the data so that all remaining categorical columns could be converted to a one-hot encoding.\nBelow outlines the separation of the race, ethnicity, and underwriting columns.\n\n#extract binary columns\n## because they're already one-hot encoded\nfr_bin = fr[[\n    'applicant_race',\n    'applicant_ethnicity',\n    'co-applicant_race',\n    'co-applicant_ethnicity',\n    'aus'\n]].copy()\n\nfor k,v in new_mapper.items():\n    for l,w in v.items():\n        fr_bin[k+'_'+w] = (fr_bin[k]&l &gt; 0).astype(int)\n\nfr_bin.drop(\n    labels=[    \n        'applicant_race',\n        'applicant_ethnicity',\n        'co-applicant_race',\n        'co-applicant_ethnicity',\n        'aus'\n    ],\n    inplace=True,\n    axis=1\n)\n\nfr.drop(\n    labels=[\n        'applicant_race',\n        'applicant_ethnicity',\n        'co-applicant_race',\n        'co-applicant_ethnicity',\n        'denial_reason',\n        'aus',\n        'outcome',\n        'action_taken'\n    ],\n    inplace=True,\n    axis=1\n)\ndisplay(\n    fr.head(10),\n    fr_bin.head(10)\n)\n\n\n\n\n\n\n\n\nderived_sex\npurchaser_type\npreapproval\nopen-end_line_of_credit\nloan_amount\nloan_to_value_ratio\ninterest_rate\ntotal_loan_costs\norigination_charges\ndiscount_points\n...\napplicant_age\nco-applicant_age\ntract_population\ntract_minority_population_percent\nffiec_msa_md_median_family_income\ntract_to_msa_income_percentage\ntract_owner_occupied_units\ntract_one_to_four_family_homes\ntract_median_age_of_housing_units\ncompany\n\n\n\n\n0\nSex Not Available\n0\n2\n2\nMH\nM\nL\nH\nH\nM\n...\n3.0\n7.0\nM\nM\nM\nM\nMH\nMH\nM\nJP Morgan\n\n\n1\nMale\n0\n2\n2\nMH\nL\nL\nM\nM\nM\n...\n1.0\n8.0\nM\nML\nM\nH\nM\nL\nML\nJP Morgan\n\n\n2\nSex Not Available\n0\n1\n2\nH\nM\nML\nM\nM\nM\n...\n3.0\n8.0\nH\nM\nM\nH\nM\nL\nML\nJP Morgan\n\n\n3\nMale\n0\n2\n2\nMH\nM\nML\nM\nM\nM\n...\n4.0\n8.0\nH\nM\nM\nH\nH\nH\nML\nJP Morgan\n\n\n4\nJoint\n0\n2\n2\nH\nM\nML\nM\nM\nM\n...\n1.0\n1.0\nM\nM\nH\nMH\nM\nM\nMH\nJP Morgan\n\n\n5\nJoint\n0\n1\n2\nMH\nM\nM\nMH\nM\nM\n...\n2.0\n3.0\nM\nML\nM\nM\nM\nM\nM\nJP Morgan\n\n\n6\nJoint\n0\n2\n2\nH\nM\nM\nM\nM\nM\n...\n2.0\n1.0\nM\nMH\nM\nM\nM\nM\nM\nJP Morgan\n\n\n7\nSex Not Available\n0\n1\n2\nM\nL\nML\nM\nM\nM\n...\n1.0\n1.0\nM\nML\nM\nH\nMH\nML\nMH\nJP Morgan\n\n\n8\nJoint\n0\n1\n2\nH\nM\nML\nM\nM\nM\n...\n1.0\n1.0\nH\nM\nM\nMH\nMH\nML\nH\nJP Morgan\n\n\n9\nSex Not Available\n0\n1\n2\nM\nML\nML\nM\nM\nM\n...\n1.0\n1.0\nM\nML\nM\nMH\nMH\nM\nML\nJP Morgan\n\n\n\n\n10 rows × 37 columns\n\n\n\n\n\n\n\n\n\n\napplicant_race_American Indian/Alaska Native\napplicant_race_Asian\napplicant_race_Asian Indian\napplicant_race_Chinese\napplicant_race_Filipino\napplicant_race_Japanese\napplicant_race_Korean\napplicant_race_Vietnamese\napplicant_race_Other Asian\napplicant_race_Black/African American\n...\nco-applicant_ethnicity_Not Applicable\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n6\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n10 rows × 64 columns\n\n\n\nThe below cell takes the remaining non-binary encoded data and performs one-hot encoding.\n\n# perform one-hot encoding of remaining columns\nohe = OneHotEncoder()\nout = ohe.fit_transform(fr)\noutdf = pd.DataFrame(out.toarray(),columns=ohe.get_feature_names_out().tolist())\n\n#prepare a copy of the dataframe\noutdf_nr = outdf.copy()\n\n#transfer columns over from the already one-hot encoded dataframe\nfor col in fr_bin.columns:\n    outdf[col] = fr_bin[col].copy()\n\n#convert all columns to integers\nfor col in outdf.columns:\n    outdf[col] = outdf[col].astype(int)\n\n#display the output\ndisplay(outdf.head())\n\n\n\n\n\n\n\n\nderived_sex_Female\nderived_sex_Joint\nderived_sex_Male\nderived_sex_Sex Not Available\npurchaser_type_0\npurchaser_type_1\npurchaser_type_3\npurchaser_type_5\npurchaser_type_6\npurchaser_type_9\n...\nco-applicant_ethnicity_Not Applicable\nco-applicant_ethnicity_No Co-applicant\naus_Desktop Underwriter\naus_Loan Prospector/Product Advisor\naus_TOTAL Scorecard\naus_GUS\naus_Other\naus_Internal Proprietary\naus_Not applicable\naus_Exempt\n\n\n\n\n0\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n1\n0\n1\n0\n0\n0\n0\n0\n...\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n5 rows × 243 columns\n\n\n\n\n\nG.1.2 Check for Buggy Columns\nMCA requires a one-hot encoded vector to have at least 1 zero and at least 1 one per column. The below checks for any columns that did not meet this requirement, and any such columns are exlcued from the MCA.\n\n#check for any columns that only have one value/result\nfor col in outdf.columns:\n    x = list(outdf[col].unique())\n    x.sort()\n    if x != [0,1]:\n        print(col)\n\napplicant_race_No Co-applicant\napplicant_ethnicity_No Co-applicant\naus_GUS\naus_Exempt\n\n\n\n\nG.1.3 MCA With Protected Classes\n\nncomp=181\nmcaNd = MCA(n_components=ncomp,one_hot=False)\n\n#exclude columns that had no variability (only a single value) \n#and fit a multiple correspondence analysis\nxformNd = mcaNd.fit_transform(\n    outdf.drop(\n        labels=[\n            'applicant_race_No Co-applicant',\n            'applicant_ethnicity_No Co-applicant',\n            'aus_GUS',\n            'aus_Exempt'\n        ],axis=1\n    )\n)\nxformNd.columns = ['MC{}'.format(i+1) for i in range(len(xformNd.columns))]\n\n\n#head of the MCA dataframe\nxformNd.head(10)\n\n\n\n\n\n\n\n\nMC1\nMC2\nMC3\nMC4\nMC5\nMC6\nMC7\nMC8\nMC9\nMC10\n...\nMC172\nMC173\nMC174\nMC175\nMC176\nMC177\nMC178\nMC179\nMC180\nMC181\n\n\n\n\n0\n0.451521\n6.128471\n61.481532\n-0.582675\n0.142174\n-0.446411\n-0.155301\n0.005913\n0.387730\n-0.210268\n...\n-0.038055\n-0.012424\n-0.000177\n-0.001205\n0.009249\n-0.005225\n-0.003449\n0.001742\n0.003897\n-0.004720\n\n\n1\n-0.319092\n0.052840\n0.031634\n0.207948\n-0.063021\n0.541456\n0.596652\n-0.107125\n0.091412\n0.025145\n...\n0.046762\n-0.086929\n-0.002160\n0.006824\n-0.084655\n-0.006127\n-0.002646\n0.012392\n0.002616\n0.000280\n\n\n2\n-0.233362\n0.054331\n0.034305\n-0.336069\n0.122278\n0.455142\n1.004323\n-0.075047\n0.135570\n-0.089234\n...\n0.028105\n-0.080263\n-0.000241\n0.011704\n-0.138774\n-0.004096\n0.011195\n0.001980\n-0.000695\n0.002452\n\n\n3\n-0.302904\n0.025349\n0.025218\n0.236800\n-0.090089\n0.174363\n0.410053\n-0.525227\n0.699016\n-0.263994\n...\n0.046171\n-0.085637\n0.000008\n0.006094\n-0.074695\n-0.002369\n0.002111\n0.009786\n-0.001861\n0.001232\n\n\n4\n0.634415\n0.012939\n0.028819\n0.400410\n-0.163398\n0.245667\n0.736226\n0.056253\n-0.248964\n0.088020\n...\n0.026770\n-0.069725\n0.000374\n0.009017\n-0.129854\n-0.002576\n0.012426\n0.013691\n0.002016\n0.000075\n\n\n5\n0.684141\n0.009116\n0.007519\n0.267646\n-0.094813\n0.307864\n0.136132\n-0.002112\n0.192184\n-0.031398\n...\n0.212160\n0.006548\n-0.001180\n0.011061\n-0.075641\n0.006081\n0.001703\n0.013096\n0.002621\n0.002301\n\n\n6\n0.649954\n0.004748\n0.014900\n0.356419\n-0.135796\n0.275216\n0.713745\n0.010690\n-0.266578\n-0.003736\n...\n0.032658\n-0.058016\n0.001301\n0.007645\n-0.091264\n0.001304\n-0.112194\n-0.005152\n-0.045460\n-0.004703\n\n\n7\n0.569649\n0.039957\n0.014486\n-0.852346\n0.334152\n0.879204\n0.562587\n0.009737\n-0.127637\n0.000647\n...\n0.059040\n-0.090289\n-0.001719\n0.013249\n-0.097626\n0.008894\n0.001857\n0.000997\n0.003241\n0.003394\n\n\n8\n0.643866\n0.013070\n0.028519\n0.316946\n-0.121840\n0.209269\n0.495583\n-0.080518\n0.103247\n-0.041161\n...\n0.031299\n-0.071656\n0.000867\n0.009461\n-0.135678\n-0.001970\n0.008649\n-0.007681\n0.002351\n0.002077\n\n\n9\n0.465507\n0.023988\n0.021353\n-1.000502\n0.358704\n0.639682\n0.285072\n-0.181713\n0.044954\n0.006286\n...\n0.066260\n-0.093161\n0.000555\n0.006261\n-0.064012\n-0.003694\n0.002558\n0.000544\n-0.000423\n0.001978\n\n\n\n\n10 rows × 181 columns\n\n\n\n\n#summary of eigenvalues\ndisplay(mcaNd.eigenvalues_summary)\n\n\n\n\n\n\n\n\neigenvalue\n% of variance\n% of variance (cumulative)\n\n\ncomponent\n\n\n\n\n\n\n\n0\n0.215\n4.69%\n4.69%\n\n\n1\n0.167\n3.63%\n8.32%\n\n\n2\n0.166\n3.62%\n11.93%\n\n\n3\n0.115\n2.49%\n14.43%\n\n\n4\n0.102\n2.21%\n16.64%\n\n\n...\n...\n...\n...\n\n\n176\n0.001\n0.02%\n99.96%\n\n\n177\n0.001\n0.01%\n99.97%\n\n\n178\n0.000\n0.01%\n99.98%\n\n\n179\n0.000\n0.01%\n99.99%\n\n\n180\n0.000\n0.01%\n100.00%\n\n\n\n\n181 rows × 3 columns\n\n\n\n\n#summary of column contributions to each component, sorted by the first component (e.g)\n#max eigenvalue's top 10 contributors\ndisplay(mcaNd.column_contributions_.sort_values(by=0,ascending=False).head(10))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n\n\n\n\nco-applicant_sex_observed_2\n0.061418\n0.000009\n9.547786e-06\n0.000537\n0.000206\n0.000370\n0.001228\n7.078606e-05\n0.000358\n0.000027\n...\n9.090788e-06\n0.000057\n9.194148e-04\n2.548507e-04\n0.000192\n2.767818e-06\n0.003954\n0.000062\n0.002840\n0.005715\n\n\nco-applicant_ethnicity_observed_2\n0.061381\n0.000009\n9.545378e-06\n0.000564\n0.000261\n0.000386\n0.001226\n7.261382e-05\n0.000358\n0.000026\n...\n1.594640e-06\n0.000037\n6.545288e-06\n1.604834e-03\n0.000099\n2.790837e-06\n0.003903\n0.000058\n0.002822\n0.005734\n\n\nco-applicant_race_observed_2\n0.061369\n0.000009\n9.542219e-06\n0.000572\n0.000275\n0.000386\n0.001215\n6.994850e-05\n0.000360\n0.000026\n...\n3.701321e-06\n0.000033\n1.013418e-03\n4.079012e-04\n0.000224\n3.182675e-06\n0.003976\n0.000059\n0.002853\n0.005723\n\n\nderived_sex_Joint\n0.053307\n0.000027\n5.610064e-06\n0.008021\n0.000851\n0.000131\n0.002191\n5.000923e-05\n0.000005\n0.000049\n...\n1.003506e-03\n0.000284\n9.737514e-05\n1.936380e-05\n0.000046\n1.287449e-07\n0.000169\n0.003894\n0.000058\n0.000248\n\n\nco-applicant_credit_score_type_9\n0.047954\n0.000029\n2.215064e-06\n0.003363\n0.000227\n0.004860\n0.002476\n8.910958e-07\n0.001545\n0.015377\n...\n9.534944e-06\n0.000128\n1.583074e-08\n1.048725e-08\n0.000330\n1.582662e-04\n0.003368\n0.000068\n0.003135\n0.006879\n\n\nco-applicant_ethnicity_Not Hispanic/Latino\n0.046580\n0.000019\n4.598133e-06\n0.012423\n0.001830\n0.000412\n0.000204\n9.886380e-03\n0.002706\n0.000063\n...\n5.756610e-07\n0.000037\n3.882840e-06\n6.085041e-06\n0.000258\n6.342339e-06\n0.004570\n0.000095\n0.009585\n0.204413\n\n\nco-applicant_ethnicity_observed_4\n0.045165\n0.000006\n3.893077e-07\n0.000104\n0.000036\n0.000117\n0.000839\n7.990930e-05\n0.000380\n0.000045\n...\n8.054315e-05\n0.000096\n1.192476e-06\n1.178042e-06\n0.000159\n1.314536e-06\n0.000200\n0.000037\n0.002541\n0.000430\n\n\nco-applicant_age_8.0\n0.045165\n0.000006\n3.893077e-07\n0.000104\n0.000036\n0.000117\n0.000839\n7.990930e-05\n0.000380\n0.000045\n...\n8.054315e-05\n0.000096\n1.192476e-06\n1.178042e-06\n0.000159\n1.314536e-06\n0.000200\n0.000037\n0.002541\n0.000430\n\n\nco-applicant_race_observed_4\n0.045165\n0.000006\n3.893077e-07\n0.000104\n0.000036\n0.000117\n0.000839\n7.990930e-05\n0.000380\n0.000045\n...\n8.054315e-05\n0.000096\n1.192476e-06\n1.178042e-06\n0.000159\n1.314536e-06\n0.000200\n0.000037\n0.002541\n0.000430\n\n\nco-applicant_sex_observed_4\n0.045165\n0.000006\n3.893077e-07\n0.000104\n0.000036\n0.000117\n0.000839\n7.990930e-05\n0.000380\n0.000045\n...\n8.054315e-05\n0.000096\n1.192476e-06\n1.178042e-06\n0.000159\n1.314536e-06\n0.000200\n0.000037\n0.002541\n0.000430\n\n\n\n\n10 rows × 181 columns\n\n\n\n\n\nG.1.4 MCA without Protected Classes\n\n#perform an MCA, excluding any information on:\n#age, gender, or race\n##need 99 components to get 100% of variance\n##may need these two versions to do full compare\n# ncomp = 90\nncomp=100\nmcaNd_nr = MCA(n_components=ncomp,one_hot=False)\nxformNd_nr = mcaNd_nr.fit_transform(outdf_nr.drop(\n    labels=[\n        'derived_sex_Female',\n        'derived_sex_Joint',\n        'derived_sex_Male',\n        'derived_sex_Sex Not Available',\n        'applicant_ethnicity_observed_1',\n        'applicant_ethnicity_observed_2',\n        'applicant_ethnicity_observed_3',\n        'co-applicant_ethnicity_observed_1',\n        'co-applicant_ethnicity_observed_2',\n        'co-applicant_ethnicity_observed_3',\n        'co-applicant_ethnicity_observed_4',\n        'applicant_race_observed_1',\n        'applicant_race_observed_2',\n        'applicant_race_observed_3',\n        'co-applicant_race_observed_1',\n        'co-applicant_race_observed_2',\n        'co-applicant_race_observed_3',\n        'co-applicant_race_observed_4',\n        'applicant_sex_1',\n        'applicant_sex_2',\n        'applicant_sex_3',\n        'applicant_sex_4',\n        'applicant_sex_6',\n        'co-applicant_sex_1',\n        'co-applicant_sex_2',\n        'co-applicant_sex_3',\n        'co-applicant_sex_4',\n        'co-applicant_sex_5',\n        'co-applicant_sex_6',\n        'applicant_sex_observed_1',\n        'applicant_sex_observed_2',\n        'applicant_sex_observed_3',\n        'co-applicant_sex_observed_1',\n        'co-applicant_sex_observed_2',\n        'co-applicant_sex_observed_3',\n        'co-applicant_sex_observed_4',\n        'applicant_age_0.0',\n        'applicant_age_1.0',\n        'applicant_age_2.0',\n        'applicant_age_3.0',\n        'applicant_age_4.0',\n        'applicant_age_5.0',\n        'applicant_age_6.0',\n        'applicant_age_7.0',\n        'co-applicant_age_0.0',\n        'co-applicant_age_1.0',\n        'co-applicant_age_2.0',\n        'co-applicant_age_3.0',\n        'co-applicant_age_4.0',\n        'co-applicant_age_5.0',\n        'co-applicant_age_6.0',\n        'co-applicant_age_7.0',\n        'co-applicant_age_8.0'\n    ], \n    axis=1\n))\nxformNd_nr.columns = ['MC{}'.format(i+1) for i in range(len(xformNd_nr.columns))]\n\n\n#head of the MCA dataframe\nxformNd_nr.head(10)\n\n\n\n\n\n\n\n\nMC1\nMC2\nMC3\nMC4\nMC5\nMC6\nMC7\nMC8\nMC9\nMC10\n...\nMC91\nMC92\nMC93\nMC94\nMC95\nMC96\nMC97\nMC98\nMC99\nMC100\n\n\n\n\n0\n-0.764720\n0.723758\n0.125389\n-0.056585\n0.170870\n0.548785\n-0.424036\n-0.241991\n0.097430\n0.513754\n...\n-0.132230\n-0.380647\n-0.171881\n0.643932\n-0.047217\n-0.006711\n-0.111764\n0.019441\n-0.075462\n-0.008763\n\n\n1\n0.140150\n1.022436\n0.076617\n-0.388087\n0.022334\n-0.423135\n-0.292322\n0.252731\n-0.214779\n0.088909\n...\n0.209340\n-0.153709\n0.088645\n-0.070257\n-0.278045\n0.102762\n-0.002714\n-0.035754\n-0.016209\n-0.007529\n\n\n2\n-0.382006\n1.207435\n0.016253\n-0.675044\n0.254554\n-0.804334\n-0.305620\n0.294332\n-0.503778\n-0.252445\n...\n0.530322\n0.212954\n-0.084617\n0.070987\n0.042070\n-0.008835\n-0.001938\n0.005713\n-0.031815\n-0.006028\n\n\n3\n-0.322021\n0.627399\n-1.025139\n-0.423863\n1.221655\n-0.311233\n-0.274495\n0.188478\n-0.149330\n-0.948647\n...\n-0.146744\n-0.033300\n-0.009274\n0.073137\n0.166896\n-0.090431\n-0.140857\n0.015810\n-0.021260\n-0.004191\n\n\n4\n-0.222096\n0.986399\n0.342411\n-0.517554\n-0.440269\n-0.432887\n-0.167812\n0.169848\n-0.530523\n-0.208360\n...\n-0.061780\n-0.004806\n0.018112\n-0.017873\n0.077980\n-0.052323\n0.012778\n-0.014635\n-0.026638\n-0.004269\n\n\n5\n-0.221344\n0.601721\n0.008450\n-0.185320\n-0.387269\n0.006025\n0.150320\n0.375523\n0.461518\n-0.034283\n...\n0.022191\n-0.134455\n-0.361678\n0.100214\n-0.095434\n0.000053\n0.081231\n0.031581\n0.214385\n0.007798\n\n\n6\n-0.042042\n0.825237\n0.223653\n-0.488704\n-0.353445\n-0.016609\n-0.012096\n0.187879\n-0.816343\n-0.194093\n...\n-0.090645\n-0.056575\n0.058021\n-0.051419\n0.022802\n0.039888\n-0.002228\n-0.028715\n-0.030764\n-0.000873\n\n\n7\n0.072632\n1.034400\n0.031038\n-0.342971\n-0.175534\n-0.413475\n-0.225371\n0.303189\n0.460221\n0.414513\n...\n0.038167\n-0.069464\n0.040725\n-0.028568\n-0.415168\n0.299228\n0.182213\n-0.011537\n-0.010310\n0.011517\n\n\n8\n-0.307864\n0.906534\n-0.026393\n-0.526053\n0.337134\n-0.601458\n-0.253877\n0.238076\n-0.385210\n0.078999\n...\n0.605544\n0.285794\n-0.096174\n0.047887\n0.039686\n0.239895\n-0.010113\n0.011185\n-0.025836\n-0.002878\n\n\n9\n-0.033257\n0.346177\n-0.359762\n-0.135270\n-0.053814\n-0.052379\n-0.025562\n-0.059310\n-0.197474\n0.223686\n...\n-0.163153\n-0.069786\n-0.003615\n0.146050\n0.032843\n0.194863\n-0.094570\n0.014722\n-0.002103\n-0.003909\n\n\n\n\n10 rows × 100 columns\n\n\n\n\n#summary of eigenvalues\ndisplay(mcaNd_nr.eigenvalues_summary)\n\n\n\n\n\n\n\n\neigenvalue\n% of variance\n% of variance (cumulative)\n\n\ncomponent\n\n\n\n\n\n\n\n0\n0.124\n3.21%\n3.21%\n\n\n1\n0.116\n3.01%\n6.23%\n\n\n2\n0.100\n2.61%\n8.83%\n\n\n3\n0.086\n2.23%\n11.06%\n\n\n4\n0.085\n2.22%\n13.28%\n\n\n...\n...\n...\n...\n\n\n95\n0.009\n0.24%\n99.52%\n\n\n96\n0.007\n0.17%\n99.69%\n\n\n97\n0.007\n0.17%\n99.86%\n\n\n98\n0.004\n0.11%\n99.97%\n\n\n99\n0.001\n0.03%\n100.00%\n\n\n\n\n100 rows × 3 columns\n\n\n\n\n#summary of column contributions to each component, sorted by the first component (e.g)\n#max eigenvalue's top contributors\ndisplay(mcaNd_nr.column_contributions_.sort_values(by=0,ascending=False).head(20))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\norigination_charges_H\n0.088933\n8.942172e-03\n3.623890e-02\n0.034505\n0.012401\n0.097634\n0.045595\n0.008395\n0.000913\n9.811357e-05\n...\n0.000608\n3.762587e-02\n0.113043\n0.012385\n0.002687\n6.808054e-05\n7.799676e-04\n1.020531e-04\n4.273904e-01\n3.882523e-07\n\n\ntotal_loan_costs_H\n0.084998\n1.035486e-02\n3.586364e-02\n0.029490\n0.011701\n0.083204\n0.043297\n0.006285\n0.000281\n7.044364e-05\n...\n0.000008\n9.952770e-05\n0.010288\n0.170899\n0.013513\n1.634182e-04\n4.238083e-05\n1.011652e-04\n3.245955e-01\n7.379104e-07\n\n\ndiscount_points_H\n0.076381\n6.579247e-03\n2.680753e-02\n0.031293\n0.012241\n0.078079\n0.024438\n0.004070\n0.007833\n1.578854e-04\n...\n0.001013\n6.219218e-02\n0.087251\n0.344232\n0.025247\n1.838789e-05\n6.597205e-04\n7.203051e-04\n1.090670e-02\n8.576594e-08\n\n\nloan_amount_ML\n0.054607\n1.096792e-02\n1.401917e-02\n0.000428\n0.003975\n0.000037\n0.098195\n0.098392\n0.010839\n4.374160e-04\n...\n0.000399\n7.009303e-05\n0.000026\n0.014265\n0.203888\n5.612735e-04\n1.900939e-01\n2.722371e-02\n1.364451e-05\n4.457975e-06\n\n\ntotal_loan_costs_MH\n0.049097\n2.092212e-03\n3.964357e-04\n0.006480\n0.000034\n0.000908\n0.086860\n0.125954\n0.003481\n7.030098e-05\n...\n0.001394\n5.438767e-02\n0.189695\n0.001752\n0.000194\n7.493222e-08\n1.082877e-07\n6.609995e-06\n6.789266e-02\n2.494463e-07\n\n\norigination_charges_MH\n0.047695\n5.291334e-03\n1.314798e-05\n0.007483\n0.000316\n0.002069\n0.114474\n0.159078\n0.003506\n1.969190e-04\n...\n0.000797\n6.254710e-02\n0.309200\n0.109115\n0.004430\n6.008099e-06\n7.908615e-05\n8.730695e-08\n6.953100e-02\n1.126411e-07\n\n\nproperty_value_ML\n0.043231\n1.311904e-02\n2.361116e-02\n0.000225\n0.006645\n0.001983\n0.079686\n0.111624\n0.002818\n1.176477e-04\n...\n0.003947\n2.806396e-04\n0.000114\n0.008425\n0.217519\n2.815655e-06\n1.540480e-01\n2.014790e-02\n2.489933e-05\n5.612855e-06\n\n\ndiscount_points_MH\n0.033078\n4.344227e-03\n3.090285e-05\n0.006287\n0.000074\n0.002746\n0.087229\n0.116690\n0.007014\n3.781955e-04\n...\n0.000003\n3.379782e-03\n0.045276\n0.146388\n0.009086\n1.432915e-05\n1.318505e-05\n2.382708e-05\n4.700539e-04\n2.749443e-08\n\n\npurchaser_type_0\n0.032864\n5.101933e-02\n5.134128e-05\n0.000131\n0.000346\n0.017773\n0.005854\n0.002768\n0.024542\n2.130675e-05\n...\n0.013549\n2.877823e-04\n0.000093\n0.000437\n0.000206\n1.423601e-06\n5.923642e-04\n1.386355e-03\n9.410552e-07\n2.602806e-07\n\n\nopen-end_line_of_credit_1\n0.030098\n3.497459e-02\n3.147512e-03\n0.270842\n0.000043\n0.016216\n0.002723\n0.000650\n0.000119\n7.713089e-05\n...\n0.000072\n4.264123e-06\n0.000008\n0.000191\n0.000081\n2.761905e-08\n1.606608e-05\n2.476935e-06\n3.188185e-07\n4.925486e-01\n\n\nincome_ML\n0.027202\n2.880571e-03\n5.876898e-03\n0.004245\n0.003249\n0.000997\n0.036600\n0.059010\n0.000014\n1.913531e-05\n...\n0.000243\n3.641664e-06\n0.000100\n0.000925\n0.001264\n1.180952e-04\n1.426697e-04\n2.402477e-05\n3.522036e-07\n4.322885e-06\n\n\napplicant_credit_score_type_8\n0.023624\n2.252189e-02\n1.936850e-03\n0.221591\n0.000008\n0.014466\n0.001494\n0.000439\n0.006472\n1.602758e-04\n...\n0.000325\n2.646222e-05\n0.000011\n0.000186\n0.000020\n7.810669e-07\n4.211464e-05\n5.847733e-04\n4.673938e-07\n3.942036e-01\n\n\ndebt_to_income_ratio_18.0\n0.022513\n1.081502e-02\n1.665984e-03\n0.020357\n0.000969\n0.006126\n0.002172\n0.001373\n0.028440\n4.319469e-05\n...\n0.000344\n3.474722e-07\n0.000018\n0.000296\n0.000127\n2.200983e-06\n3.635712e-06\n3.165596e-04\n5.901178e-07\n1.647005e-06\n\n\nloan_amount_MH\n0.022203\n1.950365e-02\n2.109436e-03\n0.000615\n0.000016\n0.001156\n0.001097\n0.000228\n0.009686\n1.696359e-04\n...\n0.002764\n2.114010e-05\n0.000608\n0.000015\n0.012514\n1.093064e-05\n5.994630e-02\n1.001983e-02\n1.617146e-04\n3.625249e-07\n\n\ncompany_Navy Federal Credit Union\n0.021360\n1.382622e-02\n7.652527e-03\n0.024050\n0.000687\n0.138437\n0.017836\n0.009102\n0.041316\n8.876005e-06\n...\n0.007175\n8.647908e-04\n0.000281\n0.001228\n0.000032\n2.159210e-05\n2.375829e-02\n1.576080e-01\n4.473596e-05\n1.660420e-04\n\n\nco-applicant_credit_score_type_9\n0.020690\n4.557131e-04\n6.755788e-07\n0.000520\n0.003270\n0.016488\n0.001447\n0.002080\n0.007025\n5.846616e-07\n...\n0.000218\n2.176786e-06\n0.000032\n0.000010\n0.000025\n1.267790e-06\n4.024599e-03\n1.622137e-02\n2.032551e-05\n4.518054e-05\n\n\norigination_charges_M\n0.018343\n1.853453e-08\n2.048184e-03\n0.004807\n0.000418\n0.003443\n0.002822\n0.009952\n0.000624\n4.297237e-05\n...\n0.000008\n9.540820e-04\n0.008231\n0.015886\n0.001038\n1.369357e-06\n8.420563e-05\n5.919102e-06\n5.366124e-02\n6.069544e-08\n\n\npurchaser_type_1\n0.015311\n1.264095e-02\n7.400198e-05\n0.000004\n0.002262\n0.002061\n0.001437\n0.001671\n0.000003\n3.912600e-05\n...\n0.005013\n9.121422e-05\n0.000009\n0.000021\n0.000058\n6.294184e-06\n2.411526e-04\n7.236456e-05\n4.126922e-07\n1.541073e-10\n\n\ntotal_loan_costs_M\n0.015282\n1.597282e-04\n3.768925e-03\n0.001067\n0.001801\n0.001352\n0.001175\n0.002676\n0.009979\n2.529182e-05\n...\n0.000004\n4.037010e-03\n0.008549\n0.008052\n0.000817\n3.548314e-06\n7.984920e-07\n6.811171e-05\n3.599548e-02\n1.046963e-08\n\n\nloan_to_value_ratio_MH\n0.014452\n4.061576e-03\n5.026527e-05\n0.003849\n0.002806\n0.085140\n0.012913\n0.006081\n0.017323\n1.591876e-05\n...\n0.000027\n7.783720e-05\n0.000164\n0.000191\n0.000906\n8.208134e-06\n1.281691e-04\n1.841615e-02\n1.551564e-09\n1.157982e-05\n\n\n\n\n20 rows × 100 columns\n\n\n\n\n#forgot to drop from outdf_nr earlier.  fixing...\noutdf_nr.drop(\n    columns=['derived_sex_Female','derived_sex_Joint','derived_sex_Male',\n            'derived_sex_Sex Not Available'],inplace=True,axis=1\n)\n\n\n\nG.1.5 Output the Results to CSV\n\nout = mcaNd.column_contributions_.copy()\nout_nr = mcaNd_nr.column_contributions_.copy()\n\nout.reset_index(inplace=True)\nout_nr.reset_index(inplace=True)\n\nout.columns = ['Column']+[\"MC{}\".format(i+1) for i in range(len(out.columns)-1)]\nout_nr.columns = ['Column']+[\"MC{}\".format(i+1) for i in range(len(out_nr.columns)-1)]\n\nmcaNd.eigenvalues_summary.to_csv('../data/mca-Nd-eig.csv',index=False)\nout.to_csv('../data/mca-Nd-ColCont.csv',index=False)\nmcaNd_nr.eigenvalues_summary.to_csv('../data/mca-Nd-npc-eig.csv',index=False)\nout_nr.to_csv('../data/mca-Nd-npc-ColCont.csv',index=False)\n\noutdf.to_csv('../data/data-one-hot.csv',index=False)\noutdf_nr.to_csv('../data/data-one-hot-npc.csv',index=False)\nxformNd.to_csv('../data/mcaNd.csv',index=False)\nxformNd_nr.to_csv('../data/mcaNd-npc.csv',index=False)\n\n\ntmp = pd.concat([out,labels],axis=1)\n\nsns.scatterplot(\n    data=tmp,\n    x='MC1',y='MC2',\n    hue='outcome'\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Multiple Correspondence Analysis</span>"
    ]
  }
]