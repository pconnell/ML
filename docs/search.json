[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Predictive Power of Protected Classes in Mortgage Approvals",
    "section": "",
    "text": "1 Introduction\nHow strong is the connection between your age, gender, or race and ability to attain a mortgage?\nIn early 2024, US Senator Sherrod Brown, along with colleagues, called for a review of lending practices by Navy Federal Credit Union (NFCU) as a result of a late 2023 CNN report that oulined disparate impact to persons of color seeking to acquire conventional loans for home purchases. The disparity in results between groups appears damning to the organization’s reputation. CNN’s analysis predominantly explored the mathematical differences between racial groups and various lenders, but did not necessarily outline a direct cause-and-effect relationship (e.g. being a person of color seeking a loan at NFCU is guaranteed to decrease your approval chances). Their analysis instead covered the rates of approval for the different groups, and compared those approval rates between groups and against other financial organizations’ approval rates.\n\n\n\nvisual comparison of NFCU to other financial institutions regarding loans to persons of color\n\n\nEach of the organizations in the above graphic (from CNN) appear to have some degree of disparity in holding higher approvals for White/Caucasian applicants than for Black/African American, at first glance. The graphic certainly highlights the highest difference in approval rates at NFCU vs that of other financial institutions. Being at the top of the list of disparity absolutely draws ire from onlookers, yet the other institutions on the list may also be worthy of similar scrutiny. Disparate outcomes are disparate outcomes, and any case in which there are significant differences in outcomes for individuals on a basis of age, gender, race, or ethnicity (also called protected classes) is of concern to everyone.\nA follow-up article by CNN includes statements from NFCU and an external reviewer. Some of the statements claim that non-race factors such as income verification and incomplete credit applications weren’t included in CNN’s study, and that other proprietary and non-public information are included in the organization’s mortgage underwriting processes.\nAlso of interest is an August 2024 story from Lehigh University on Artificial Intelligence, in particular Large Language Models, exhibiting bias in mortgage underwriting decisions. The article covers a study done using ChatGPT 3.5 and 4.0, Llama3-8B and 3-70, and Claude3 Sonnet and Opus, in which the LLMs were prompted to decide whether or not to approve a loan application based on provided data. The study found that the LLMs had similar biases for recommending interest rates, and great variation in bias as to whether or not loans would be approved. When the LLMs were directly instructed to use no bias and ignore race in making the decisions, the disparity in approval outcomes disappeared. This suggests that technology in decision-making processes is a double-edged sword, capable of doing both great harm and great good, depending on how it is weilded.\nThe Home Mortgage Disclosure Act (HMDA) is a federal law that establishes mandatory reporting for financial institutions that provide loans to borrowers. It mandates reporting of various values, metrics, and qualities of applicants and co-applicants, such as their debt-to-income ratio, loan amount requested vs. property value, age/gender/race of applicants, whether or not the loan was approved, along with many other datapoints. Part of the intent of this law is to enable the examination of lenders for potential discriminatory practices in lending, and enable accountability and oversight to combat such issues. HMDA data was leveraged for both the CNN’s report and for the study referenced in Lehigh University’s article.\nIn an ideal world, a potential borrower’s protected class features should have no impact on their ability to attain a loan, but instead be based solely on the borrower’s overall ability to repay the loan, given multiple financial factors. This research seeks to leverage HMDA data to explore CNN’s identified disparities, and explore other possible disparities within other lending organizations, through predictive modeling. Leveraging multiple techniques such as identifying common associations and trends within the data, performing statistical analyses, developing multiple model constructs, and via inclusion and exclusion of protected classes as part of model training and evaluation data, this study seeks to explore and answer some of the following questions (solely using publically available data):\n\nWhich factors within avaialble HMDA data are the greatest influencers in mortgage approvals?\nAre protected classes of applicants and co-applicants strong predictors for determining loan approval?\nIf protected classes are strong predictors for loan approval, is the predictive strength greater for one institution over another?\nIf protected classes are strong predictors for loan approval, is the predictor’s strength higher within a particular geographic region?\nHow is predictive model performance impacted when including or excluding protected class data in training and testing data? e.g. Does performance increase when including? If so, by how much?\nHow well do predictive models perform when trained using protected class information that was collected by a lender observationally (e.g. via inspection of surname or visually seeing the applicant)?\nHow do loan-specific or home-specific qualities and features, absent borrower features, impact predictions of approval or denial?\nAre borrower features aside from race as or more associated with a result of loan denial? e.g. is being female or being under or over a certain age as associated with loan denial as being a certain race?\nCan available HMDA data be leveraged to effectively predict mortgage borrower interest rates?\nIf effective mortgage rate predictions are possible, how do they change when including or controlling for protected class data in the model?\nCan selected / identified features and outcome (e.g. debt-to-income-ratio, loan-to-value ratio, and decision for approval or denial of the loan) be used to predict the borrower’s protected class features of race, gender, or age group?\nAs inferred by lenders, can various latent variables be identified as impactful within modeling, and can the degree or extent of their impact on the decision process be determined as it pertains to mortgage underwriting decisions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "",
    "text": "2.1 Links to Data and Code\nThe outcomes of the work performed in this section are present in multiple locations. The data itself was too large to include as part of the github repository for this effort.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#links-to-data-and-code",
    "href": "data.html#links-to-data-and-code",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "",
    "text": "The GitHub Respository can be found here.\nScript to leverage APIs and download the target data is located here.\nInitial data download, prior to scoping and cleaning, is located here.\nThe script to scope, clean, and combine the dataset is located here.\nThe cleaned dataset is located here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#data-gathering",
    "href": "data.html#data-gathering",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.2 Data Gathering",
    "text": "2.2 Data Gathering\nTo pursue the established research objectives, the following resources were identified and leveraged as data sources:\n\nThe Global Legal Entity Identifier (GLEIF) API\nThe FFIEC Home Mortgage Disclosure Act (HMDA) API\n\nCNN provides explicit data selection, cleaning, and transformation information at the base of their first article in the section labeled “How We Reported This Story.” The scope of this research includes the scope identified by CNN, as the nature of their filtration and focus could be ascribed to what one would consider being part of the American dream - the ability to attain a first loan for a single-family property as a primary residence, not for the purpose of business or commerce. The CNN article predominantly leveraged 2022 and prior year data, whereas this research will inspect 2023 data.\nLenders in question, listed in the graphics for the first CNN article, are identified via the GLEIF API. HMDA Data solely contains the Legal Entity Identifier (LEI) in each record as opposed to the name of the entity. In order to perform aggregate analysis and labeling of data as being linked to one of these organizations, it is necessary to query another source to identify the appropriate LEIs for use in querying HMDA. Furthermore, HMDA is a large database, and being able to provide filtering conditions substantially reduces the size of the returned data. Absent key filters, large volumes are returned in a single file. At the same time, the number of filters that can be supplied to the HMDA API simultaneously is limited, so filtering by organization and other key filters will minimize the data pull before further cleaning.\nThe HMDA API provides simple access to a wide array of data on mortgages. Additionally, the API provides a well-documented data dictionary that spells out the returned features and their meanings in the context of the Loan Application Register. An API query returns a total of 99 columns with quantitative and qualitative data on the prospective borrower, the property they seek to purchase, aspects and features of the loan, and the final action taken on the loan application. Some of these columns may exceed the scope of this research, whereas many others are necessary and important to the established research questions. Finding creative and effective means of reducing dimensionality is key.\nNeither of the API endpoints leveraged in data collection for this research required an API key, thus simplifying the data gathering process.\nThe source code for data gathering efforts in this research is located here.\nExample use of the GLEIF API:\nimport requests\nendpoint = \"https://api.gleif.org/api/v1/lei-records\"\nparams = {\n    'page[size]':100, #specify number of records to return per page\n    'page[number]':1, #specify the page number to return within the same search scope\n    'filter[entity.names]':'NAVY FEDERAL CREDIT UNION' #provide an organization\n}\n\nrequests.get(endpoint,params)\nFor the GLEIF API, the JSON response will contain the LEI for organizations with similar names to what is provided in the filter[entity.names] parameter. Depending on how specific one is with the name provided, this could produce a long list of results. This was the case when examining larger banks like JP Morgan and Wells Fargo while building the code for this research. In some cases, the GLEIF API returned 400 records. When this occurs, it is necessary to make use of the page[size] and page[number] parameters for this API.\nIn the case of gathering 400 results, in order to pull all of them into data records via use of this API, one would need to perform 4 API calls, each time updating the page[number] parameter to the next valid value. The API response also always provides a lastPage value for valid responses under the path response.meta.pagination.lastPage, which one can use for iteration if needed to extract all LEIs for an entity.\nBelow depicts example use of the HMDA API in Python:\nhmda_endpoint = \"https://ffiec.cfpb.gov/v2/data-browser-api/view/csv\"\nhmda_params = {\n    'years':'', #specify the year or list of years for which you seek to query\n    'loan_types':1, #conventional loans only\n    'leis':'' #will change/update based upon what orgs we're downloading\n}\n\nresp = requests.get(endpoint,params)\n\nwith open('/file/path/here.csv','wb') as f:\n    f.write(resp.content)\n    f.close()\nOf note for the HMDA endpoint in this case is the fact that it natively returns record data in CSV format. As such, the means to store the data is relatively simple, requiring no substantial parsing of JSON content to process the data into record or dataframe format.\nThe data, from the initial API queries and prior to further cleaning and tailoring, is located here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.3 Data Cleaning",
    "text": "2.3 Data Cleaning\nThe primary methods of cleaning in this research revolve around dimensionality reduction and row reduction. The goal for cleaning is to minimize the number of columns in the dataset to key quantitative and qualitative features holding potential use in modeling, eliminating records out of the scope of this research, and handling blank/missing/incorrect values in the remaining records.\n\n2.3.1 Dimensionality Reduction\n\n2.3.1.1 Collapsing Redundant Columns with Binary Encoding\nThis research collapses the following columns to deduplicate data and reduce columnar dimensionality:\nApproximately 29 columns in the data contain categorical information that can be encoded in binary and summed into a single column to retain all data while reducing dimensionality. These columns pertain to applicant and co-applicant race and ethnicity, the automated underwriting systems used by organizations to support decision making in approving or denying loan applications, and reasons for loan denial, if applicable.\nSince each of these columns can take on multiple values, the HMDA dataset owners have allowed for up to 5 columns for each of these categorical variables. By translating each possible value to a unique binary encoding (with number of bits equal to the number of classes, and solely a single 1 in each encoding), the sum across multiple columns will provide for a unique value containing all classifications in a single categorical variable.\nFurthermore - this transformation addressses some potential biases in CNN’s methods. Their report excluded records in the case of mixed-race applicants and co-applicants as being members of a racial group. The binary encoding allows for their inclusion as being part of such groups; while individuals of mixed race may not be fully part of any single group, they remain part of it.\nThe data contain multiple column datapoints for the ethnicity and race of applicants and co-applicants for a total of 20 additional columns (e.g. “co-applicant_race-1, co-applicant_race-2…”). Not all of these columns contain viable data. An individual can have multiple races and ethnicities. To allow for retaining of the data while also eliminating unnecessary blank columns, these columns are collapsed to a single column, each - reducing the data by 16 columns.\nAnother set of column collapses are performed on the ‘aus-’ (automated underwriting system) and ‘denial_reason’ columns. There are 5 columns in the source data containing the different system(s) used by lenders to support underwriting decision making processes. The denial_reason is rich with potentially important information that could lend itself to the identification of latent or unavailable variables (e.g. credit score, income verification, etc). These columns are collapsed in the same manner as for the aforementioned race and ethnicities of applicants.\nEach value was translated from the digit representing it to instead a binary representation with a number of bits equivalent to the number of valid classes available for the variable from the data dictionary. After each value in each column was mapped to a bitwise representation, the sum across each column was taken to produce a final column with bits containing all of the original information. This transformation enables simple bitwise AND operations to identify records that meet a specific condition (or combination of conditions) while also eliminating columnar redundancy.\n\n\n\n\nTable 2.1: Remapping of Racial Categories for applicants and co-applicants\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nAmerican Indian or Alaska Native\n1\n1\n\n\nAsian\n2\n2\n\n\nAsian Indian\n21\n4\n\n\nChinese\n22\n8\n\n\nFilipino\n23\n16\n\n\nJapanese\n24\n32\n\n\nKorean\n25\n64\n\n\nVietnamese\n26\n128\n\n\nOther Asian\n27\n256\n\n\nBlack or African American\n3\n512\n\n\nNative Hawaiian or Other Pacific Islander\n4\n1024\n\n\nNative Hawaiian\n41\n2048\n\n\nGuamanian or Chamorro\n42\n4096\n\n\nSamoan\n43\n8192\n\n\nOther Pacific Islander\n44\n16384\n\n\nWhite\n5\n32768\n\n\nInformation not provided by applicant in mail, internet, or telephone application\n6\n65536\n\n\nNot applicable\n7\n131072\n\n\nNo co-applicant\n8\n262144\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.2: Remapping of Ethnic Categories for applicants and co-applicants\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nHispanic or Latino\n1\n1\n\n\nMexican\n11\n2\n\n\nPuerto Rican\n12\n4\n\n\nCuban\n13\n8\n\n\nOther Hispanic or Latino\n14\n16\n\n\nNot Hispanic or Latino\n2\n32\n\n\nInformation not provided by applicant in mail, internet, or telephone application\n3\n64\n\n\nNot applicable\n4\n128\n\n\nNo co-applicant\n5\n256\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.3: Remapping of Automated Underwriting Systems\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nDesktop Underwriter (DU)\n1\n1\n\n\nLoan Prospector (LP) or Loan Product Advisor\n2\n2\n\n\nTechnology Open to Approved Lenders (TOTAL) Scorecard\n3\n4\n\n\nGuaranteed Underwriting System (GUS)\n4\n8\n\n\nOther\n5\n16\n\n\nInternal Proprietary System\n7\n32\n\n\nNot applicable\n6\n64\n\n\nExempt\n1111\n128\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4: Remapping of Denial Reasons\n\n\n\n\n\n\n\n\nValid Classes\nHMDA Encoding\nBinary Encoding\n\n\n\n\nDebt-to-income ratio\n1\n1\n\n\nEmployment history\n2\n2\n\n\nCredit history\n3\n4\n\n\nCollateral\n4\n8\n\n\nInsufficient cash (downpayment, closing costs)\n5\n16\n\n\nUnverifiable information\n6\n32\n\n\nCredit application incomplete\n7\n64\n\n\nMortgage insurance denied\n8\n128\n\n\nOther\n9\n256\n\n\nNot applicable\n10\n512\n\n\n\n\n\n\n\n\nExamining Table 2.4, for instance consider a cell value of 96 in the ‘denial_reason’ column. There are two unique binary encoding values - 32 and 64, that produce this sum. The value 96, then, would signify a loan that was denied both for having ‘Unverifiable Information’ and for ‘Credit application incomplete’. Similarly, referencing Table 2.1, a value of 32770 in this column would signify a person whose races are Asian (32768) and White (2) (2+32768=32770).\nThis binary numeric representation of categorical data allows the column to contain multiple distinct and potentially impactful datum while reducing dimensionality. E.g. to filter the dataset to records pertaining to the race Asian, the filter can be performed by selecting records where the binary AND of 32768 & [column_value] = 32768. Other races may also be present in the filtered records, but all Asians will be captured.\nLeveraging the columns natively for tasks such as clustering may not be effective or efficient, and could pose challenges in performing unsupervised learning. Further cleaning may be necessary for such tasks, but collapsing and retaining the data ensures its availablity for further transformation and cleaning. For instances, the re-splitting and pivoting of these columns into boolean values could be leveraged in association rule mining or in Bernoulli Naive Bayes analyses.\nNotice also that encodings for not available or not applicable types of data are higher values within each encoding list. This actually provides a very simple data validation technique, and allows for the identification or elimination of potentially erroneous records. Namely any value:\n\nwithin the exclusive interval (65536,131072), or greater than 131072 for applicant_race\nwithin the exclusive intervals (65536,131072) and (131072,262144), or greater than 262144 for co-applicant_race\nwithin the exculsive interval (64, 128), or greater than 128 for applicant_ethnicity\nwithin the exclusive intervals (64,128) and (128,256), or greater than 256 for co-applicant_ethnicity\nwithin the exclusive interval (64,128), or greater than 128 for aus (automated underwriting system)\ngreater than 512 for denial_reason\n\nwill signify an erroneous record. Namely, if any value is found to include “Not Applicable” in conjunction with another valid selection, one of the two selections were selected in error, and the data is not reliable. As such, these records can be filtered from the source data. As such, records meeting the above criteria are removed from the source data.\nBecause the totality of racial and ethnicity information can now be contained within a single column, the derived_ columns for ethnicity and race can be dropped from the dataset.\n\n\n2.3.1.2 Column Elimination\nThe following list of columns were eliminated from the source data\n\n\n\n\n\n\n\nColumn Dropped\nReason\n\n\n\n\nactivity_year\n2023 Data Only\n\n\nderived_msa-md\nUsing county_code\n\n\ncensus_tract\nUsing county_code\n\n\nderived_loan_product_type\nAvailable in other columns\n\n\nderived_dwelling_category\nAvailable in other columns\n\n\nconforming_loan_limit\nProject scope\n\n\nlien_status\nProject scope\n\n\nreverse_mortgage\nOne value after scope applied\n\n\nbusiness_or_commercial_purpose\nProject scope\n\n\nnegative_amortization\nOne value after scope applied\n\n\noccupancy_type\nProject scope\n\n\nconstruction_method\nProject scope\n\n\nmanufactured_home_secured_property_type\nProject scope\n\n\nmanufactured_home_land_property_interest\nProject scope\n\n\nsubmission_of_application\nProject scope\n\n\ninitially_payable_to_institution\nProject scope\n\n\nderived_ethnicity\nAvailable in other columns\n\n\nderived_race\nAvailable in other columns\n\n\nloan_type\nProject scope\n\n\nprepayment_penalty_term\nProject scope\n\n\napplicant_age_above_62\nAvailable in other columns\n\n\nco-applicant_age_above_62\nAvailable in other columns\n\n\ntotal_points_and_fees\n&lt;1% of records have values\n\n\nrate_spread\n&lt;1% of records have values\n\n\nmultifamily_affordable_units\n&lt;1% of records have values\n\n\nlei\nSubstituting company name\n\n\nstate_code\nUsing county_code\n\n\nhoepa_status\nSingle value in column after scope applied\n\n\n\n\n\n\n\n\n2.3.2 Row Elimination\nThe initial pull from the HMDA API allowed solely for filtering by one or two filters. This effort pulled data for 5 selected lenders in the year 2023, for conventional loans only. To reduce the data to the same scope stipulated in the CNN article, row records must further be eliminated based upon the following criteria:\n\nexact duplication of another row (from the source data, before any transformations are applied). The nature of the data is such that the probability of an exact duplicate of 99 column values between two records is negligible, and as such exact duplicates should be ignored.\nscoping to same frame as CNN:\n\nlien_status != 1, or only first lien secured properties\ntotal_units not in [1,2,3,4], or only 1-4 unit homes\nconforming_loan_limit != ‘C’, or only conforming loans\nbusiness_or_commercial purpose != 2, or non-commercial and non-business properties\noccupancy_type != 1, or primary residence\nloan_purpose != 1, or for the purchase of a home\n\nrecords in which there is no value for county_code\nrecords for which the loan would be neither approved nor denied: when the action_taken value is 4 (Application Withdrawn) or 5 (File closed for incompleteness).\nrecords that have invalid values as defined in the binary encoding section above for 6 variables (applicant_race, co-applicant_race, applicant_ethnicity, co-applicant_ethnicity, aus, denial_reason).\n\n\n\n2.3.3 Ordinal Encoding\nThe following columns are re-encoded as ordinal data:\n\ntotal_units\napplicant_age\ndebt_to_income_ratio\n\n\n\n2.3.4 Missing and Blank Values\nThe following aspects of the dataset have the potential for missing or blank values. The reporting requirements vary for the different columns in the HMDA LAR, and the blanks and missing values can occur for a multitude of reasons:\n\napplicant didn’t provide the information\napplicant withdrew their application\ndata was not entered or submitted by the institution\ninstitution did not have or receive the data for the applicant or their prospective property\nthe column may not be applicable or filled because the application was denied, or was not applicable for the particular loan circumstances (e.g. an introductory rate period wouldn’t be applicable for an adjustable rate mortgage)\n\nEach of the following is replaced by the median value of the variable when the data is grouped by state_code and total_units (e.g. number of rooms in the home):\n\nproperty_value\nloan_to_value_ratio. special note: when property_value and loan_amount are available for a record, this value is filled with the value of the loan amount (times 100) divided by the listed property value. Otherwise, any blanks are filled with the median loan to value ratio when the data is grouped by state and number of rooms.\n\nEach of the following is replaced by the median value of the variable when the data is grouped by state_code and company (e.g. institution processing the mortgage application):\n\nlender_credits\nintro_rate_period\ninterest_rate\norigination_charges\ndiscount_points\ntotal_loan_costs\n\nNote: Whenever the grouping by state_code and company produced N/A values, these were replaced with the value 1 in the event of a need for logarithmic transformations on these variables.\n\n\nEach of the following is replaced with the mode of the variable when the data is grouped by state_code and company (e.g. institution processing the mortgage application):\n\nintro_rate_period\ndebt_to_income_ratio\n\nNote: Whenever the grouping by state_code and total_units produced N/A values, these were replaced with the value 0 as they better align with categorical variables than as numeric, and may not undergo transformations.\n\n\nAnother common blank value included the income feature. Blanks and missing values are replaced with the value from the column ffiec_msa_md_median_family_income divided by 1000. This value is an approximation for median family income in the specific metropolitan statistical area (MSA) in which the home is located.\n\n\n2.3.5 Incorrect Column Values\nSome values of interest for inspection include loan_to_value_ratio, property_value, and loan_amount, which should, in theory, all be related and have reasonable values. A first-lien for-the-purpose-of-purchase, conventional, conforming loan is highly unlikely to have a value greater than 100%. This is because a savvy lender would not secure a property as collateral for a loan that greatly exceeds its value.\nThat being said, there may be cases were this ratio could stretch further based upon qualities of the borrower and status of the market, and especially if the borrower seeks to include the overage as part of plans for improvements of the home, or covering closing costs or other mortgage requirements - in combination with the borrower having an excellent credit score or having a very high income. The case of high loan-to-value-ratio (henceforth LTV Ratio), may be unlikely, but is not impossible.\nBelow is an examination for each lender for extreme values in LTV ratio:\n\n\n\n\n\n\n\n\nFigure 2.1: Violin plot of Loan To Value Ratio, by lender\n\n\n\n\n\nThere appear to be some extreme values to the high right for each lender, in some cases with the ratio nearing 1000%. To examine further, whether or not the loan was approved should be considered, and if the loan was denied, the reason for denial should be considered as well.\nIf any values in this extreme range consist of loans that were denied, and reasons for denial include items such as “insufficient collateral”, then these items may simply be extreme case outliers that are valid and real as opposed to errors in the data.\nConsidering a range of 100% or greater loan_to_value_ratio, the following information is available:\n\n\n\n\n\n\n\n\nFigure 2.2: Approvals/Denials for Applications with LTV &gt; 100%\n\n\n\n\n\nIn Figure 2.2, it is clear that the majority of loans in this category were denied by lenders. There is a subset of loans that were approved (~40%), and examining further to examine factors such as the loan_to_value_ratios in this group, as well as reasons for denial amongst the denied applications, is necessary before determining need for identifying these records as errors vs. outliers.\n\n\n\n\n\n\n\n\nFigure 2.3: Reasons Cited for Loan Denials with LTV Ratio &gt; 100%\n\n\n\n\n\nThis plot makes it apparent that many of these high LTV ratio loan applications are likely legitimate. Many of them were denied on the bases of debt to income ratio and collateral (e.g. the property is insufficient collateral to cover the risk of the loan).\nThat being said, it’s also necessary to examine the 40% of loans that were actually approved in this window.\n\n\n\n\n\n\n\n\nFigure 2.4: strip plot of high LTV (&gt;100%) 2023 loans by approval\n\n\n\n\n\nFrom Figure 2.4, its apparent that the approved loans in this excess range tended to be on the much lower end of the spectrum, seemingly hovering just above 100%, whereas the denied applications span from just over 100% all the way up to 1000%. All in all, it would seem these extreme values, in both the cases of approved and denied loans, are legitimate values.\nA remaining concern is for potential discrepancy in the loan to value ratio itself. It’s appears odd that many data points bunch up at the 1000% mark as opposed to going above or below it, like it’s an arbitrary cap on the actual value. Below the same plot is examined, but instead the Loan to Value Ratio is replaced the value of Loan Amount * 100 / Property Value.\n\n\n\n\n\n\n\n\nFigure 2.5: strip plot of high LTV (&gt;100%) 2023 loans by approval, with adjusted LTV value\n\n\n\n\n\nIt would seem that the 1000% mark may indeed be an arbitrary cap, or a potential error of some sort. Per the Consumer Financial Protection Bureau on reporting requirements, the value for the combined loan to value ratio is to be “the ratio of the total amount of debt secured by the property to the value of the property relied on in making the credit decision.”\nIn most other cases in which the values sit below 100%, this calculated value is within a small unit difference of the reported value. This may be because the decimal places on the decision are only to be included if those decimal points were relied upon to make the decision on whether or not to approve the loan.\nIn light of the exploration on loan to value ratio, the feature loan_to_value_ratio will be replaced with the value of \\(\\frac{100\\cdot\\text{loan amount}}{\\text{property value}}\\) . In cases where one of these values is blank, the loan_to_value_ratio will be filled with the median value for loan_to_value_ratio when the data is grouped by state_code and total_units (or number of rooms) in the home.\nAnother feature worth exploring is income. This feature, per the HMDA LAR Data Dictionary, is in thousands of dollars. There are millionaires and even billionaires in the United States. Generally, though, one might expect to see an exponential distribution of income in the source dataset. Furthermore, one should expect to see a narrow string of values getting thinner and thinner as it approaches the upper end of the dataset. Inspecting income with a boxplot, there are some clear challenges:\n\n\n\n\n\n\n\n\nFigure 2.6: boxplot of borrower income\n\n\n\n\n\nA cursory inspection of this data suggests that there may be a single, erroneous outlier sitting at a value of approximately $209M worth of income, compressing the visibility of the boxplot down to nearly nothing. It is possible that this datapoint is real and correct, and that there was a single, very wealthy applicant for a mortgage in the data.\nProducing the same plot, absent the outlier, however, displays similar results:\n\n\n\n\n\n\n\n\nFigure 2.7: Boxplot of Income (Outlier Removed)\n\n\n\n\n\nHowever, below is an examination of the data with and without this high-end datapoint using kernel density estimation for the natural logarithm of income:\n\n\n\n\n\n\n\n\nFigure 2.8: Logarithmic Kernel Density Estimate for Applicant Income\n\n\n\n\n\nWith the exception of some deviations from normality in the central portions of the curve, these plots appear to showcase a normally distributed variable. Examining the mean and standard deviations for the log of income, with and without the extreme outlier, produces the following:\n\n\n\n\nTable 2.5: mean and standard deviation, with and without extreme outlier for income\n\n\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\n\nWith Outlier\n4.725800\n0.623842\n\n\nWithout Outlier\n4.725769\n0.623654\n\n\n\n\n\n\n\n\n\n\nThe mean and standard deviation of the log-transformed feature, income, appears to retain the same features of central tendency with or without this extreme value. Provided that a log transformation of the feature is used within any models, this outlier should have little to no impact on training and testing data, as the underlying parent distribution can be approximated by that of the normal distribution \\(N(4.726,0.624)\\) in either case. As such, the row will be retained, and the value will not be adjusted (minus the log-transformation).\n\n\n2.3.6 Cleaned Columns\nPrior to cleaning, there were numerous blank or not applicable values within the data. The below image depicts the state of a subset of the data, after initial download.\n\n\n\nData, after initial download, held multiple blanks\n\n\nThe column volume is not done justice by this image; the original dataset held nearly 100 columns with large swaths of blank and missing values. Through the methods described previously, this data was appropriately scoped, transformed, cleaned, and simplified for further use.\nThe final results of the cleaning process are too large to fully depict in images here. Despite a substantial cleaning effort, there remains a large volume of columns (~50 down from 100) of varying types. Here, some key cleanups performed during the cleaning effort are highlighted:\n\n\n\nAfter Filling Blanks With Medians, Modes\n\n\nOne can see that there are no blanks in this sector of the dataset, and that there is a wider array of values and reasonable variablility in the columns, examining record by record. There may be additional work and adjustments to be performed in the realm of cleaning, but having populated columns, filled with median and modal values, provides a point of departure for further examination and analysis.\n\n\n\nCollapsing 29 Columns to 6\n\n\nThe above depiction of the collapsed columns is a substantial dimensionality reduction while retaining all of the underlying data. Each of these columns were originally 5 columns in the source data (less denial_reason, which was 4 columns).\nIn any case where the value in one of these columns holds an exact power of 2, it means that only one of those 4-5 original columns held a value - values like 1, 16, 32, 64, 128, 256, 512, 32768, 65536, 131072, and 262144 (all present in this graphic).\nThis means there were nearly 24 columns worth of blanks for each record depicted in the above image. One can nearly count on their fingers the number of instances for these records in which more than one of those columns had legitimate values. This collapsing of records while retaining data is good compression and retention for storage and future modeling.\nThe cleaned dataset can be found and downloaded here. Alternatively, one can clone this research project’s GitHub Repository, run the data pulling script, and then run the data cleaning script, in order to reproduce the same dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#further-exploratory-data-analysis",
    "href": "data.html#further-exploratory-data-analysis",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.4 Further Exploratory Data Analysis",
    "text": "2.4 Further Exploratory Data Analysis\nTo examine the data under similar conditions as CNN for 2023 data, below is a plot of the top 5 lenders and their approval rates for select racial groups, controlling for no other variables:\n\n\n\n\n\n\n\n\nFigure 2.9: 2023 Loan Approval Rates, By Company and Select Race\n\n\n\n\n\nThe above appears to flow from 2022’s findings into 2023 - that, at least for NFCU, the trend appears to continue under the specified scope with approval rates of about 76% for White applicants, and 43.8% for Black/African American Applicants. Comparing Figure 2.9 to CNN’s figures, the outcomes are remarkably similar for 2023.\nSimilar exploration into sex and age is also of interest for this study:\n\n\n\n\n\n\n\n\nFigure 2.10: Approval Rates by Institution and Borrower Sex\n\n\n\n\n\nThis chart also reveals some interesting trends, not just for NFCU, but across lenders. Notably - when no sex is provided by the applicant, 100% of loans across lenders received approval of some sort. Somewhat similarly, for lenders like JP Morgan, Rocket Mortgage, and Wells Fargo, the institutions had high approval rates in cases for which the applicant selected both sexes on their application.\nLastly, as it is pertinent to the established research questions, examination of differences in outcomes for reported age is necessary.\n\n\n\n\n\n\n\n\nFigure 2.11: barplots of approval rates by age, 2023\n\n\n\n\n\nThis examination reveals similar results as for sex; when no age is reported or documented, it appears that the approval rate approaches 100% for applicants. However, it is not the case when race is not reported. In Figure 2.9, one can see that in the case of no race reported, approval rates for such mortgages ranged from 62.9% (NFCU) to 98.6% (JP Morgan).\nThese comparisons, however, are simply numerical and do not establish any kind of a cause and effect relationship in either case, for any of the variables. Just because the outcomes in 2023 when gender or age were not reported to these lenders were nearly 100% of the time an approved mortgage, does not mean that one not reporting their gender or age on a mortgage application guarantees that they will be approved by one of these 5 lenders.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "data.html#last-thoughts-on-source-data",
    "href": "data.html#last-thoughts-on-source-data",
    "title": "2  Data Gathering and Exploratory Analysis",
    "section": "2.5 Last Thoughts on Source Data",
    "text": "2.5 Last Thoughts on Source Data\nThe provided visuals and exploration in this research thus far are observations of a snapshot in time - they, in and of themselves, do not establish cause-and-effect relationships or the presence of statistically significant differences in outcomes between protected classes and lenders.\nThe goals of this research include bi-directional modeling. While machine learning modeling on its own does not elucidate a cause-and-effect relationship between variables, it comes closer than simple observations or statistical analyses. Performing bi-directional modeling (can available features and subject’s age/gender/race predict their outcome? and can a subject’s outcome and features predict the subject’s age/gender/race) takes additional steps in the direction of causality.\nEstablishing significant differences can show a difference, but not the cause. Establishing models that effectively predict show that the statistical differences in groups can be leveraged effectively to predict outcomes, but also do not establish cause. Exploration of latent variables, interjection and intervention of available and latent variables, and further research, are all needed to establish causality.\nMore exploration is necessary within this data, and potentially additional cleaning and transformations. Few if any numeric variables were scaled or transformed to collapse them all within similar ranges. This will be needed for building certain models within this research effort. Further understanding of categorical variable associations and relationships need further exploration and study. This will be an ongoing effort throughout this research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Gathering and Exploratory Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "3  Principal Component Analysis",
    "section": "",
    "text": "3.1 Overview\narray([2.52950398e+00, 1.78082662e+00, 9.56079269e-01, 8.70800814e-01,\n       5.69628535e-01, 3.11896840e-01, 1.03456815e-01, 6.56749179e-02,\n       5.87546698e-02, 3.11636167e-02, 3.07880388e-03, 3.28044050e-27])\n\n\nFigure 3.1: PCA Calc",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#data",
    "href": "pca.html#data",
    "title": "3  Principal Component Analysis",
    "section": "3.2 Data",
    "text": "3.2 Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#code",
    "href": "pca.html#code",
    "title": "3  Principal Component Analysis",
    "section": "3.3 Code",
    "text": "3.3 Code",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "pca.html#results",
    "href": "pca.html#results",
    "title": "3  Principal Component Analysis",
    "section": "3.4 Results",
    "text": "3.4 Results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "4  Data Clustering",
    "section": "",
    "text": "4.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#data",
    "href": "clustering.html#data",
    "title": "4  Data Clustering",
    "section": "4.2 Data",
    "text": "4.2 Data",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#code",
    "href": "clustering.html#code",
    "title": "4  Data Clustering",
    "section": "4.3 Code",
    "text": "4.3 Code",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#results",
    "href": "clustering.html#results",
    "title": "4  Data Clustering",
    "section": "4.4 Results",
    "text": "4.4 Results",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Clustering</span>"
    ]
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "5  Association Rule Mining",
    "section": "",
    "text": "5.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#data",
    "href": "arm.html#data",
    "title": "5  Association Rule Mining",
    "section": "5.2 Data",
    "text": "5.2 Data",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#code",
    "href": "arm.html#code",
    "title": "5  Association Rule Mining",
    "section": "5.3 Code",
    "text": "5.3 Code",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "arm.html#results",
    "href": "arm.html#results",
    "title": "5  Association Rule Mining",
    "section": "5.4 Results",
    "text": "5.4 Results",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Association Rule Mining</span>"
    ]
  },
  {
    "objectID": "dt.html",
    "href": "dt.html",
    "title": "6  Decision Tree Modeling",
    "section": "",
    "text": "6.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "dt.html#data",
    "href": "dt.html#data",
    "title": "6  Decision Tree Modeling",
    "section": "6.2 Data",
    "text": "6.2 Data",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "dt.html#code",
    "href": "dt.html#code",
    "title": "6  Decision Tree Modeling",
    "section": "6.3 Code",
    "text": "6.3 Code",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "dt.html#results",
    "href": "dt.html#results",
    "title": "6  Decision Tree Modeling",
    "section": "6.4 Results",
    "text": "6.4 Results",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Decision Tree Modeling</span>"
    ]
  },
  {
    "objectID": "nb.html",
    "href": "nb.html",
    "title": "7  Naive Bayes",
    "section": "",
    "text": "7.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "nb.html#data",
    "href": "nb.html#data",
    "title": "7  Naive Bayes",
    "section": "7.2 Data",
    "text": "7.2 Data",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "nb.html#code",
    "href": "nb.html#code",
    "title": "7  Naive Bayes",
    "section": "7.3 Code",
    "text": "7.3 Code",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "nb.html#results",
    "href": "nb.html#results",
    "title": "7  Naive Bayes",
    "section": "7.4 Results",
    "text": "7.4 Results",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Naive Bayes</span>"
    ]
  },
  {
    "objectID": "reg.html",
    "href": "reg.html",
    "title": "8  Regression Modeling",
    "section": "",
    "text": "8.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#data",
    "href": "reg.html#data",
    "title": "8  Regression Modeling",
    "section": "8.2 Data",
    "text": "8.2 Data",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#code",
    "href": "reg.html#code",
    "title": "8  Regression Modeling",
    "section": "8.3 Code",
    "text": "8.3 Code",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "reg.html#results",
    "href": "reg.html#results",
    "title": "8  Regression Modeling",
    "section": "8.4 Results",
    "text": "8.4 Results",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Modeling</span>"
    ]
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "9  Support Vector Machine Modeling",
    "section": "",
    "text": "9.1 Overview",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machine Modeling</span>"
    ]
  },
  {
    "objectID": "svm.html#data",
    "href": "svm.html#data",
    "title": "9  Support Vector Machine Modeling",
    "section": "9.2 Data",
    "text": "9.2 Data",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machine Modeling</span>"
    ]
  },
  {
    "objectID": "svm.html#code",
    "href": "svm.html#code",
    "title": "9  Support Vector Machine Modeling",
    "section": "9.3 Code",
    "text": "9.3 Code",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machine Modeling</span>"
    ]
  },
  {
    "objectID": "svm.html#results",
    "href": "svm.html#results",
    "title": "9  Support Vector Machine Modeling",
    "section": "9.4 Results",
    "text": "9.4 Results",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Support Vector Machine Modeling</span>"
    ]
  }
]