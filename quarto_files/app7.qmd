# Regression Code {#sec-App7 .appendix}

```{python moduleImports}
#probably doing this to examine / predict interest rates...
#refit the MCA to exclude interest rate!
    #one set with the ethnic/racial/age predictors
    #one set without 
import pandas as pd, numpy as np, seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import (
    accuracy_score,roc_auc_score,
    precision_score, recall_score, 
    f1_score, confusion_matrix,
    ConfusionMatrixDisplay
)
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE
```

```{python DataImports}
mca=pd.read_csv('../data/mcaNd.csv')
mca_npc=pd.read_csv('../data/mcaNd-npc.csv')
labels=pd.read_csv('../data/final_clean_r2.csv')['outcome']
```

## Logistic Regression

Data was prepared in @sec-MCA-app.

Performing train-test split of 80/20, stratified on the outcome "approve/deny".

```{python}
#| echo: true
#| eval: true

#build the training and test data
X_train,X_test,y_train,y_test = train_test_split(
    mca, #mca[mca.columns[:126]],
    labels,
    stratify=labels,
    random_state=8808,
    test_size=0.2
)
X_train_npc,X_test_npc,y_train,y_test = train_test_split(
    mca_npc, #mca_npc[mca_npc.columns[:86]],
    labels,
    stratify=labels,
    random_state=8808,
    test_size=0.2
)
y_train_copy = y_train.copy()
# X_train,y_train = # SMOTE(random_state=8808).fit_resample(X_train,y_train_copy.copy())
# X_train_npc,y_train = SMOTE(random_state=8808).fit_resample(X_train_npc,y_train_copy.copy())

results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
```

```{python}
#| echo: true
#| eval: true

#train the model
lr = LogisticRegression(max_iter=300)
lr_npc = LogisticRegression(max_iter=300)
```

```{python}
#| echo: true
#| eval: true

lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
results.loc[len(results)] = {
    'Model':'Logistic Regression',
    'Data':'With Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'ROC-AUC':roc_auc_score(y_test,y_pred)
}
```

```{python}
#| echo: true
#| eval: true

#run the model with the test dataset
lr_npc.fit(X_train_npc,y_train)
y_pred_npc = lr_npc.predict(X_test_npc)
results.loc[len(results)] = {
    'Model':'Logistic Regression',
    'Data':'Without Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred_npc),
    'Precision':precision_score(y_test,y_pred_npc),
    'Recall':recall_score(y_test,y_pred_npc),
    'F1':f1_score(y_test,y_pred_npc),
    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
}
```

```{python}
#| echo: true
#| eval: true
# display_labels=My_BNB_Model.classes_)
#display summarized classification results
import matplotlib.pyplot as plt
fig,axes=plt.subplots(nrows=1,ncols=2)

ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred,y_true=y_test
    ),
    display_labels=['Deny','Approve']
).plot(ax=axes[0])
ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred_npc,y_true=y_test
    ),
    display_labels=['Deny','Approve']
).plot(ax=axes[1])
axes[0].set_title('With Protected\nClasses')
axes[1].set_title('Without Protected\nClasses')
plt.suptitle("Confusion Matrices - Logistic Regression")
plt.tight_layout()
plt.show()
```

```{python}
display(results.style.hide(axis='index'))
```

```{python}
#| echo: true
#| eval: false
# are the results significantly different? do a randomization test...

results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
np.random.seed(2036)
for i in range(500):
    print('iteration: {}'.format(i+1))
    r = np.random.randint(0,5000,1)
    X_train,X_test,y_train,y_test = train_test_split(
        mca, #mca[mca.columns[:126]],
        labels,
        stratify=labels,
        random_state=r[0],
        test_size=0.2
    )
    X_train_npc,X_test_npc,y_train,y_test = train_test_split(
        mca_npc, #mca_npc[mca_npc.columns[:86]],
        labels,
        stratify=labels,
        random_state=r[0],
        test_size=0.2
    )
    y_train_copy = y_train.copy()
    # X_train,y_train = SMOTE(random_state=8808).fit_resample(X_train,y_train_copy.copy())
    # X_train_npc,y_train = SMOTE(random_state=8808).fit_resample(X_train_npc,y_train_copy.copy())
# lr = LogisticRegression(max_iter=300)
# lr_npc = LogisticRegression(max_iter=300)
    lr.fit(X_train,y_train)
    lr_npc.fit(X_train_npc,y_train)
    y_pred = lr.predict(X_test)
    y_pred_npc = lr_npc.predict(X_test_npc)
    results.loc[len(results)] = {
        'Model':'Logistic Regression',
        'Data':'With Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred),
        'Precision':precision_score(y_test,y_pred),
        'Recall':recall_score(y_test,y_pred),
        'F1':f1_score(y_test,y_pred),
        'ROC-AUC':roc_auc_score(y_test,y_pred)
    }
    results.loc[len(results)] = {
        'Model':'Logistic Regression',
        'Data':'Without Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred_npc),
        'Precision':precision_score(y_test,y_pred_npc),
        'Recall':recall_score(y_test,y_pred_npc),
        'F1':f1_score(y_test,y_pred_npc),
        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
    }

```

```{python}
#| echo: true
#| eval: false
results.to_csv('../data/logRegRandTest.csv',index=False)
```

```{python}
#| echo: false
#| eval: true
results = pd.read_csv('../data/logRegRandTest.csv')
```

```{python}
#| echo: true
#| eval: true

#visualize the output distributions
res = results.copy()
res = res.melt(id_vars=['Model','Data'],var_name='Metric',value_name='Score')
g = sns.FacetGrid(data=res,col='Metric',hue='Data',col_wrap=3)
g.map_dataframe(
    sns.kdeplot,
    x='Score'#,
    # hue='Data'
)
g.add_legend(loc='lower right')
plt.suptitle("Distributions for Randomization of Training/Testing Data\nLogistic Regression")
plt.tight_layout()
plt.show()
```


```{python}
from statsmodels.stats.weightstats import ztest
tbl_pc = results[results['Data']=='With Protected Classes'].describe().T.reset_index()
tbl_npc = results[results['Data']=='Without Protected Classes'].describe().T.reset_index()

from scipy import stats
sig = pd.DataFrame({
    'Stat':[],
    'z-score':[],
    'p-value':[],
    'top performer':[],
    'top mean':[],
    'difference in means':[]
})

z_stat,p_value = ztest(
    results.loc[results['Data']=='With Protected Classes']['Accuracy'],
    results.loc[results['Data']=='Without Protected Classes']['Accuracy'],
)
scores = tbl_pc['index'].unique()
for score in scores: 
    z_stat,p_value = ztest(
        results.loc[results['Data']=='With Protected Classes'][score],
        results.loc[results['Data']=='Without Protected Classes'][score],
    )
    mu_pc, mu_npc = (
        tbl_pc.loc[tbl_pc['index']==score,'mean'].iloc[0],
        tbl_npc.loc[tbl_npc['index']==score,'mean'].iloc[0]
    )
    winner = np.select(
        [
            mu_pc < mu_npc,
            mu_npc < mu_pc
        ],
        [
            'Without Protected Classes',
            'With Protected Classes'
        ],
        default='tie'
    )
    diff = np.abs(mu_pc-mu_npc)
    m = max(mu_pc,mu_npc)
    sig.loc[len(sig)] = {
        'Stat':score,
        'z-score':z_stat,
        'p-value':p_value,
        'top performer':winner,
        'top mean':m,
        'difference in means':diff
    }

display(sig.style.hide(axis='index'))
```

## Multinomial Naive Bayes

Multinomial Naive Bayes code and data processing can be found in @sec-App6-NB
