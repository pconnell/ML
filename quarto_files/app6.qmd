# Naive Bayes Code {#sec-App6-NB .appendix}

```{python}
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB
from sklearn.metrics import (
    accuracy_score,roc_auc_score,
    precision_score, recall_score, 
    f1_score, confusion_matrix,
    ConfusionMatrixDisplay
)
import pandas as pd, numpy as np, seaborn as sns
from sklearn.preprocessing import LabelEncoder
```

```{python}
#| echo: true
#| eval: true

#for Bernoulli NB
data = pd.read_csv('../data/data-one-hot.csv')
data_npc = pd.read_csv('../data/data-one-hot-npc.csv')
# labels=pd.read_csv('../data/final_clean_r2.csv')['outcome']
fr = pd.read_csv('../data/final_clean_r2.csv')
gaussian_data = fr.copy()
labels = fr['outcome'].copy()
fr.drop(columns=['outcome','purchaser_type'],inplace=True)
```

## Data PreProcessing for CategoricalNB

```{python}
#| echo: true
#| eval: true
#prep the data - turn binary back to true/false
mapper = {
    'applicant_race':{
        'American Indian/Alaska Native':0b0000000000000000001,
        'Asian':0b0000000000000000010,
        'Asian Indian':0b0000000000000000100,
        'Chinese':0b0000000000000001000,
        'Filipino':0b0000000000000010000,
        'Japanese':0b0000000000000100000,
        'Korean':0b0000000000001000000,
        'Vietnamese':0b0000000000010000000,
        'Other Asian':0b0000000000100000000,
        'Black/African American':0b0000000001000000000,
        'Native Hawaiian/Pacific Islander':0b0000000010000000000,
        'Native Hawaiian':0b0000000100000000000,
        'Guamanian/Chamorro':0b0000001000000000000,
        'Samoan':0b0000010000000000000,
        'Other Pacific Islander':0b0000100000000000000,
        'White':0b0001000000000000000,
        'Information not provided':0b0010000000000000000,
        'Not Applicable':0b0100000000000000000,
        'No Co-applicant':0b1000000000000000000
    },
    'co-applicant_race':{
        'American Indian/Alaska Native':0b0000000000000000001,
        'Asian':0b0000000000000000010,
        'Asian Indian':0b0000000000000000100,
        'Chinese':0b0000000000000001000,
        'Filipino':0b0000000000000010000,
        'Japanese':0b0000000000000100000,
        'Korean':0b0000000000001000000,
        'Vietnamese':0b0000000000010000000,
        'Other Asian':0b0000000000100000000,
        'Black/African American':0b0000000001000000000,
        'Native Hawaiian/Pacific Islander':0b0000000010000000000,
        'Native Hawaiian':0b0000000100000000000,
        'Guamanian/Chamorro':0b0000001000000000000,
        'Samoan':0b0000010000000000000,
        'Other Pacific Islander':0b0000100000000000000,
        'White':0b0001000000000000000,
        'Information not provided':0b0010000000000000000,
        'Not Applicable':0b0100000000000000000,
        'No Co-applicant':0b1000000000000000000
    },
    'applicant_ethnicity':{
        'Hispanic/Latino':0b000000001,
        'Mexican':0b000000010,
        'Puerto Rican':0b000000100,
        'Cuban':0b000001000,
        'Other Hispanic/Latino':0b000010000,
        'Not Hispanic/Latino':0b000100000,
        'Information Not Provided':0b001000000,
        'Not Applicable':0b010000000,
        'No Co-applicant':0b100000000
    },
    'co-applicant_ethnicity':{
        'Hispanic/Latino':0b000000001,
        'Mexican':0b000000010,
        'Puerto Rican':0b000000100,
        'Cuban':0b000001000,
        'Other Hispanic/Latino':0b000010000,
        'Not Hispanic/Latino':0b000100000,
        'Information Not Provided':0b001000000,
        'Not Applicable':0b010000000,
        'No Co-applicant':0b100000000
    },
    'aus':{
        'Desktop Underwriter':0b00000001,
        'Loan Prospector/Product Advisor':0b00000010,
        'TOTAL Scorecard':0b00000100,
        'GUS':0b00001000,
        'Other':0b00010000,
        'Internal Proprietary':0b00100000,
        'Not applicable':0b01000000,
        'Exempt':0b10000000,
    }, 
}

new_mapper = {}
for k,v in mapper.items():
    new_mapper[k] = {}
    #print(k)
    for j,w in v.items():
        #print(w,j)
        new_mapper[k][w] = j

#drop columns that will not be leveraged in MCA
fr.drop(
    labels = [
        'balloon_payment', 
        'interest_only_payment', 
        'other_nonamortizing_features',
        'income_from_median',
        'state_code',
        'county_code'
    ],
    axis=1,inplace=True
)

#identify numeric columns to convert to categorical
numerics = [
    'income',
    'loan_amount',
    'interest_rate',
    'total_loan_costs',
    'origination_charges',
    'discount_points',
    'lender_credits',
    'loan_term',
    'intro_rate_period',
    'property_value',
    'total_units',
    'tract_population',
    'tract_minority_population_percent',
    'ffiec_msa_md_median_family_income',
    'tract_to_msa_income_percentage',
    'tract_owner_occupied_units',
    'tract_one_to_four_family_homes',
    'tract_median_age_of_housing_units',
    'loan_to_value_ratio'
]

#set the cutting boundaries
bounds = [i/5 for i in range(1,5)]

for col in numerics:
    #income had some errors, for some reason
    if col == 'income':
        fr.loc[fr[col]<=0,col] = 0.01
        fr[col] = np.log(fr[col])

    s = fr[col].std()

    m = fr[col].mean()

    #cut everything based on standard deviations
    cut_level = [
        m-2*s,
        m-s,
        m+s,
        m+2*s
    ]
    
    cut_level = [-np.inf] + cut_level + [np.inf]

    #assign value based on cut boundaries
    fr[col] = pd.cut(
        fr[col],
        bins=cut_level,
        labels=["L","ML","M","MH","H"]
    )

    #convert to categorical
    fr[col] = fr[col].astype('category')

fr[numerics].head(10)
```

```{python}
#| echo: true
#| eval: true
fr_bin = fr[[
    'applicant_race',
    'applicant_ethnicity',
    'co-applicant_race',
    'co-applicant_ethnicity',
    'aus'
]].copy()

for k,v in new_mapper.items():
    for l,w in v.items():
        fr_bin[k+'_'+w] = (fr_bin[k]&l > 0).astype(int)

fr_bin.drop(
    labels=[    
        'applicant_race',
        'applicant_ethnicity',
        'co-applicant_race',
        'co-applicant_ethnicity',
        'aus'
    ],
    inplace=True,
    axis=1
)

fr.drop(
    labels=[
        'applicant_race',
        'applicant_ethnicity',
        'co-applicant_race',
        'co-applicant_ethnicity',
        'denial_reason',
        'aus',
        # 'outcome',
        'action_taken'
    ],
    inplace=True,
    axis=1
)
display(
    fr.head(10),
    fr_bin.head(10)
)
```
```{python}
#| echo: true
#| eval: true

#perform label encoding for categorical columns...
le = LabelEncoder()
maps = []
outdf = fr.copy()
for col in outdf.columns:
    le.fit(outdf[col])
    d = dict(zip(le.classes_,le.transform(le.classes_)))
    outdf[col] = le.transform(outdf[col]) #le.fit(outdf[col])
    maps.append({col:d})
# outdf.head(10)
outdf.head(10)
```

```{python}
#| echo: true
#| eval: true 

#rejoin the binary columns to the main frame
outdf = outdf.join(fr_bin,how='outer')
# for col in fr_bin.columns:
#     outdf[col] = fr_bin[col].copy()

#convert all columns to integers
# for col in outdf.columns:
#     outdf[col] = outdf[col].astype(int)
outdf_nr = outdf.copy()
```

```{python}
#| echo: true
#| eval: true

outdf.head(10)
```

```{python}
#| echo: true
#| eval: true

#drop columns that have protected classes...
outdf_nr.drop(columns=
    [
        'derived_sex',
        'applicant_ethnicity_observed',
        'co-applicant_ethnicity_observed',
        'applicant_race_observed',
        'co-applicant_race_observed',
        'applicant_sex',
        'co-applicant_sex',
        'applicant_sex_observed',
        'co-applicant_sex_observed',
        'applicant_age',
        'co-applicant_age',
        'applicant_race_American Indian/Alaska Native',
        'applicant_race_Asian',
        'applicant_race_Asian Indian',
        'applicant_race_Chinese',
        'applicant_race_Filipino',
        'applicant_race_Japanese',
        'applicant_race_Korean',
        'applicant_race_Vietnamese',
        'applicant_race_Other Asian',
        'applicant_race_Black/African American',
        'applicant_race_Native Hawaiian/Pacific Islander',
        'applicant_race_Native Hawaiian',
        'applicant_race_Guamanian/Chamorro',
        'applicant_race_Samoan',
        'applicant_race_Other Pacific Islander',
        'applicant_race_White',
        'applicant_race_Information not provided',
        'applicant_race_Not Applicable',
        'applicant_race_No Co-applicant',
        'co-applicant_race_American Indian/Alaska Native',
        'co-applicant_race_Asian',
        'co-applicant_race_Asian Indian',
        'co-applicant_race_Chinese',
        'co-applicant_race_Filipino',
        'co-applicant_race_Japanese',
        'co-applicant_race_Korean',
        'co-applicant_race_Vietnamese',
        'co-applicant_race_Other Asian',
        'co-applicant_race_Black/African American',
        'co-applicant_race_Native Hawaiian/Pacific Islander',
        'co-applicant_race_Native Hawaiian',
        'co-applicant_race_Guamanian/Chamorro',
        'co-applicant_race_Samoan',
        'co-applicant_race_Other Pacific Islander',
        'co-applicant_race_White',
        'co-applicant_race_Information not provided',
        'co-applicant_race_Not Applicable',
        'co-applicant_race_No Co-applicant',
        'applicant_ethnicity_Hispanic/Latino',
        'applicant_ethnicity_Mexican',
        'applicant_ethnicity_Puerto Rican',
        'applicant_ethnicity_Cuban',
        'applicant_ethnicity_Other Hispanic/Latino',
        'applicant_ethnicity_Not Hispanic/Latino',
        'applicant_ethnicity_Information Not Provided',
        'applicant_ethnicity_Not Applicable',
        'applicant_ethnicity_No Co-applicant',
        'co-applicant_ethnicity_Hispanic/Latino',
        'co-applicant_ethnicity_Mexican',
        'co-applicant_ethnicity_Puerto Rican',
        'co-applicant_ethnicity_Cuban',
        'co-applicant_ethnicity_Other Hispanic/Latino',
        'co-applicant_ethnicity_Not Hispanic/Latino',
        'co-applicant_ethnicity_Information Not Provided',
        'co-applicant_ethnicity_Not Applicable',
        'co-applicant_ethnicity_No Co-applicant'
    ],
    axis=1,
    inplace=True
)
```

```{python}
#| echo: true
#| eval: true

#write to csv...
outdf.to_csv('../data/cnb_pc.csv',index=False)
outdf_nr.to_csv('../data/cnb_npc.csv',index=False)
```
<!-- ############################################################ -->

## CategoricalNB
```{python}
from sklearn.naive_bayes import CategoricalNB

cnb_pc = CategoricalNB()
cnb_npc = CategoricalNB()
X_train,X_test,y_train,y_test = train_test_split(
    outdf,
    labels,
    stratify=labels,
    test_size=0.2,
    random_state=8808
)
X_train_npc,X_test_npc,y_train,y_test = train_test_split(
    outdf_nr,
    labels,
    stratify=labels,
    test_size=0.2,
    random_state=8808
)
cnb_pc.fit(X_train,y_train)
cnb_npc.fit(X_train_npc,y_train)
y_pred = cnb_pc.predict(X_test)
y_pred_npc = cnb_npc.predict(X_test_npc)
results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
results.loc[len(results)] = {
    'Model':'CategoricalNB',
    'Data':'With Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'ROC-AUC':roc_auc_score(y_test,y_pred)
}
results.loc[len(results)] = {
    'Model':'CategoricalNB',
    'Data':'Without Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred_npc),
    'Precision':precision_score(y_test,y_pred_npc),
    'Recall':recall_score(y_test,y_pred_npc),
    'F1':f1_score(y_test,y_pred_npc),
    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
}
```
```{python}
display(results.style.hide(axis='index'))
import matplotlib.pyplot as plt
fig,axes=plt.subplots(nrows=1,ncols=2)
ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred,y_true=y_test
    )
).plot(ax=axes[0])
ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred_npc,y_true=y_test
    )
).plot(ax=axes[1])
axes[0].set_title('With Protected\nClasses')
axes[1].set_title('Without Protected\nClasses')
plt.suptitle("Confusion Matrices - Categorical Naive Bayes")
plt.tight_layout()
plt.show()
```
```{python}
# mnb_data.head()
```
<!-- ############################################################ -->

```{python}
#| echo: true
#| eval: false

# are the results significantly different? do a randomization test...

results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
np.random.seed(2089)
for i in range(500):
    r = np.random.randint(0,5000,1)
    # display(r)
    X_train,X_test,y_train,y_test = train_test_split(
        outdf,
        labels,
        stratify=labels,
        test_size=0.2,
        random_state=r[0]
    )
    X_train_npc,X_test_npc,y_train,y_test = train_test_split(
        outdf_nr,
        labels,
        stratify=labels,
        test_size=0.2,
        random_state=r[0]
    )
    try:
        cnb_pc = CategoricalNB()
        cnb_npc = CategoricalNB()
        cnb_pc.fit(X_train,y_train)
        cnb_npc.fit(X_train_npc,y_train)
        y_pred = cnb_pc.predict(X_test)
        y_pred_npc = cnb_npc.predict(X_test_npc)
    except IndexError:
        i-=1
        print("Couldn't predict for iteration {}".format(i+1))
        pass

    results.loc[len(results)] = {
        'Model':'Categorical Naive Bayes',
        'Data':'With Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred),
        'Precision':precision_score(y_test,y_pred),
        'Recall':recall_score(y_test,y_pred),
        'F1':f1_score(y_test,y_pred),
        'ROC-AUC':roc_auc_score(y_test,y_pred)
    }
    results.loc[len(results)] = {
        'Model':'Categorical Naive Bayes',
        'Data':'Without Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred_npc),
        'Precision':precision_score(y_test,y_pred_npc),
        'Recall':recall_score(y_test,y_pred_npc),
        'F1':f1_score(y_test,y_pred_npc),
        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
    }
```

```{python}
#| echo: true
#| eval: false
results.to_csv('../data/cnbRandTest.csv',index=False)
```

```{python}
#| echo: false
#| eval: false

#check for statistically significant differences in model performance
from statsmodels.stats.weightstats import ztest
tbl_pc = results[results['Data']=='With Protected Classes'].describe().T.reset_index()
tbl_npc = results[results['Data']=='Without Protected Classes'].describe().T.reset_index()

from scipy import stats
sig = pd.DataFrame({
    'Stat':[],
    'z-score':[],
    'p-value':[],
    'top performer':[],
    'top mean':[],
    'difference in means':[]
})

z_stat,p_value = ztest(
    results.loc[results['Data']=='With Protected Classes']['Accuracy'],
    results.loc[results['Data']=='Without Protected Classes']['Accuracy'],
)
scores = tbl_pc['index'].unique()
for score in scores: 
    z_stat,p_value = ztest(
        results.loc[results['Data']=='With Protected Classes'][score],
        results.loc[results['Data']=='Without Protected Classes'][score],
    )
    mu_pc, mu_npc = (
        tbl_pc.loc[tbl_pc['index']==score,'mean'].iloc[0],
        tbl_npc.loc[tbl_npc['index']==score,'mean'].iloc[0]
    )
    winner = np.select(
        [
            mu_pc < mu_npc,
            mu_npc < mu_pc
        ],
        [
            'Without Protected Classes',
            'With Protected Classes'
        ],
        default='tie'
    )
    diff = np.abs(mu_pc-mu_npc)
    m = max(mu_pc,mu_npc)
    sig.loc[len(sig)] = {
        'Stat':score,
        'z-score':z_stat,
        'p-value':p_value,
        'top performer':winner,
        'top mean':m,
        'difference in means':diff
    }

display(sig.style.hide(axis='index'))
```

```{python}
#| echo: false
#| eval: false

#visualize the output distributions
res = results.copy()
res = res.melt(id_vars=['Model','Data'],var_name='Metric',value_name='Score')
g = sns.FacetGrid(data=res,col='Metric')
g.map_dataframe(
    sns.kdeplot,
    x='Score',
    hue='Data'
)
g.add_legend({})
plt.suptitle("Distributions for Randomization of Training/Testing Data\nCategorical Naive Bayes")
plt.tight_layout()
plt.show()
```

<!-- ##################################################################### -->

## MultinomialNB (New)

Since the data can't be transformed to counts, binary/bernoulli must be leveraged.

```{python}
#| echo: true
#| eval: true
X_train, X_test, y_train, y_test = train_test_split(
    data,
    labels,
    stratify=labels,
    test_size=0.2,
    random_state=8808
)

X_train_npc, X_test_npc, y_train, y_test = train_test_split(
    data_npc,
    labels,
    stratify=labels,
    test_size=0.2,
    random_state=8808
)
results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
mnb_pc=MultinomialNB()
mnb_pc.fit(X_train,y_train)
y_pred = mnb_pc.predict(X_test)
results.loc[len(results)] = {
    'Model':'MultinomialNB',
    'Data':'With Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'ROC-AUC':roc_auc_score(y_test,y_pred)
}
mnb_npc=MultinomialNB()
mnb_npc.fit(X_train_npc,y_train)
y_pred_npc=mnb_npc.predict(X_test_npc)
results.loc[len(results)] = {
    'Model':'MultinomialNB',
    'Data':'Without Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred_npc),
    'Precision':precision_score(y_test,y_pred_npc),
    'Recall':recall_score(y_test,y_pred_npc),
    'F1':f1_score(y_test,y_pred_npc),
    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
}
```

```{python}
#| echo: true
#| eval: true
import matplotlib.pyplot as plt
display(results)
fig,axes=plt.subplots(nrows=1,ncols=2)

ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred,y_true=y_test
    )
).plot(ax=axes[0])
ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred_npc,y_true=y_test
    )
).plot(ax=axes[1])
axes[0].set_title('With Protected\nClasses')
axes[1].set_title('Without Protected\nClasses')
plt.suptitle("Confusion Matrices - Multinomial Naive Bayes")
plt.tight_layout()
plt.show()
```

```{python}
#| echo: true
#| eval: false

# are the results significantly different? do a randomization test...

results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
np.random.seed(2049)
for i in range(500):
    r = np.random.randint(0,5000,1)
    X_train,X_test,y_train,y_test = train_test_split(
        outdf,
        labels,
        stratify=labels,
        test_size=0.2,
        random_state=r[0]
    )
    X_train_npc,X_test_npc,y_train,y_test = train_test_split(
        outdf_nr,
        labels,
        stratify=labels,
        test_size=0.2,
        random_state=r[0]
    )
    mnb_pc.fit(X_train,y_train)
    mnb_npc.fit(X_train_npc,y_train)
    y_pred = mnb_pc.predict(X_test)
    y_pred_npc = mnb_npc.predict(X_test_npc)
    results.loc[len(results)] = {
        'Model':'Multinomial Naive Bayes',
        'Data':'With Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred),
        'Precision':precision_score(y_test,y_pred),
        'Recall':recall_score(y_test,y_pred),
        'F1':f1_score(y_test,y_pred),
        'ROC-AUC':roc_auc_score(y_test,y_pred)
    }
    results.loc[len(results)] = {
        'Model':'Multinomial Naive Bayes',
        'Data':'Without Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred_npc),
        'Precision':precision_score(y_test,y_pred_npc),
        'Recall':recall_score(y_test,y_pred_npc),
        'F1':f1_score(y_test,y_pred_npc),
        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
    }
```

```{python}
#| echo: true
#| eval: false
results.to_csv('../data/mnbRandTest.csv',index=False)
```

```{python}
#| echo: false
#| eval: false

from statsmodels.stats.weightstats import ztest
tbl_pc = results[results['Data']=='With Protected Classes'].describe().T.reset_index()
tbl_npc = results[results['Data']=='Without Protected Classes'].describe().T.reset_index()

from scipy import stats
sig = pd.DataFrame({
    'Stat':[],
    'z-score':[],
    'p-value':[],
    'top performer':[],
    'top mean':[],
    'difference in means':[]
})

z_stat,p_value = ztest(
    results.loc[results['Data']=='With Protected Classes']['Accuracy'],
    results.loc[results['Data']=='Without Protected Classes']['Accuracy'],
)
scores = tbl_pc['index'].unique()
for score in scores: 
    z_stat,p_value = ztest(
        results.loc[results['Data']=='With Protected Classes'][score],
        results.loc[results['Data']=='Without Protected Classes'][score],
    )
    mu_pc, mu_npc = (
        tbl_pc.loc[tbl_pc['index']==score,'mean'].iloc[0],
        tbl_npc.loc[tbl_npc['index']==score,'mean'].iloc[0]
    )
    winner = np.select(
        [
            mu_pc < mu_npc,
            mu_npc < mu_pc
        ],
        [
            'Without Protected Classes',
            'With Protected Classes'
        ],
        default='tie'
    )
    diff = np.abs(mu_pc-mu_npc)
    m = max(mu_pc,mu_npc)
    sig.loc[len(sig)] = {
        'Stat':score,
        'z-score':z_stat,
        'p-value':p_value,
        'top performer':winner,
        'top mean':m,
        'difference in means':diff
    }

display(sig.style.hide(axis='index'))
```

```{python}
#| echo: false
#| eval: false

#visualize the output distributions
res = results.copy()
res = res.melt(id_vars=['Model','Data'],var_name='Metric',value_name='Score')
g = sns.FacetGrid(data=res,col='Metric')
g.map_dataframe(
    sns.kdeplot,
    x='Score',
    hue='Data'
)
plt.suptitle("Distributions for Randomization of Training/Testing Data\nMultinomial Naive Bayes")
plt.tight_layout()
plt.show()
```

## Bernoulli Naive Bayes

```{python}
#| echo: true
#| eval: true
#prep the data - done
```

```{python}
#| echo: true
#| eval: true
#split the data in to training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    data,
    labels,
    stratify=labels,
    test_size=0.2,
    random_state=8808
)

X_train_npc, X_test_npc, y_train, y_test = train_test_split(
    data_npc,
    labels,
    stratify=labels,
    test_size=0.2,
    random_state=8808
)
```

```{python}
#| echo: true
#| eval: true
#train the model
results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
bnb=BernoulliNB()
bnb.fit(X_train,y_train)
y_pred = bnb.predict(X_test)
results.loc[len(results)] = {
    'Model':'BernoulliNB',
    'Data':'With Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred),
    'Precision':precision_score(y_test,y_pred),
    'Recall':recall_score(y_test,y_pred),
    'F1':f1_score(y_test,y_pred),
    'ROC-AUC':roc_auc_score(y_test,y_pred)
}
bnb_nr=BernoulliNB()
bnb_nr.fit(X_train_npc,y_train)
y_pred_npc=bnb_nr.predict(X_test_npc)
results.loc[len(results)] = {
    'Model':'BernoulliNB',
    'Data':'Without Protected Classes',
    'Accuracy':accuracy_score(y_test,y_pred_npc),
    'Precision':precision_score(y_test,y_pred_npc),
    'Recall':recall_score(y_test,y_pred_npc),
    'F1':f1_score(y_test,y_pred_npc),
    'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
}
```

```{python}
#| echo: true
#| eval: true
#display summarized results (confusion matrix)
import matplotlib.pyplot as plt
display(results)
fig,axes=plt.subplots(nrows=1,ncols=2)

ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred,y_true=y_test
    )
).plot(ax=axes[0])
ConfusionMatrixDisplay(
    confusion_matrix(
        y_pred=y_pred_npc,y_true=y_test
    )
).plot(ax=axes[1])
axes[0].set_title('With Protected\nClasses')
axes[1].set_title('Without Protected\nClasses')
plt.suptitle("Confusion Matrices - Bernoulli Naive Bayes")
plt.tight_layout()
plt.show()
```

### Check for Difference in Performance Due to Random Chance

```{python}
#| echo: true
#| eval: false

# are the results significantly different? do a randomization test...

results = pd.DataFrame({
    'Model':[],
    'Data':[],
    'Accuracy':[],
    'Precision':[],
    'Recall':[],
    'F1':[],
    'ROC-AUC':[]
})
np.random.seed(2034)
for i in range(500):
    r = np.random.randint(0,5000,1)
    X_train, X_test, y_train, y_test = train_test_split(
        data,
        labels,
        stratify=labels,
        test_size=0.2,
        random_state=r[0]
    )
    X_train_npc, X_test_npc, y_train, y_test = train_test_split(
        data_npc,
        labels,
        stratify=labels,
        test_size=0.2,
        random_state=r[0]
    )
    mnb_pc.fit(X_train,y_train)
    mnb_npc.fit(X_train_npc,y_train)
    y_pred = mnb_pc.predict(X_test)
    y_pred_npc = mnb_npc.predict(X_test_npc)
    results.loc[len(results)] = {
        'Model':'BernoulliNB',
        'Data':'With Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred),
        'Precision':precision_score(y_test,y_pred),
        'Recall':recall_score(y_test,y_pred),
        'F1':f1_score(y_test,y_pred),
        'ROC-AUC':roc_auc_score(y_test,y_pred)
    }
    results.loc[len(results)] = {
        'Model':'BernoulliNB',
        'Data':'Without Protected Classes',
        'Accuracy':accuracy_score(y_test,y_pred_npc),
        'Precision':precision_score(y_test,y_pred_npc),
        'Recall':recall_score(y_test,y_pred_npc),
        'F1':f1_score(y_test,y_pred_npc),
        'ROC-AUC':roc_auc_score(y_test,y_pred_npc)
    }
```

```{python}
#| echo: true
#| eval: false
results.to_csv('../data/bnbRandTest.csv',index=False)
```

```{python}
#| echo: false
#| eval: false

#visualize the output distributions
res = results.copy()
res = res.melt(id_vars=['Model','Data'],var_name='Metric',value_name='Score')
g = sns.FacetGrid(data=res,col='Metric')
g.map_dataframe(
    sns.kdeplot,
    x='Score',
    hue='Data'
)
plt.suptitle("Distributions for Randomization of Training/Testing Data\nBernoulli Naive Bayes")
plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| eval: false
from statsmodels.stats.weightstats import ztest
tbl_pc = results[results['Data']=='With Protected Classes'].describe().T.reset_index()
tbl_npc = results[results['Data']=='Without Protected Classes'].describe().T.reset_index()

from scipy import stats
sig = pd.DataFrame({
    'Stat':[],
    'z-score':[],
    'p-value':[],
    'top performer':[],
    'top mean':[],
    'difference in means':[]
})

z_stat,p_value = ztest(
    results.loc[results['Data']=='With Protected Classes']['Accuracy'],
    results.loc[results['Data']=='Without Protected Classes']['Accuracy'],
)
scores = tbl_pc['index'].unique()
for score in scores: 
    z_stat,p_value = ztest(
        results.loc[results['Data']=='With Protected Classes'][score],
        results.loc[results['Data']=='Without Protected Classes'][score],
    )
    mu_pc, mu_npc = (
        tbl_pc.loc[tbl_pc['index']==score,'mean'].iloc[0],
        tbl_npc.loc[tbl_npc['index']==score,'mean'].iloc[0]
    )
    winner = np.select(
        [
            mu_pc < mu_npc,
            mu_npc < mu_pc
        ],
        [
            'Without Protected Classes',
            'With Protected Classes'
        ],
        default='tie'
    )
    diff = np.abs(mu_pc-mu_npc)
    m = max(mu_pc,mu_npc)
    sig.loc[len(sig)] = {
        'Stat':score,
        'z-score':z_stat,
        'p-value':p_value,
        'top performer':winner,
        'top mean':m,
        'difference in means':diff
    }

display(sig.style.hide(axis='index'))
```



```{python}

```